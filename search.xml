<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[JUnit]]></title>
    <url>%2F2022%2F03%2F09%2FJUnit5%2F</url>
    <content type="text"><![CDATA[JUnit5JUnit 5 JUnit 5 = JUnit Platform + JUnit Jupiter + JUnit Vintage 架构 第一层 :开发人员 (这里只进行业务开发撰写单元测试) 使用 junit-jupiter-api 等测试框架 api 编写单元测试。 第二层 : 测试引擎，JUnit 或其他测试框架实现引擎 API 的框架，jupiter-engine 和 vintage-engine 分别是 junit5 和 junit4 对测试引擎 API 的实现，其他的测试框架也可以通过实现引擎 API 从而接入 JUnit 平台。 第三层: 平台引擎 junit-platform-engine 是上一层各种引擎实现的抽象，即引擎的接口标准。 第四层： 启动器 通过 ServiceLoader 发现测试引擎的实现并安排其执行。 它为 IDE 和构建工具提供了 API，因此 IDE 可以与测试执行交互，例如，通过启动单个测试并显示其结果。 1234567891011121314151617181920212223242526272829303132333435363738394041424344import static org.junit.jupiter.api.Assertions.fail;import static org.junit.jupiter.api.Assumptions.assumeTrue;import org.junit.jupiter.api.AfterAll;import org.junit.jupiter.api.AfterEach;import org.junit.jupiter.api.BeforeAll;import org.junit.jupiter.api.BeforeEach;import org.junit.jupiter.api.Disabled;import org.junit.jupiter.api.Test;class StandardTests &#123; @BeforeAll static void initAll() &#123; &#125; @BeforeEach void init() &#123; &#125; @Test void succeedingTest() &#123; &#125; @Test void failingTest() &#123; fail("a failing test"); &#125; @Test @Disabled("for demonstration purposes") void skippedTest() &#123; // not executed &#125; @AfterEach void tearDown() &#123; &#125; @AfterAll static void tearDownAll() &#123; &#125;&#125;]]></content>
      <categories>
        <category>单元测试</category>
      </categories>
      <tags>
        <tag>JUnit</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[单元测试]]></title>
    <url>%2F2022%2F03%2F09%2F%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[什么是单元测试一个单元测试是一段代码，这段代码调用一个工作单元，并检验该工作单元的一个具体的最终结果。如果关于这个最终结果的假设是错误的，单元测试就失败了。一个单元测试的范围可以小到一个方法，大到一个类。 单元测试意义很多人经常以 “时间紧，任务重” 或者 “单元测试没用” 为借口来拒绝编写单元测试。但是 BUG 在软件的生命周期越早阶段发现，付出的代价越少。单元测试可以让很多 BUG 在编码阶段就能够及时发现并解决，而不需要交给测试人员兜底，如果测试人员兜底失败，可能造成线上故障。有了单元测试作保障，我们还可以放心对函数进行重构，如果重构代码导致单元测试运行失败，则说明重构的代码有问题。长远来看，单元测试对编码的益处（如提高代码质量和避免 BUG）远比编写单元测试的投入所花费的代价要大的多。 单元测试标准 好的单元测试必须遵守 AIR 原则。 单元测试在线上运行时，感觉像空气(AIR)一样并不存在，但在测试质量的保障上，却是非常关键的。好的单元测试宏观上来说，具有自动化、独立性、可重复执行的特点。 A: Automatic(自动化)单元测试应该是全自动执行的，并且非交互式的。测试用例通常是被定期执行的，执行过程必须完全自动化才有意义。输出结果需要人工检查的测试不是一个好的单元测试。单元测试中不准使用 System.out 来进行人肉验证，必须使用 assert 来验证。 I: Independent(独立性)保持单元测试的独立性。为了保证单元测试稳定可靠且便于维护，单元测试用例之间决不能互相调用，也不能依赖执行的先后次序。 R: Repeatable(可重复) 单元测试是可以重复执行的，不能受到外界环境的影响。 单元测试结构典型的单元测试可分为三个阶段，分别为准备、执行和验证。 准备阶段: 主要负责创建测试数据、构造mock 方法的返回值，准备环节的编码是单元测试最复杂的部分。需要注意的是 Mockito[[Mockito.md]]] 库中以 when 开头的函数其实是在准备阶段。 执行阶段: 一般只是调用测试的函数，此部分代码通常较短。 验证阶段[[AssertJ.md]]]: 通常验证测试函数的执行的结果、 准备阶段 mock 函数的调用次数等是否符合预期。 单元测试工具常用的 Java 单元测试有: JUnit、TestNG。 TestNG 受 JUnit 和 NUnit 的启发，功能相似，但是比 JUnit 更强大。TestNG 不只为单元测试而设计，其框架的设计目标是支持单元测试、公共能测试，端对端测试，集成测试等。 JUnit 具体用法比较简单，最新版本为JUnit5[[JUnit5.md]],如果想系统学习可官方使用指南，参考《JUnit 实战 (第 2 版)》, TestNG 和 JUnit 非常相似，如果想深入学习，首推 TestNG 官方文档。 主流的 Java mock 框架有: Mockito[[Mockito.md]], JMockit, Easy Mock 。 Mockito 简洁易用，有 PowerMock 拓展，允许静态函数测试，社区强大，对结果的验证和异常处理非常简洁、灵活。缺点是框架本身不支持 static 和 private 函数的 mock。 JMockit 简单易用；可以 mock “一切”，包括 final 类， final/private/static 函数，而其他 mock 框架往往只支持其中一部分；缺点是社区支持不够活跃，3 个 contributers 介乎只有一个在干活，学习曲线比较陡峭。 Easy Mock 上手简单，文档清晰；同样的社区较小，导致更多人选择其它的 mock 框架。 还有很多其他配合单元测试的框架，如强大的构造随机 Java 对象的 Easy Random ，构造随机字符串的 Java Faker 等。 验证库有：Hamcrest，AssertJ[[AssertJ.md]]。 构造单元测试数据的方式单元测试的重要环节就是构造测试数据，单元测试构造测试数据往往非常耗时，这也是很多人不喜欢写单元测试的重要原因之一。如下为构造单侧数据的几种方式： 手动手动构造单元测试数据，是指在测试类或者函数中直接声明测试数据(对象属性较多时，比较耗时)： 12345678910@Testvoid submitTest() &#123; // Setup final UserInfo userInfo = new UserInfo(); final User user = new User(); user.setId(0L); user.setOrgId(0L); user.setCode("code"); user.setAccount("account");&#125; 半自动半自动构造单元测试数据，是指使用插件自动填充所要构造对象的属性。采用 JSON 序列化和反序列化方式，将构造的对象通过 JSON 序列化到 JSON 文件里，使用时反序列化为Java对象即可： 123456@Test public void submitTest() &#123; // 构造测试数据 UserInfo userInfo = ResourceUtil.parseJson(UserInfo.class, "/data/userInfo.json"); log.info("构造的数据:&#123;&#125;", JSON.toJSONString(userInfo));&#125; 自动半自动的方式构造单元测试数据效率仍然不够高，而且缺乏灵活性，比如需要构造随机属性的对象，需要构造不同属性的 N 个对象，就会造成编码复杂度陡增。因此， Java Faker 和 Easy Random 应运而生。 java-faker123456789101112131415161718192021222324@Slf4jpublic class FakeTest &#123; @Test public void test() &#123; // 指定语言 Faker faker = new Faker(new Locale("zh-CN")); // 姓名 String name = faker.name().fullName(); log.info(name); String firstName = faker.name().firstName(); String lastName = faker.name().lastName(); log.info(lastName + firstName); // 街道 String streetAddress = faker.address().streetAddress(); log.info(streetAddress); // 颜色 Color color = faker.color(); log.info(color.name() + "--&gt;" + color.hex()); // 大学 University university = faker.university(); log.info(university.name() + "--&gt;" + university.prefix()+":"+university.suffix()); &#125;&#125; easy-randomeasy-random 可以轻松构造复杂对象，支持定义对象中集合长度，字符串长度范围，生成集合等。 1234567891011121314class EasyRandomTest &#123; private EasyRandom easyRandom; @Test void customRandomzierForFieldsShouldBeUsedToPopulateObjects() &#123; EasyRandomParameters parameters = new EasyRandomParameters() .randomize(named("name").and(ofType(String.class)).and(inClass(Human.class)), randomizer); easyRandom = new EasyRandom(parameters); Person person = easyRandom.nextObject(Person.class); assertThat(person).isNotNull(); assertThat(person.getName()).isEqualTo(FOO); &#125;&#125;]]></content>
      <categories>
        <category>单元测试</category>
      </categories>
      <tags>
        <tag>单元测试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kvm]]></title>
    <url>%2F2021%2F12%2F31%2F2021-12-31-kvm%2F</url>
    <content type="text"></content>
      <categories>
        <category>mariadb</category>
      </categories>
      <tags>
        <tag>mariadb</tag>
        <tag>galera</tag>
        <tag>haproxy</tag>
        <tag>keepalived</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MariaDB+Galera集群+haproxy+keepalived]]></title>
    <url>%2F2020%2F11%2F23%2FMariaDB-Galera%E9%9B%86%E7%BE%A4-haproxy-keepalived%2F</url>
    <content type="text"><![CDATA[Galera集群环境要求Galera集群至少需要三个节点的服务器硬件，以下操作在三个节点执行。安装后，在任意一个节点执行SQL，都是同步的。 现有三台服务器，ip分别为10.30.1.14、10.30.1.15、10.30.1.16。 安装 3台服务器的主机名修改： 123hostnamectl set-hostname node1hostnamectl set-hostname node2hostnamectl set-hostname node3 3台服务器的hosts文件修改： 12310.30.1.14 node110.30.1.15 node210.30.1.16 node3 建立3台服务器之间的SSH免密通信（3台服务器执行）： 1234ssh-keygen -t rsassh-copy-id node1ssh-copy-id node2ssh-copy-id node3 3台服务器关闭Selinux: 12setenforce 0sed -i "s/SELINUX=enforcing/SELINUX=disabled/g" /etc/selinux/config 3台服务器关闭防火墙或者添加端口允许: 123456789# 3306 MySQL client connections and mysqldump SSTfirewall-cmd --zone=public --add-port=3306/tcp --permanent# 4567 Galera Cluster replication trafficfirewall-cmd --zone=public --add-port=4567/tcp --permanent# 4568 ISTfirewall-cmd --zone=public --add-port=4568/tcp --permanent# 4444 all SSTs besides mysqldumpfirewall-cmd --zone=public --add-port=4444/tcp --permanentfirewall-cmd --reload 3台服务器设置Yum源: 12345678cd /etc/yum.repos.dcat &gt;&gt; mariadb.repo &lt;&lt;EOF[mariadb]name = MariaDBbaseurl = https://mirrors.ustc.edu.cn/mariadb/yum/10.4/centos7-amd64gpgkey=https://mirrors.ustc.edu.cn/mariadb/yum/RPM-GPG-KEY-MariaDBgpgcheck=1EOF 3台服务器安装软件: 1yum install MariaDB-server MariaDB-client galera-4 -y 3台服务器配置远程访问: 12grant all privileges on *.* to root@'%' identified by 'd001!' with grant option;flush privileges; 3台服务器配置编码为utf-8,编辑配置文件vi /etc/my.cnf: 1234567891011[client]default-character-set = utf8mb4[mysql]default-character-set = utf8mb4[mysqld]character-set-client-handshake = FALSEcharacter-set-server = utf8mb4collation-server = utf8mb4_unicode_ciinit_connect='SET NAMES utf8mb4' galera集群配置 编辑Node1上的/etc/my.cnf.d/server.cnf 添加如下配置： 123456789101112131415161718[galera]# Mandatory settingswsrep_on=ONwsrep_provider=/usr/lib64/galera-4/libgalera_smm.sowsrep_cluster_address="gcomm://10.30.1.14,10.30.1.15,10.30.1.16" #整个集群的IP地址binlog_format=row # binlog文件格式：行default_storage_engine=InnoDBinnodb_autoinc_lock_mode=2# Allow server to accept connections on all interfaces.# bind-address=0.0.0.0# Optional setting# wsrep_slave_threads=1# innodb_flush_log_at_trx_commit=0wsrep_provider_options="gcache.size=1G"wsrep_cluster_name=MariaDB-Galera-Clusterwsrep_node_name=node1 #hostname，对应前面网路配置/etc/hostswsrep_node_address=10.30.1.14 #机器IP地址wsrep_sst_method=rsync #拷贝模式xtrabackup-v2 或者 rsync 编辑Node2上的/etc/my.cnf.d/server.cnf 添加如下配置： 123456789101112131415161718[galera]# Mandatory settingswsrep_on=ONwsrep_provider=/usr/lib64/galera-4/libgalera_smm.sowsrep_cluster_address="gcomm://10.30.1.14,10.30.1.15,10.30.1.16" #整个集群的IP地址binlog_format=row # binlog文件格式：行default_storage_engine=InnoDBinnodb_autoinc_lock_mode=2# Allow server to accept connections on all interfaces.# bind-address=0.0.0.0# Optional setting# wsrep_slave_threads=1# innodb_flush_log_at_trx_commit=0wsrep_provider_options="gcache.size=1G"wsrep_cluster_name=MariaDB-Galera-Clusterwsrep_node_name=node2 #hostname，对应前面网路配置/etc/hostswsrep_node_address=10.30.1.15 #机器IP地址wsrep_sst_method=rsync #拷贝模式xtrabackup-v2 或者 rsync 编辑Node3上的/etc/my.cnf.d/server.cnf 添加如下配置： 123456789101112131415161718[galera]# Mandatory settingswsrep_on=ONwsrep_provider=/usr/lib64/galera-4/libgalera_smm.sowsrep_cluster_address="gcomm://10.30.1.14,10.30.1.15,10.30.1.16" #整个集群的IP地址binlog_format=row # binlog文件格式：行default_storage_engine=InnoDBinnodb_autoinc_lock_mode=2# Allow server to accept connections on all interfaces.# bind-address=0.0.0.0# Optional setting# wsrep_slave_threads=1# innodb_flush_log_at_trx_commit=0wsrep_provider_options="gcache.size=1G"wsrep_cluster_name=MariaDB-Galera-Clusterwsrep_node_name=node3 #hostname，对应前面网路配置/etc/hostswsrep_node_address=10.30.1.16 #机器IP地址wsrep_sst_method=rsync #拷贝模式xtrabackup-v2 或者 rsync 启动集群 启动第一个节点，底层命令是：mysqld –wsrep-new-cluster galera_new_cluster 在其他节点上启动服务 systemctl start mariadb.service 主节点中添加集群认证用户galera，密码galera（可选） 12MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON *.* TO 'galera'@'%' IDENTIFIED BY 'galera' WITH GRANT OPTION;MariaDB [(none)]&gt; flush privileges; 故障排查 启动集群时报错[ERROR] WSREP: It may not be safe to bootstrap the cluster from this node. It was not the last one to leave the cluster and may not contain all the updates. 打开文件/var/lib/mysql/grastate.dat，修改safe_to_bootstrap的值置为1 集群测试 确认集群启动成功（返回当前的集群节点数量） mysql&gt; show status like &#39;wsrep_cluster_size&#39;; 查看galera状态 mysql&gt; show status like &#39;wsrep%&#39;; HAProxy安装yum install -y haproxy 配置编辑HAProxy配置文件 vi /etc/haproxy/haproxy.cfg,配置如下(两台haproxy服务器配置相同): 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748global log 127.0.0.1 local2 chroot /var/lib/haproxy pidfile /var/run/haproxy.pid maxconn 4000 user haproxy group haproxy daemon # turn on stats unix socket stats socket /var/lib/haproxy/stats defaults mode http log global option tcplog option dontlognull option http-server-close #option forwardfor except 127.0.0.0/8 option redispatch retries 3 timeout http-request 10s timeout queue 1m timeout connect 10s timeout client 1m timeout server 1m timeout http-keep-alive 10s timeout check 10s maxconn 3000 listen galera bind 0.0.0.0:13306 balance roundrobin mode tcp option tcpka option mysql-check user haproxy server galera-mariadb-1 192.168.123.36:3306 check weight 1 server galera-mariadb-2 192.168.123.35:3306 check weight 1 server galera-mariadb-3 192.168.123.34:3306 check weight 1 listen stats## HAProxy stats web gui running on port 9000 - username and password: haproxy. bind 0.0.0.0:9000 mode http stats enable stats uri /stats stats realm HAProxy\ Statistics stats auth haproxy:haproxy stats admin if TRUE 启动systemctl start haproxy keepalived安装 创建依赖环境： 123yum -y install openssl-devel gcc gcc-c++mkdir /etc/keepalivedwget https://www.keepalived.org/software/keepalived-2.0.18.tar.gz 安装keepalived: 1234tar -zxvf keepalived-2.0.18.tar.gzmv keepalived-2.0.18 /usr/local/keepalivedcd /usr/local/keepalived./configure &amp;&amp; make &amp;&amp; make install 设置开机启动 systemctl enable keepalived 配置 编辑haproxy检测脚本，vi /etc/keepalived/chk_haproxy.sh 12345#!/bin/bashchkha=`ps -C haproxy --no-header |wc -l`if [ $chkha -eq 0 ];then systemctl stop keepalivedfi 赋予脚本执行权限 chmod +x chk_haproxy.sh 编辑keepalived主配置文件，vi /etc/keepalived/keepalived.conf 123456789101112131415161718192021222324252627282930global_defs &#123; router_id Haproxy1 #服务器标识&#125;vrrp_script chk_haproxy &#123; script "/etc/keepalived/chk_haproxy.sh" interval 1 #（检测脚本执行的间隔，单位是秒) weight 2 #权重&#125;vrrp_instance VI_1 &#123; state MASTER #指定keepalived的角色，MASTER为主，BACKUP为备 interface eth0 #绑定的网卡 virtual_router_id 201 #虚拟路由编号，主从要一直 priority 100 #优先级，数值越大，获取处理请求的优先级越高 advert_int 1 #检查间隔，默认为1s(vrrp组播周期秒数) authentication &#123; #设置验证类型和密码，MASTER和BACKUP必须使用相同的密码才能正常通信 auth_type PASS auth_pass 1111 &#125; track_script &#123; chk_haproxy #调用检测脚本） &#125; virtual_ipaddress &#123; 10.30.1.26 #定义虚拟ip(VIP)，可多设，每行一个 &#125; track_interface &#123; eth0 &#125;&#125; 编辑keepalived备配置文件，vi /etc/keepalived/keepalived.conf 123456789101112131415161718192021222324252627282930global_defs &#123; router_id Haproxy2&#125;vrrp_script chk_haproxy &#123; script "/etc/keepalived/chk_haproxy.sh" interval 1 weight 2&#125;vrrp_instance VI_1 &#123; state BACKUP interface eth0 virtual_router_id 201 priority 99 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; track_script &#123; chk_haproxy &#125; virtual_ipaddress &#123; 192.168.123.44 &#125; track_interface &#123; eth0 &#125;&#125; 如果开启了防火墙，需打开如下配置 123firewall-cmd --add-rich-rule='rule protocol value="vrrp" accept' --permanentfirewall-cmd --reloadfirewall-cmd --list-all 启动keepalived systemctl start keepalived]]></content>
      <categories>
        <category>mariadb</category>
      </categories>
      <tags>
        <tag>mariadb</tag>
        <tag>galera</tag>
        <tag>haproxy</tag>
        <tag>keepalived</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rabbitMQ集群]]></title>
    <url>%2F2020%2F10%2F21%2FrabbitMQ%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[单一模式参考https://blog.haomingzx.top/2020/02/22/RabbitMQ%E5%AD%A6%E4%B9%A0/ 登录rabbitmq报错User can only log in via localhost? 配置文件/etc/rabbitmq/添加 如下配置 loopback_users = none 普通模式默认的集群模式。 环境 IP地址 主机名 操作系统 防火墙和SELinux 用途 192.168.56.114 mq1 CentOS7(64位) 关闭 磁盘节点 192.168.56.115 mq2 CentOS7(64位) 关闭 内存节点 RabbitMQ集群节点必须在同一网段里，如果是跨广域网，效果会变差。 配置 修改主机名 vi /etc/hostname，mq2相同操作,修改后需要重启系统生效。 mq1.localdomain 修改hosts文件 vi /etc/hosts 12192.168.56.114 mq1192.168.56.125 mq2 拷贝erlang.cookie Rabbitmq的集群是依附于erlang的集群来工作的,所以必须先构建起erlang的集群景象。Erlang的集群中各节点是经由过程一个magic cookie来实现的,这个cookie存放在/var/lib/rabbitmq/.erlang.cookie中，文件是400的权限。所以必须保证各节点cookie一致,不然节点之间就无法通信。 安装插件 12rabbitmq-plugins list //查看插件安装情况rabbitmq-plugins enable rabbitmq_management //启用rabbitmq_management服务 将mq1作为内存节点加入mq1节点集群中，在mq2执行如下命令： 123rabbitmqctl stop_app //停掉rabbit应用rabbitmqctl join_cluster --ram rabbit@mq1 //加入到磁盘节点 ram表示内存节点rabbitmqctl start_app //启动rabbit应用 查看集群状态 rabbitmqctl cluster_status 更改节点类型 内存节点： 内存节点将所有的队列、交换机、绑定、用户、权限和 vhost 的元数据定义存储在内存中，好处是可以使得像交换机和队列声明等操作更加的快速。例外情况是：持久的 queue 的内容将被保存到磁盘。 磁盘节点： 将元数据存储在磁盘中，单节点系统只允许磁盘类型的节点，防止重启 RabbitMQ 的时候，丢失系统的配置信息。 注意点： 1、内存节点由于不进行磁盘读写，它的性能比磁盘节点高。 2、集群中可以存在多个磁盘节点，磁盘节点越多整个集群可用性越好，但是集群整体性能不会线性增加，需要权衡考虑。 3、RabbitMQ 要求在集群中至少有一个磁盘节点，所有其他节点可以是内存节点，当节点加入或者离开集群时，必须要将该变更通知到至少一个磁盘节点。如果集群中唯一的一个磁盘节点崩溃的话，集群仍然可以保持运行，但是无法进行其他操作（增删改查），直到节点恢复。 4、设置两个磁盘节点，至少有一个是可用的，可以保存元数据的更改。 12345678# 停止节点 sbin/rabbitmqctl stop_app # 与集群通讯，从集群中删除节点 sbin/rabbitmqctl reset # 以RAM模式重新加入集群 sbin/rabbitmqctl join_cluster rabbit@MQ1 --ram # 启动节点 sbin/rabbitmqctl start_app 节点单机状态时，reset 命令将清空节点的状态，并将其恢复到空白状态。当节点是集群的一部分时，该命令也会和集群中的磁盘节点通信，告诉他们该节点正在离开集群。 这很重要，不然，集群会认为该节点出了故障，并期望其最终能够恢复回来，在该节点回来之前，集群禁止新的节点加入。 镜像模式把需要的队列做成镜像队列，存在于多个节点，属于RabbiMQ的HA方案，在对业务可靠性要求较高的场合中比较适用；要实现镜像模式，需要先搭建一个普通集群模式，在这个模式的基础上再配置镜像模式以实现高可用。 上面已经完成RabbitMQ默认集群模式，但并不保证队列的高可用性，尽管交换机、绑定这些可以复制到集群里的任何一个节点，但是队列内容不会复制。虽然该模式解决一项目组节点压力，但队列节点宕机直接导致该队列无法应用，只能等待重启，所以要想在队列节点宕机或故障也能正常应用，就要复制队列内容到集群里的每个节点，必须要创建镜像队列。 镜像队列是基于普通的集群模式的，然后再添加一些策略，所以你还是得先配置普通集群，然后才能设置镜像队列，我们就以上面的集群接着做。 配置图形界面配置 登录控制台 http://192.168.56.114:15672/#/ Name:策略名称 Pattern：匹配的规则，这里表示匹配a开头的队列，如果是匹配所有的队列，那就是^. Definition:使用ha-mode模式中的all，也就是同步所有匹配的队列。问号链接帮助文档。 命令行配置rabbitmqctl set_policy &lt;name&gt; [-p &lt;vhost&gt;] &lt;pattern&gt; &lt;definition&gt; [--apply-to &lt;apply-to&gt;] 123456789101112131415name: 策略名称 vhost: 指定vhost, 默认值 / pattern: 通过正则表达式匹配哪些需要镜像, ^为所有 definition: ha-mode: 指明镜像队列的模式，有效值为 all/exactly/nodes all 表示在集群所有的节点上进行镜像，无需设置ha-params exactly 表示在指定个数的节点上进行镜像，节点的个数由ha-params指定 nodes 表示在指定的节点上进行镜像，节点名称通过ha-params指定ha-params: ha-mode 模式需要用到的参数 ha-sync-mode: 镜像队列中消息的同步方式，有效值为automatic，manually apply-to: 策略作用对象。可选值3个，默认all exchanges 表示镜像 exchange queues 表示镜像 queue all 表示镜像 exchange和queue 示例命令： rabbitmqctl set_policy admin &quot;^&quot; &#39;{&quot;ha-mode&quot;:&quot;all&quot;,&quot;ha-sync-mode&quot;:&quot;automatic&quot;}&#39; ha-mode ha-params 功能 all 空 镜像队列将会在整个集群中复制。当一个新的节点加入后，也会在这 个节点上复制一份。 exactly count 镜像队列将会在集群上复制 count 份。如果集群数量少于 count 时候，队列会复制到所有节点上。如果大于 Count 集群，有一个节点 crash 后，新进入节点也不会做新的镜像。 nodes node name 镜像队列会在 node name 中复制。如果这个名称不是集群中的一个，这不会触发错误。如果在这个 node list 中没有一个节点在线，那么这个 queue 会被声明在 client 连接的节点。 测试参考https://blog.51cto.com/11134648/2155934 https://www.rabbitmq.com/access-control.html#default-state https://www.cnblogs.com/passzhang/p/13207426.html]]></content>
      <categories>
        <category>rabbitMQ</category>
      </categories>
      <tags>
        <tag>rabbitMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis高可用方案]]></title>
    <url>%2F2020%2F10%2F21%2Fredis%E9%AB%98%E5%8F%AF%E7%94%A8%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[单机安装 下载安装包 http://download.redis.io/releases/redis-4.0.14.tar.gz 解压文件 tar -xzvf redis-4.0.14.tar.gz 编译安装 make &amp;&amp; make install 配置 在/usr/local/redis/redis4.0.14目录下复制redis.conf到/etc/redis/目录下，修改配置文件 端口：port 6379 后台启动：daemonize yes 日志文件输出名字：logfile ./redis.log (需要手动创建文件) 文件存储目录：dir /usr/local/redis/log (需要手动创建文件夹) 连接redis密码：requirepass dcits001! 允许任何人连接redis：bind 0.0.0.0 tcp-backlog修改为2048并执行echo 511 &gt; /proc/sys/net/core/somaxconn 要不启动会出现警告： 将下列内容添加到/etc/sysctl.conf中,保存退出, 执行sysctl -p 配置生效 12net.core.somaxconn = 1024 vm.overcommit_memory = 1 将echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled添加到/etc/rc.local中,执行source /etc/rc.local生效配置。 开机启动 在/usr/local/redis/redis-4.0.14/utils目录下，执行./install_server.sh 设为开机启动 chkconfig redis_6379 on 常用启动及停止命令 service redis_6379 start service redis_6379 stop 如果停止失败，出现: 是由于设置密码的原因,修改 /etc/init.d/redis_6379 打开防火墙并开放6379端口 123456systemctl status firewalld #查看防火墙状态systemctl start firewalld #打开防火墙systemctl stop firewalld #关闭防火墙firewall-cmd --zone=public --add-port=6379/tcp --permanent # 开放6379端口firewall-cmd --zone=public --remove-port=6379/tcp --permanent #关闭6379端口firewall-cmd –reload #配置立即生效 主从开启两台redis从服务器，可以位于两台服务器或同一台服务器的不同端口。 配置 master的redis配置文件只要设置好bind即可 bind 0.0.0.0 修改slave的redis配置文件,如果master设置了验证密码，还需配置masterauth 12slaveof 192.168.56.112 6379 (映射到主服务器上，6379是端口号)masterauth d001! 配置完之后启动slave的Redis服务 service redis_6379 restart 哨兵模式 配置 Sentinel可以切换主从数据库，主数据库可能会变成从数据库，所以三台机器上都需要同时设置requirepass和masterauth配置项。编辑 /etc/redis/redis.conf 12requirepass d001!masterauth d001! 配置sentinel配置文件 /etc/redis/sentinel.conf，每个sentinel配置基本相同，内容如下 1234567891011daemonize yesport 26379bind 192.168.56.112 #每个主机该配置不同sentinel monitor redis-master 192.168.56.112 6379 2sentinel down-after-milliseconds redis-master 5000sentinel failover-timeout redis-master 180000sentinel parallel-syncs redis-master 2sentinel auth-pass redis-master dctis001!sentinel notification-script redis-master /etc/redis/notify.sh#sentinel client-reconfig-script mymaster /etc/redis/failover.shlogfile /var/log/redis/redis-sentinel.log daemonize yes 以后台进程模式运行。 port 26379 Sentinel实例之间的通讯端口，该端口号默认为26379。 bind 192.168.56.112 Sentinel默认会绑定到127.0.0.1，这里要在多台机器间通信，我们将它绑定到主机IP上。 sentinel monitor redis-master 192.168.56.112 6379 2 Sentinel去监视一个名为redis-master的主服务器，这个主服务器的IP地址为192.168.56.112 ，端口号为6379。将这个主服务器判断为失效至少需要2个Sentinel同意，一般设置为N/2+1(N为Sentinel总数)。只要同意Sentinel的数量不达标，自动故障迁移就不会执行。 sentinel down-after-milliseconds redis-master 5000 down-after-milliseconds选项指定了Sentinel认为服务器已经断线所需的毫秒数。如果服务器在给定的毫秒数之内，没有返回Sentinel发送的PING命令的回复，或者返回一个错误，那么Sentinel将这个服务器标记为主观下线(subjectively down，简称SDOWN)。 sentinel failover-timeout redis-master 180000 如果在多少毫秒内没有把宕掉的那台Master恢复，那Sentinel认为这是一次真正的宕机。在下一次选取时排除该宕掉的Master作为可用的节点，然后等待一定的设定值的毫秒数后再来探测该节点是否恢复，如果恢复就把它作为一台Slave加入Sentinel监测节点群，并在下一次切换时为他分配一个”选取号”。 sentinel parallel-syncs redis-master 2 parallel-syncs选项指定了在执行故障转移时，最多可以有多少个从服务器同时对新的主服务器进行同步。这个数字越小，完成故障转移所需的时间就越长。 sentinel auth-pass redis-master d001! 当Master设置了密码时，Sentinel连接Master和Slave时需要通过设置参数auth-pass配置相应密码。 sentinel notification-script redis-master /etc/redis/notify.sh 指定Sentinel检测到该监控的Redis实例failover时调用的报警脚本。脚本被允许执行的最大时间为60秒，超过这个时间脚本会被kill。该配置项可选，但线上系统建议配置。这里的通知脚本简单的记录一下failover事件。 sentinel client-reconfig-script redis-master /etc/redis/failover.sh 指定Sentinel failover之后重配置客户端时执行的脚本，该配置项可选，但线上系统建议配置。 logfile /var/log/redis/redis-sentinel.log 日志文件所在位置，默认在/var/log/redis/redis-sentinel.log。该文件要手动创建。 创建通知脚本/etc/redis/notify.sh，并增加执行权限 12#! /bin/bashecho "master failovered at `date`" &gt; /var/log/redis/redis_issues.log chmod +x /etc/redis/notify.sh 运行sentinel redis-sentinel /etc/redis/sentinel.conf 开机自启 将redis预设脚本复制到 /etc/init.d/ 中 cp ~/redis-4.0.14/utils/redis_init_script /etc/init.d/redis_sentinel 调整脚本内容 vi /etc/init.d/redis_sentinel REDISPORT 默认为26379,但只要和其他端口不重复就行 EXEC /usr/local/bin/redis-sentinel 指定redis-sentinel命令路径 CONF /etc/redis/sentinel.conf 配置文件路径 建立systemd, vi /etc/systemd/system/redis_sentinel.service，内容如下 12345678910[Unit] Description=Redis Sentinel on port 26379 [Service] Type=forkingExecStart=/etc/init.d/redis_sentinel startExecStop=/etc/init.d/redis_sentinel stop [Install]WantedBy=multi-user.target 更新systemd配置 systemctl daemon-reload 设置开机自启动 systemctl enable redis_sentinel.service 集群未完待续]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql高可用方案]]></title>
    <url>%2F2020%2F10%2F19%2Fmysql%E9%AB%98%E5%8F%AF%E7%94%A8%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[galera集群环境要求galera集群至少需要三个节点的服务器硬件，以下操作在三个节点执行。安装后，在任意一个节点执行SQL，都是同步的。 安装 添加RPM源 vi etc/yum.repos.d/galera.repo 123456789[galera]name = Galerabaseurl = https://releases.galeracluster.com/galera-3/DIST/RELEASE/ARCH gpgkey = https://releases.galeracluster.com/GPG-KEY-galeracluster.com gpgcheck = 1[mysql-wsrep]name = MySQL-wsrepbaseurl=https://releases.galeracluster.com/mysql-wsrep-VERSION/DIST/RELEASE/ARCH gpgkey = https://releases.galeracluster.com/GPG-KEY-galeracluster.comgpgcheck = 1 yum 安装 yum install galera-3 mysql-wsrep-5.7 rsync 配置开机自启动 systemctl enable mysqld 启动mysql systemctl start mysqld 登录MySql命令行，修改密码 如果版本为5.7，系统为root设置了随机密码，需要修改配置文件 /etc/my.cnf,在最后添加如下配置，并重启mysql服务 skip-grant-tables=1 #跳过密码验证，等密码设置成功后，再将此配置删除掉 登录mysql mysql -uroot -p 修改密码 12update mysql.user set authentication_string=password(&apos;001!&apos;) where user=&apos;root&apos;;flush privileges; 设置远程访问 12GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;%&apos; IDENTIFIED BY &apos;001!&apos; WITH GRANT OPTION;flush privileges; 如果执行出现如下错误 12&gt; ERROR 3009 (HY000): Column count of mysql.user is wrong. Expected 45, found 43. Created with MySQL 50649, now running 50730. Please use mysql_upgrade to fix this error.&gt; 原因：用户在创建时选择的是MySQL5.7的版本，而导入的备份文件为MySQL5.6的，版本不一致导致MySQL系统表有差异所之后。执行如下命令解决:mysql_upgrade -uroot -p 服务器关闭selinux 12setenforce 0sed -i "s/SELINUX=enforcing/SELINUX=disabled/g" /etc/selinux/config 服务器关闭防火墙或者添加端口允许 123456# 3306 MySQL client connections and mysqldump SSTfirewall-cmd --zone=public --add-port=3306/tcp --permanent# 4567 Galera Cluster replication trafficfirewall-cmd --zone=public --add-port=4567/tcp --permanent# 4568 ISTfirewall-cmd --zone=public --add-port=4568/tcp --permanent 集群配置 修改 /etc/my.cnf 文件，添加配置 1!includedir /etc/my.cnf.d/ 增加配置文件 /etc/my.cnf.d/galera.cnf 123456789101112131415161718192021[mysqld]binlog_format=ROW #binlog文件格式：行default-storage-engine=innodbinnodb_autoinc_lock_mode=2bind-address=0.0.0.0# Galera Provider Configurationwsrep_on=ONwsrep_provider=/usr/lib64/galera-3/libgalera_smm.so# Galera Cluster Configurationwsrep_cluster_name="fucloud_cluster"wsrep_cluster_address="gcomm://192.168.56.108,192.168.56.109,192.168.56.110" #整个集群的IP地址# Galera Synchronization Configurationwsrep_sst_method=rsync #拷贝模式xtrabackup-v2 或者 rsync#wsrep_sst_method=xtrabackup-v2# Galera Node Configuration 节点配置，每个节点只是这部分不同wsrep_node_address="192.168.56.110" #本节点ip地址wsrep_node_name="mysql3" #本节点名称 启动集群 随机选择一个节点，使用专用脚本 mysqld_bootstrap 初始化集群 mysqld_bootstrap 在其他节点上启动 mysqld 服务 systemctl start mysqld 集群测试 确认集群启动成功（返回当前的集群节点数量） mysql&gt; show status like &#39;wsrep_cluster_size&#39;; 查看galera状态 mysql&gt; show status like &#39;wsrep%&#39;; 参考https://www.cnblogs.com/weijie0717/p/8445167.html https://my.oschina.net/colben/blog/1831527 https://blog.51cto.com/14089205/2477697 https://galeracluster.com/ 双主mysql+keepalived集群两台mysql互为主备，使用keepalived监控mysql状态，进行自动failover。 安装mysqlmysql安装参考以上。 keepalived 创建依赖环境 123[root@localhost yum -y install openssl-devel gcc gcc-c++[root@localhost mkdir /etc/keepalived[root@localhost wget https://www.keepalived.org/software/keepalived-2.0.18.tar.gz 安装keepalived 1234[root@localhost]# tar -zxvf keepalived-2.0.18.tar.gz[root@localhost]# mv keepalived-2.0.18 /usr/local/keepalived[root@localhost]# cd /usr/local/keepalived[root@localhost]# ./configure &amp;&amp; make &amp;&amp; make install 创建启动文件 123[root@localhost]# cp -a /usr/local/etc/keepalived /etc/init.d/[root@localhost]# cp -a /usr/local/etc/sysconfig /keepalived/etc/sysconfig/[root@localhost]# cp -a /usr/local/sbin/keepalived /usr/sbin/ 设置开机启动 [root@localhost yum.repos.d]# systemctl enable keepalived 配置mysql主备配置 配置服务器mysql1，修改/etc/my.cnf文件，增加如下配置： 123456[mysqld]server-id=1 #id唯一log-bin=mysql-bin #开启binlog日志功能auto-increment-increment=2auto-increment-offset=1log-slave-updates 配置服务器mysql2，修改/etc/my.cnf文件，增加如下配置： 123456[mysqld]server-id=2log-bin=mysql-bin #开启binlog日志功能 auto-increment-increment=2auto-increment-offset=2log-slave-updates 重启两台服务器的mysql服务 systemctl restart mysqld 在mysql1服务器上建立账户并授权 grant replication slave on *.* to &#39;itscmpsync&#39;@&#39;%&#39; identified by &#39;dcits001!&#39;; 注：一般不用root帐号，“%”表示所有客户端都可能连，只要帐号，密码正确，此处可用具体客户端IP代替，如192.168.56.109(mysql2服务器的ip地址)，加强安全 登录mysql1服务器的mysql，查询master的状态 mysql&gt; show master status; 注：执行完此步骤后不要再操作master1服务器MYSQL，防止主服务器状态值变化 配置mysql2服务器 1mysql&gt;change master to master_host='101.200.56.108',master_user='itscmpsync',master_password='dcits001!',master_log_file='mysql-bin.000008',master_log_pos=154; 注：master_log_file和master_log_pos的值应与mysql1服务器状态列出的值对应 启动mysql2服务器复制功能 mysql&gt;start slave; 检查mysql2服务器复制功能状态 mysql&gt;show slave status\G Slave_IO_Running: Yes //此状态必须YES Slave_SQL_Running: Yes //此状态必须YES 同样的操作，设置mysql1为mysql2的从服务器，在mysql2服务器上建立账户并授权 mysql&gt;grant replication slave on *.* to &#39;itscmpsync&#39;@&#39;%&#39; identified by &#39;dcits001!&#39;; 注：一般不用root帐号，“%”表示所有客户端都可能连，只要帐号，密码正确，此处可用具体客户端IP代替，如192.168.56.108(mysql1服务器的ip地址)，加强安全 登录mysql2服务器的mysql，查询master的状态 mysql&gt;show master status; 注：执行完此步骤后不要再操作主服务器MYSQL，防止主服务器状态值变化 配置mysql1服务器 1mysql&gt;change master to master_host='192.168.56.109',master_user='itscmpsync',master_password='dcits001!',master_log_file='mysql-bin.000010',master_log_pos=154; 注：master_log_file和master_log_pos的值应与mysql2服务器状态列出的值对应 启动mysql1服务器复制功能 mysql&gt;show slave status\G 检查mysql1服务器复制功能状态 mysql&gt;show slave status\G keepalived配置 编辑mysql1服务器keepalived配置文件，/etc/keepalived/keepalived.conf 1234567891011121314151617181920212223242526272829303132333435363738394041global_defs &#123; router_id mysql-1 #运行keepalived服务器标识&#125;vrrp_instance VI_1 &#123; state BACKUP #指定keepalived的角色，两台配置此处均是BACKUP，设为BACKUP将根据优先级决定主或从 interface enp0s8 #指定检测网络的接口 #虚拟路由标示，这个标示是一个数字(取值在0-255之间，用来区分多个instance的VRRP组播)，同一个vrrp实例使用 唯一的标示，确保和mysql2相同，同网内不同集群此项必须不同，否则发生冲突 virtual_router_id 51 priority 100#用来选举master,该项取值范围是1-255（在此范围之外会被识别成默认值100），数值大的为master advert_int 1 #发vrrp包的时间间隔，即多久进行一次master选举（可以认为是健康检测时间间隔） #不抢占，允许优先级较低的作为master，即使有priority更高的节点启动，一般只在优先级高的mysql配置 Nopreempt authentication &#123; #认证区域，认证类型有PASS和HA(IPSEC),推荐使用PASS(密码只识别前8位) auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; #VIP区域，指定vip地址 192.168.56.150 #虚拟vip &#125;&#125;#设置虚拟服务器，需要指定虚拟IP地址和服务端口，IP与端口之间用空格隔开virtual_server 192.168.56.150 3306 &#123; delay_loop 2 #每隔2秒检查一次real_server状态 lb_algo rr #设置后端调度算法，这里设置为rr,即轮询算法 lb_kind DR #设置LVS实现负载均衡的机制，有NAT、TUN、DR三个模式可选 #会话保持时间，单位为秒。这个选项对动态网页非常有用，为集群系统中的session共享提供了一个很好的解决方案，有了这个会话保持功能，用户的请求会被一只分发到某个服务节点，直到超过这个会话的保持时间 persistence_timeout 60 protocol TCP #指定转发协议类型，有TCP和UDP两种 real_server 192.168.56.108 3306 &#123; #配置服务节点1，需要指定real server的真实IP地址和端口 weight 3 #配置服务的权重 notify_down /etc/keepalived/bin/mysql.sh #检测到服务down后执行的脚本 TCP_CHECK &#123; connect_timeout 3 #连接超时时间 nb_get_retry 3 #重连次数 delay_before_retry 3 #重连间隔时间 connect_port 3306 #健康检查端口 &#125; &#125;&#125; 编辑mysql2服务器keepalived配置文件，/etc/keepalived/keepalived.conf 123456789101112131415161718192021222324252627282930313233343536global_defs &#123; router_id mysql-2 #服务器标识&#125;vrrp_instance VI_1 &#123; state BACKUP interface enp0s8 virtual_router_id 51 priority 50 #优先级，用来选举 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 192.168.56.150 &#125;&#125;virtual_server 192.168.56.150 3306 &#123; delay_loop 2 lb_algo rr lb_kind DR persistence_timeout 60 protocol TCP real_server 192.168.56.109 3306 &#123; weight 3 notify_down /etc/keepalived/bin/mysql.sh TCP_CHECK &#123; connect_timeout 3 nb_get_retry 3 delay_before_retry 3 connect_port 3306 &#125; &#125;&#125; 分别在mysql1和mysql2上创建服务down后脚本 /etc/keepalived/bin/mysql.sh 1234#!/bin/bashpkill keepalivedsleep 10systemctl start keepalived 配置运行权限 chmod +x mysql.sh 启动keepalived systemctl start keepalived 总结 采用keepalived作为高可用方案时，两个节点最好都设置成BACKUP模式，避免因为意外情况下（比如脑裂）相互抢占导致往两个节点写入相同数据而引发冲突； 把两个节点的auto_increment_increment（自增步长）和auto_increment_offset（自增起始值）设成不同值。其目的是为了避免master节点意外宕机时，可能会有部分binlog未能及时复制到slave上被应用，从而会导致slave新写入数据的自增值和原先master上冲突了，因此一开始就使其错开；当然了，如果有合适的容错机制能解决主从自增ID冲突的话，也可以不这么做； .slave节点服务器配置不要太差，否则更容易导致复制延迟。作为热备节点的slave服务器，硬件配置不能低于master节点； 如果对延迟问题很敏感的话，可考虑使用MariaDB分支版本，或者直接上线MySQL 5.7最新版本，利用多线程复制的方式可以很大程度降低复制延迟； 参考https://cloud.tencent.com/developer/article/1139739 https://cloud.tencent.com/developer/article/1343127 https://cloud.tencent.com/developer/article/1343127 https://www.cnblogs.com/gered/p/11221702.html]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[echart+百度地图]]></title>
    <url>%2F2020%2F08%2F23%2Fechart-%E7%99%BE%E5%BA%A6%E5%9C%B0%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[最近使用echart在地图上绘制散点图，总结一下，防止以后忘记。 注意：使用了django框架，如果使用其他技术，需要修改路径。 引入文件echart&lt;script src=&quot;js/echarts.js&quot;&gt;&lt;/script&gt; 百度地图扩展&lt;script src=&quot;js/bmap.js&quot;&gt;&lt;/script&gt; 源码地址、使用文档 百度地图api&lt;script src=&quot;http://api.map.baidu.com/api?v=2.0&amp;ak=#{在百度地图开放平台申请的ak}&quot;&gt;&lt;/script&gt; 开放平台地址 使用方式ECharts 将百度地图部分配置集成在了 bmap 中，包括： 参数 说明 格式 center 中心点的百度坐标 坐标数组, 如：[116.307698, 40.056975] zoom 初始缩放比 number roam 是否允许用户缩放操作 boolean mapStyle 地图自定义样式 object, 如：{ styleJson: […] } 自定义百度地图样式地图的样式配置 bmap.mapStyle 中 styleJson 与百度地图内置的样式配置一致，具体参考百度地图API开发指南中 定制个性地图章节的介绍。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566var option = &#123; bmap: &#123; center: [116.307698, 40.056975], zoom: 5, roam: true, // 允许缩放 mapStyle: &#123; // 百度地图自定义样式 styleJson: [ // 陆地 &#123; "featureType": "land", "elementType": "all", "stylers": &#123; "color": "#073763" &#125; &#125;, // 水系 &#123; "featureType": "water", "elementType": "all", "stylers": &#123; "color": "#073763", "lightness": -54 &#125; &#125;, // 国道与高速 &#123; "featureType": "highway", "elementType": "all", "stylers": &#123; "color": "#45818e" &#125; &#125;, // 边界线 &#123; "featureType": "boundary", "elementType": "all", "stylers": &#123; "color": "#ffffff", "lightness": -62, "visibility": "on" &#125; &#125;, // 行政标注 &#123; "featureType": "label", "elementType": "labels.text.fill", "stylers": &#123; "color": "#ffffff", "visibility": "on" &#125; &#125;, &#123; "featureType": "label", "elementType": "labels.text.stroke", "stylers": &#123; "color": "#444444", "visibility": "on" &#125; &#125; ] &#125; &#125;, ... &#125; 百度地图api除了上述四个配置，其他地图设置都可以通过 百度地图提供的API 实现 获取百度地图实例：123var bmap = bmapCharts.getModel().getComponent('bmap').getBMap(); // 百度地图实例bmap.addControl(new BMap.NavigationControl()); // 缩放控件bmap.addControl(new BMap.ScaleControl()); // 比例尺 var bmap = bmapCharts.getModel().getComponent(‘bmap’).getBMap(); // 百度地图实例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268&lt;!DOCTYPE html&gt;&#123;% load static %&#125;&lt;html&gt;&lt;body&gt; &lt;div id="echarts-records" style="width: 1000px;height:500px"&gt;&lt;/div&gt;&lt;!--引入百度地图的jssdk，这里需要使用你在百度地图开发者平台申请的 ak--&gt;&lt;script src="http://api.map.baidu.com/api?v=2.0&amp;ak=#&#123;在百度地图开放平台申请的ak&#125;"&gt;&lt;/script&gt;&lt;script src="&#123;% static 'js/echarts.js'%&#125;"&gt;&lt;/script&gt;&lt;!-- 引入百度地图扩展 --&gt;&lt;script src="&#123;% static 'js/bmap.js'%&#125;"&gt;&lt;/script&gt;&lt;script&gt; var echartsRecords = echarts.init(document.getElementById('echarts-records'), 'walden'); var data = []; var geoCoordMap = &#123;&#125;; var convertData = function (data) &#123; var res = []; for (var i = 0; i &lt; data.length; i++) &#123; var geoCoord = geoCoordMap[data[i].name]; if (geoCoord) &#123; res.push(&#123; name: data[i].name, value: geoCoord.concat(data[i].value[0].value) &#125;); &#125; &#125; return res; &#125;; option = &#123; backgroundColor: 'transparent', title: &#123; text: '数据统计', left: 'center', textStyle: &#123; color: '#fff' &#125; &#125;, tooltip : &#123; trigger: 'item', formatter: function(params, ticket, callback)&#123;//定制提示信息 var toolTiphtml = '' var point = new BMap.Point(params.data.value[0],params.data.value[1]); gc.getLocation(point, function (rs) &#123;//根据经纬度获取地址信息 var address = rs.address; for(var i = 0;i&lt;data.length;i++)&#123; if(params.name==data[i].name)&#123; toolTiphtml += address+':&lt;br&gt;' for(var j = 0;j&lt;data[i].value.length;j++)&#123; toolTiphtml+=data[i].value[j].name+': '+data[i].value[j].value+"&lt;br&gt;" &#125; callback(ticket, toolTiphtml);//异步回掉获取提示信息 &#125; &#125; &#125;); return 'loading';//内容还没返回时显示的内容 &#125; &#125;, bmap: &#123; center: [104.114129, 37.550339], //mapType: 'china', zoom: 5,// 百度地图缩放等级，数字越大，放大越大，地图比例尺越小 roam: 'move', // 是否开启拖拽缩放，可以只设置 'scale' 或者 'move' true mapStyle: &#123;//地图样式，参考https://developer.baidu.com/map/custom/ styleJson: [ &#123; "featureType": "water", "elementType": "all", "stylers": &#123; "color": "#044161" &#125; &#125;, &#123; "featureType": "land", "elementType": "all", "stylers": &#123; "color": "#004981" &#125; &#125;, &#123; "featureType": "boundary", "elementType": "geometry", "stylers": &#123; "color": "#064f85" &#125; &#125;, &#123; "featureType": "railway", "elementType": "all", "stylers": &#123; "visibility": "off" &#125; &#125;, &#123; "featureType": "highway", "elementType": "geometry", "stylers": &#123; "color": "#004981" &#125; &#125;, &#123; "featureType": "highway", "elementType": "geometry.fill", "stylers": &#123; "color": "#005b96", "lightness": 1 &#125; &#125;, &#123; "featureType": "highway", "elementType": "labels", "stylers": &#123; "visibility": "off" &#125; &#125;, &#123; "featureType": "arterial", "elementType": "geometry", "stylers": &#123; "color": "#004981" &#125; &#125;, &#123; "featureType": "arterial", "elementType": "geometry.fill", "stylers": &#123; "color": "#00508b" &#125; &#125;, &#123; "featureType": "poi", "elementType": "all", "stylers": &#123; "visibility": "off" &#125; &#125;, &#123; "featureType": "green", "elementType": "all", "stylers": &#123; "color": "#056197", "visibility": "off" &#125; &#125;, &#123; "featureType": "subway", "elementType": "all", "stylers": &#123; "visibility": "off" &#125; &#125;, &#123; "featureType": "manmade", "elementType": "all", "stylers": &#123; "visibility": "off" &#125; &#125;, &#123; "featureType": "local", "elementType": "all", "stylers": &#123; "visibility": "off" &#125; &#125;, &#123; "featureType": "arterial", "elementType": "labels", "stylers": &#123; "visibility": "off" &#125; &#125;, &#123; "featureType": "boundary", "elementType": "geometry.fill", "stylers": &#123; "color": "#029fd4" &#125; &#125;, &#123; "featureType": "building", "elementType": "all", "stylers": &#123; "color": "#1a5787" &#125; &#125;, &#123; "featureType": "label", "elementType": "all", "stylers": &#123; "visibility": "off" &#125; &#125; ] &#125; &#125;, series : [ &#123; name: 'Net capacity', //type: 'scatter', type: 'effectScatter', coordinateSystem: 'bmap', data: convertData(data), encode: &#123; value: 2 &#125;, symbolSize: function (val) &#123; return val[2] / 700; &#125;, label: &#123; formatter: '&#123;b&#125;', position: 'right', fontSize :20 &#125;, itemStyle: &#123; color: '#ddb926' &#125;, emphasis: &#123; label: &#123; show: true &#125; &#125; &#125;, &#123; type: 'custom', coordinateSystem: 'bmap', renderItem: renderItem, itemStyle: &#123; opacity: 0.5 &#125;, animation: false, silent: true, data: [0], z: -10 &#125; ] &#125;; echartsRecords.setOption(option); // 获取百度地图实例 var bmap = echartsRecords.getModel().getComponent('bmap').getBMap(); //使用百度地图自带的控件 //bmap.addControl(new BMap.MapTypeControl()); var top_left_control = new BMap.ScaleControl(&#123;anchor: BMAP_ANCHOR_TOP_LEFT&#125;);// 左上角，添加比例尺 var top_left_navigation = new BMap.NavigationControl(); //左上角，添加默认缩放平移控件 bmap.addControl(top_left_control) bmap.addControl(top_left_navigation) var gc = new BMap.Geocoder(); // echarts 窗口缩放自适应 //用于使chart自适应高度和宽度,通过窗体高宽计算容器高宽 var resizeEchartContainer = function () &#123; width = $("#echart_card").width(); height = $("#echart_card").height(); document.getElementById('echarts-records').style.width = width+'px'; document.getElementById('echarts-records').style.height = height+'px'; &#125;; window.onresize = function()&#123; resizeEchartContainer(); echartsRecords.resize(); &#125; resizeEchartContainer(); &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; echart底图加载方式以上使用的为第一种 底图加载方式 获取坐标json数据 根据经纬度获取详细地址地址解析服务123456789101112var map = new BMap.Map("l-map"); map.centerAndZoom(new BMap.Point(116.404, 39.915), 11); // 创建地址解析器实例 var myGeo = new BMap.Geocoder(); // 将地址解析结果显示在地图上，并调整地图视野 myGeo.getPoint("北京市海淀区上地10街10号", function(point)&#123; if (point) &#123; map.centerAndZoom(point, 16); map.addOverlay(new BMap.Marker(point)); &#125; &#125;, "北京市"); 逆地址解析服务指定经纬度获取地址12345678910var map = new BMap.Map("l-map"); map.centerAndZoom(new BMap.Point(116.404, 39.915), 11); // 创建地理编码实例, 并配置参数获取乡镇级数据var myGeo = new BMap.Geocoder(&#123;extensions_town: true&#125;); // 根据坐标得到地址描述 myGeo.getLocation(new BMap.Point(116.364, 39.993), function(result)&#123; if (result)&#123; alert(result.address); &#125; &#125;); 鼠标点击地图获取地址1234567891011var map = new BMap.Map("allmap");var point = new BMap.Point(116.331398,39.897445);map.centerAndZoom(point,12);var geoc = new BMap.Geocoder(); map.addEventListener("click", function(e)&#123; var pt = e.point; geoc.getLocation(pt, function(rs)&#123; var addComp = rs.addressComponents; alert(addComp.province + ", " + addComp.city + ", " + addComp.district + ", " + addComp.street + ", " + addComp.streetNumber); &#125;); &#125;); 官方文档 参考https://www.cnblogs.com/hao-1234-1234/p/10954979.html https://blog.csdn.net/qq_35736779/article/details/99772172 https://juejin.im/post/6844904160945504269 ###]]></content>
      <categories>
        <category>echart</category>
      </categories>
      <tags>
        <tag>-echart -百度地图</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vue-cli4快速搭建项目详解]]></title>
    <url>%2F2020%2F07%2F28%2Fvue-cli4%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA%E9%A1%B9%E7%9B%AE%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[安装卸载旧版本,并安装vue-cli4123npm uninstall vue-cli -gnpm install -g @vue/cli //vue-cli2.x的安装命令是 cnpm install -g vue-clivue -V //校验是否安装成功 如图，表示安装成功 配置创建项目vue create 项目名 选择配置 default：默认配置（只有babel和eslint）Manually select features：手动配置一般做项目默认配置是不够的，我们选第二项。 进行配置 选项 解释 Babel 一种能让浏览器自动识别向后兼容各版本JavaScript的功能（选） TypeScript 一种.ts后缀兼容js的语法（不选） Progressive Web App（PWA）Support 渐进式网络应用（不选） Router vue的路由管理组件（选） Vuex vue的状态仓库管理组件（选） CSS Pre-processors CSS预编译（选） Linter/Formatter 代码检验 格式检查（选） Unit Testing 单元测试 以开发角度测试代码（不选） E2E Testing e2e测试 以用户角度测试代码（不选） 选择router模式 history和hash是router组件的mode选项，一般默认用history更普遍和实用。 具体可以参考：https://segmentfault.com/q/1010000010340823 选择CSS预编译方式 这里看你个人习惯，我经常用Sass/Scss（with dart-sass） node-sass和dart-sass区别：https://www.dart-china.org/t/topic/146 https://www.sasscss.com/ 选择代码校验方式 我习惯选ESLint with error prevention only或者最后一项ESLint+Prettier。如果是用VScode的小伙伴推荐最后一项，与VScode里的Prettier插件配合不错。 选择代码检查时间点 选择第一项Lint on save，在保存后就自动检验代码和格式. 询问配置保存位置 询问是否将配置放在package.json文件中。推荐选第一项In dedicated config files，创建新的配置文件。 如果是选择 独立文件放置，项目会有单独如下图所示的几件文件。 保存为预配置 询问是否将以上配置保存为预配置。这个根据自己意愿，预配置对以后创建项目来说还是方便点的，选择“Y”or“N”。如果“Y”，就继续设置预配置名称；如果“N”就自动跳过。 开始创建 回车开始自动创建项目，需要一定时间，创建成功结束后可以看到以下项目结构！ 启动项目控制台输入如下命令启动项目 12cd my-project // 进入到项目根目录npm run serve // 启动项目]]></content>
      <categories>
        <category>前端</category>
      </categories>
      <tags>
        <tag>vue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot异常处理]]></title>
    <url>%2F2020%2F06%2F21%2FSpringBoot%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[统一异常处理 http://www.gxitsky.com/2019/01/30/springboot-app-38-global-exception-handle/ http://www.zhaojun.im/springboot-exception/ https://www.jianshu.com/p/9792846ee029 https://www.cnblogs.com/harrychinese/p/SpringBoot_exception_handler_best_practice.html]]></content>
      <categories>
        <category>exception</category>
      </categories>
      <tags>
        <tag>springboot</tag>
        <tag>exception</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringSecurity]]></title>
    <url>%2F2020%2F06%2F21%2FSpringSecurity%2F</url>
    <content type="text"><![CDATA[解决SpringSecurity手动退出登录后再次登录成功会重定向到登录界面的问题(https://www.jianshu.com/p/4a5061951c77) .and().logoutSuccessUrl(&quot;/&quot;) 设置session超时退出(https://www.cnblogs.com/zyly/p/12316099.html) 1server.servlet.session.timeout=1800 12//session管理,失效后跳转http.sessionManagement().invalidSessionUrl("/login"); SpringBoot中SpringSecurity 中不能抛出异常UserNameNotFoundException 问题解析与处理 https://juejin.im/post/5c432efce51d4551e653a77e https://www.jianshu.com/p/4a7654fa0bd3 https://blog.csdn.net/sun1021873926/article/details/60332059 https://www.jianshu.com/p/5714777114b1 12345678@Beanpublic DaoAuthenticationProvider authenticationProvider() &#123; DaoAuthenticationProvider provider = new DaoAuthenticationProvider(); provider.setHideUserNotFoundExceptions(false); provider.setUserDetailsService(mUserDetailsService); provider.setPasswordEncoder(passwordEncoder); return provider;&#125; 添加验证码 https://www.jianshu.com/p/5a83e364869c Spring Security整合thymeleaf https://www.jianshu.com/p/953c8998e2bd 源码分析 https://segmentfault.com/a/1190000018616620 https://blog.csdn.net/u012702547/article/details/89629415 https://www.cnblogs.com/crazymakercircle/p/12040402.html]]></content>
      <categories>
        <category>SpringSecurity</category>
      </categories>
      <tags>
        <tag>SpringSecurity</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[thymeleaf]]></title>
    <url>%2F2020%2F06%2F21%2Fthymeleaf%2F</url>
    <content type="text"><![CDATA[判断对象是否为空 使用 ${xxx?.xxx?} 可以判断对象是否为空，? 号 前的对象或者属性为空那么就不再进行渲染，而且不会导致出现报错的情况]]></content>
      <categories>
        <category>thymeleaf</category>
      </categories>
      <tags>
        <tag>thymeleaf</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[idea热加载配置]]></title>
    <url>%2F2020%2F06%2F07%2Fidea%E7%83%AD%E5%8A%A0%E8%BD%BD%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[热加载方式 JRebel 收费，有点贵 Spring-Loader 不稳定 DCEVM+Hotswap Agent 免费（当然选择这种） DCEVM+Hotswap Agent下载DCEVMlatest release of DCEVM Java patch sudo java -jar DCEVM-8u181-installer-build2.jar 选择jdk安装目录，并点击”Install DCEVM as altjvm”按钮 下载Hotswap agent jarlatest release of Hotswap agent jar 放到任意目录 安装idea插件 HotSwapAgent 配置插件 以debug模式启动应用程序 测试修改代码或者添加新方法，点击编译 更多信息DCEVM &amp; HotswapAgent参考https://github.com/dmitry-zhuravlev/hotswap-agent-intellij-plugin]]></content>
      <categories>
        <category>idea</category>
      </categories>
      <tags>
        <tag>idea</tag>
        <tag>热加载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java8学习]]></title>
    <url>%2F2020%2F05%2F30%2Fjava8%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[函数式 函数式接口就是只定义一个抽象方法的接口 函数式接口的抽象方法的签名称为函数描述符。 Predicate:java.util.function.Predicate接口定义了一个名叫test的抽象方法，它接受泛型T对象，并返回一个boolean。 Consumer:java.util.function.Consumer接口定义了一个名叫accept的抽象方法，它接受泛型T的对象，没有返回（void）。 Function:java.util.function.Function&lt;T, R&gt;接口定义了一个叫作apply的抽象方法，它接受泛型T的对象，并返回一个泛型R的对象。 DoublePredicate、IntConsumer、LongBinaryOperator、IntFunction、ToIntFunction、IntToDoubleFunction 流常用方法 filter 筛选 distinct 去重 （据流所生成元素的hashCode和equals方法实现） 切片 takeWhile 遭遇第一个不符合要求的元素时停止处理 dropWhile 它会从头开始，丢弃所有谓词结果为false的元素。一旦遭遇谓词计算的结果为true，它就停止处理，并返回所有剩余的元素，即便要处理的对象是一个由无限数量元素构成的流，它也能工作得很好 limit 截短流 skip 跳过元素 map 映射 （使用映射一词，是因为它和转换类似，但其中的细微差别在于它是“创建一个新版本”而不是去“修改”） flatMap 扁平化为一个流，把一个流中的每个值都换成另一个流，然后把所有的流连接起来成为一个流 allMatch 匹配所有元素 anyMatch 至少匹配一个 noneMatch 没有任何匹配元素 findAny 返回当前流中的任意元素 findFist 查找第一个元素 原始类型流特化Java 8引入了三个原始类型特化流接口来解决这个问题：IntStream、DoubleStream和LongStream，分别将流中的元素特化为int、long和double，从而避免了暗含的装箱成本。 常用方法mapToInt、mapToDouble和mapToLong 转换回对象流boxed 数值范围 range 不包含结束值 rangeClosed 包含结束值 构建流由值创建流12Stream&lt;String&gt; java = Stream.of("java", "php", "js");Stream&lt;Object&gt; empty = Stream.empty(); //创建空流 由数组创建流12int[] numbers = &#123;2,3,4,5,11,23&#125;;IntStream stream = Arrays.stream(numbers); 由文件生成流由函数生成无限流 Stream.iterate Stream.generate ​]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python学习]]></title>
    <url>%2F2020%2F05%2F15%2Fpython%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[if__name__==&#39;__main__&#39; 在学习Python基础语法的时候，在程序最后经常会遇到这句话，这里简单解释下它的意义。总地来说，这句代码的作用是既能保证当前的.py文件直接运行，也能保证其可以作为模块被其他.py文件导入。 12345def getName(): print(__name__)if __name__ == '__main__': getName()//输出__main__ 这说明，__name__本身是一个变量，但它不是一般的变量。实际上，它是在程序执行前就创建并赋值的，而赋值的机制是这里的关键。在当前程序被当作主程序被执行的时候，__name__自动被赋值为固定的字符串__main__；当它作为模块被其他文件调用的时候，自动被赋值为模块所在的文件名。]]></content>
  </entry>
  <entry>
    <title><![CDATA[java8时间API]]></title>
    <url>%2F2020%2F04%2F30%2Fjava8%E6%97%B6%E9%97%B4API%2F</url>
    <content type="text"><![CDATA[Java8之前的日期时间API表示时刻信息的 DateDate的设计饱受诟病，其缺陷包括但不限于： 类名误导，该类实际上不仅反映日期，还反映时间 方法名误导，getDate()返回日期中的天，getDay()返回的是周几 年份是与1900年的差值，可读性极差 月份是从0计数的，可读性极差 周几是相对于周日的差值，可读性极差 不提供时区设置，内部总是使用本地时区 不提供历法设置，内部使用格里历或儒略历 不提供格式化的转换，从字符串中解析日期时相当难用 参数返回太随意，比如设置1月33日，实际是2月2日 存在同名类，java.sql包下依然有一个作用相同的Date类 该类允许扩展，实际上，应当把日期-时间类设计为不可变的final类 当前定位Date 的目前定位是，唯一表示一个时刻，现在的 Date 类中接近百分之八十的方法都已废弃，被标记为 @Deprecated。还有几个为数不多没有被废弃的方法： public long getTime()：返回内部存储的毫秒数 public void setTime(long time)：重新设置内存的毫秒数 public boolean before(Date when)：比较给定的时刻是否早于当前 Date 实例 public boolean after(Date when)：比较给定的时刻是否晚于当前 Date 实例 描述年历的 CalendarCalendar 用于表示年月日等日期信息，它是一个抽象类，一般通过以下四种工厂方法获取它的实例对象: 123456public static Calendar getInstance()public static Calendar getInstance(TimeZone zone)public static Calendar getInstance(Locale aLocale)public static Calendar getInstance(TimeZone zone,Locale aLocale)//最终调用的方法，因为不同的时区与国家语言对于时刻和年月日信息的输出是不同的，所以这也是为什么一个 Calendar 实例必须传入时区和国家信息的一个原因private static Calendar createCalendar(TimeZone zone,Locale aLocale) DateFormat 格式化转换12345DateFormat dateFormat = new SimpleDateFormat("yyyy年MM月dd日");//将一个日期对象格式化为字符串public final String format(Date date)//将一个格式化的字符串装换为一个日期对象public Date parse(String source) yyyy：年份用四位进行输出 MM：月份用两位进行输出 dd：两位表示日信息 HH：两位来表示小时数 mm：两位表示分钟数 ss：两位来表示秒数 E：表示周几，如果 Locale 在中国则会输出 星期x，如果在美国或英国则会输出英文的星期 a：表示上午或下午 Java8 的时间日期 APIJava 8的日期和时间类包括Instant、Duration、Period、LocalDate、LocalTime、LocalDateTime，这些类都包含在java.time包中。 表示时刻的 InstantInstant是时间线上的一个点，表示一个时间戳。Instant可以精确到纳秒，这超过了long的最大表示范围，所以在Instant的实现中是分成了两部分来表示，一部分是seconds，表示从1970-01-01 00:00:00开始到现在的秒数，另一个部分是nanos，表示纳秒部分。 123456//根据系统当前时间创建一个 Instant 实例，表示当前时刻Instant now = Instant.now(); //通过传入一个标准时间的偏移值来构建一个 Instant 实例Instant instant = Instant.ofEpochSecond(long epochSecond, long nanoAdjustment);//通过毫秒数值直接构建一个 Instant 实例public static Instant ofEpochMilli(long epochMilli)； 时间段DurationDuration是两个时间戳的差值，使用java.time中的时间戳类，例如Instant、LocalDateTime等实现了Temporal类的日期时间类为参数 123456LocalDateTime from = LocalDateTime.of(2020,4, 22, 16, 6, 0);LocalDateTime to = LocalDateTime.of(2020,5, 22, 16, 6, 0);Duration duration = Duration.between(from, to);Duration duration1 = Duration.of(5, ChronoUnit.DAYS); // 5天Duration duration2 = Duration.of(1000, ChronoUnit.MILLIS); // 1000毫秒 日期段PeriodPeriod是以年月日来衡量一个时间段，由于Period是以年月日衡量时间段，所以between()方法只能接收LocalDate类型的参数 12// 2020-03-22 到 2020-04-22 这段时间Period period = Period.between(LocalDate.of(2020, 3, 22),LocalDate.of(2020, 4, 22)); 处理日期的 LocalDateLocalDate 是一个不可变类，它关注时间中年月日部分。 12345678910111213141516//用当前系统时间的年月日信息初始化一个实例对象public static LocalDate now();//显式指定年月日信息public static LocalDate of(int year, int month, int dayOfMonth);//根据 dayOfYear 可以推出 month 和 dayOfMonthpublic static LocalDate ofYearDay(int year, int dayOfYear);//相对于格林零时区时间的日偏移量public static LocalDate ofEpochDay(long epochDay);LocalDate localDate = LocalDate.of(2020, 4, 22); // 初始化一个日期：2022-04-22int year = localDate.getYear(); // 年份：2020Month month = localDate.getMonth(); // 月份：MARCHint dayOfMonth = localDate.getDayOfMonth(); // 月份中的第几天：22DayOfWeek dayOfWeek = localDate.getDayOfWeek(); // 一周的第几天：FRIDAYint length = localDate.lengthOfMonth(); // 月份的天数：30boolean leapYear = localDate.isLeapYear(); // 是否为闰年：true 处理时间的 LocalTime类似于 LocalDate，LocalTime 专注于时间的处理 1234567891011121314151617//根据系统当前时刻获取其中的时间部分内容public static LocalTime now();//显式传入小时和分钟来构建一个实例对象public static LocalTime of(int hour, int minute);//通过传入时分秒构造实例public static LocalTime of(int hour, int minute, int second);//传入时分秒和纳秒构建一个实例public static LocalTime of(int hour, int minute, int second, int nanoOfSecond);//传入一个长整型数值代表当前日已经过去的秒数public static LocalTime ofSecondOfDay(long secondOfDay);//传入一个长整型代表当前日已经过去的纳秒数public static LocalTime ofNanoOfDay(long nanoOfDay);LocalTime localTime = LocalTime.of(16, 25, 52); // 初始化一个时间：16:25:52int hour = localTime.getHour(); // 时：16int minute = localTime.getMinute(); // 分：25int second = localTime.getSecond(); // 秒：52 处理日期和时间的 LocalDateTimeLocalDateTime类是LocalDate和LocalTime的结合体 12345678910//通过of()方法直接创建LocalDateTime ldt1 = LocalDateTime.of(2020, Month.FEBRUARY, 22, 16, 23, 12);//可以调用LocalDate的atTime()方法或LocalTime的atDate()方法将LocalDate或LocalTime合并成一个LocalDateTimeLocalDate localDate = LocalDate.of(2020, Month.FEBRUARY, 22);LocalTime localTime = LocalTime.of(16, 23, 12);LocalDateTime ldt2 = localDate.atTime(localTime);//向LocalDate和LocalTime的转化LocalDate date = ldt1.toLocalDate();LocalTime time = ldt1.toLocalTime(); 日期操作比较简单的日期操作 12345678//比较简单的日期操作LocalDate date = LocalDate.of(2020, 4, 22); // 2020-04-22LocalDate date1 = date.withYear(2021); // 修改为 2021-04-22LocalDate date2 = date.withMonth(3); // 修改为 2020-03-22LocalDate date3 = date.withDayOfMonth(1); // 修改为 2020-04-01LocalDate date4 = date.plusYears(1); // 增加一年 2021-04-22LocalDate date5 = date.minusMonths(2); // 减少两个月，到2020年的2月LocalDate date6 = date.plus(5, ChronoUnit.DAYS); // 增加5天 2020-04-27 比较复杂的日期操作，比如将时间调到下一个工作日，或者是下个月的最后一天，这时候我们可以使用with()方法的另一个重载方法，它接收一个TemporalAdjuster参数，可以使我们更加灵活的调整日期： 1234//返回下一个距离当前时间最近的星期日LocalDate date7 = date.with(TemporalAdjusters.nextOrSame(DayOfWeek.SUNDAY));// 返回本月最后衣蛾周六LocalDate date9 = date.with(TemporalAdjusters.lastInMonth(DayOfWeek.SATURDAY)); 下面列出时间调节器类TemporalAdjuster提供的一些方法 方法名 描述 dayOfWeekInMonth 返回同一个月中每周的第几天 firstDayOfMonth 返回当月的第一天 firstDayOfNextMonth 返回下月的第一天 firstDayOfNextYear 返回下一年的第一天 firstDayOfYear 返回本年的第一天 firstInMonth 返回同一个月中第一个星期几 lastDayOfMonth 返回当月的最后一天 lastDayOfNextMonth 返回下月的最后一天 lastDayOfNextYear 返回下一年的最后一天 lastDayOfYear 返回本年的最后一天 lastInMonth 返回同一个月中最后一个星期几 next / previous 返回后一个/前一个给定的星期几 nextOrSame / previousOrSame 返回后一个/前一个给定的星期几，如果这个值满足条件，直接返回 时区ZoneId123456//接收一个“区域/城市”的字符串作为参数ZoneId shanghaiZoneId = ZoneId.of("Asia/Shanghai");//获取系统默认时区ZoneId systemZoneId = ZoneId.systemDefault();//获取所有合法的“区域/城市”字符串Set&lt;String&gt; zoneIds = ZoneId.getAvailableZoneIds(); ZonedDateTimeZonedDateTime 可以被理解为 LocalDateTime 的外层封装，它的内部存储了一个 LocalDateTime 的实例，专门用于普通的日期时间处理。此外，它还定义了 ZoneId 和 ZoneOffset 来描述时区的概念。 12345678910111213//系统将以默认时区计算并存储日期时间信息public static ZonedDateTime now();//指定时区public static ZonedDateTime now(ZoneId zone);//指定日期时间和时区public static ZonedDateTime of(LocalDate date, LocalTime time, ZoneId zone)：public static ZonedDateTime of(LocalDateTime localDateTime, ZoneId zone)//通过时刻和时区构建实例对象public static ZonedDateTime ofInstant(Instant instant, ZoneId zone)：//有了ZoneId，我们就可以将一个LocalDate、LocalTime或LocalDateTime对象转化为ZonedDateTime对象：LocalDateTime localDateTime = LocalDateTime.now();ZonedDateTime zonedDateTime = ZonedDateTime.of(localDateTime, shanghaiZoneId); 格式化日期时间DateTimeFormatter 作为格式化日期时间的主要类，它与之前的 DateFormat 类最大的不同就在于它是线程安全的，其他的使用上的操作基本类似。 1234567DateTimeFormatter formatter = DateTimeFormatter.ofPattern("yyyy年MM月dd日 HH:mm:ss");LocalDateTime localDateTime = LocalDateTime.now();formatter.format(localDateTime);String str = "2020年04月23日 23:59:59";DateTimeFormatter formatter2 = DateTimeFormatter.ofPattern("yyyy年MM月dd日 HH:mm:ss");LocalDateTime localDateTime2 = LocalDateTime.parse(str,formatter2); 世界标准时间（UTC）/格林威治时间（GMT）UTC是协调世界时(Universal Time Coordinated)英文缩写，是由国际无线电咨询委员会规定和推荐,并由国际时间局(BIH)负责保持的以秒为基础的时间标度。UTC相当于本初子午线(即经度0度)上的平均太阳时，过去曾用格林威治平均时(GMT)来表示.北京时间比UTC时间早8小时，以1999年1月1日0000UTC为例，UTC时间是零点，北京时间为1999年1月1日早上8点整。 具体参考：https://pansci.asia/archives/84978 参考https://www.cnblogs.com/mcbye/p/java8-date-time-api.html https://juejin.im/post/5addc7a66fb9a07aa43bd2a0#heading-5 https://www.jianshu.com/p/68d8291c3bd5]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java8 时间</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ideaVim安装配置]]></title>
    <url>%2F2020%2F04%2F29%2FideaVim%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[安装和安装其它插件一样，直接在插件市场搜索ideaVim安装即可。 配置快捷键设置ideaViim键与idea快捷键有冲突，可以在Editor Vim Emulation设置，我的快捷键设置如下： 常用快捷键 zo:打开折叠 zc:关闭折叠 .ideavim配置文件讲解逻辑开发过idea插件的人都知道，在idea中，任何目录选项上的点击操作，工具栏上的按钮都会被映射为一个action。即点击对应的按钮或者选项，执行相应的action。理解很简单，这就是MVC的思想，比如springmvc， 一个访问路径映射为一个控制器中的方法这样子。 明白action映射这一点很重要。因为ideavim的插件支持使用vim命令执行对应的action。所以，这个设计逻辑就打通了vim和idea的交互，使得我们可以在vim中完成所有和idea相关的操作。 常规操作vim的定位是文本编辑。所以在idea中一般情况下vim快捷键和命令生效的基本条件就是当前窗口焦点在编辑区。这是最基础的条件，如果你当前的快键键在Project或者Run这些侧边栏中，按vim的命令肯定是不会生效的。 明白了基本条件，那我们要做的第一件事也是最重要的事情就是在idea中，如何快速的将焦点回到的编辑区。答案是：Esc 这应该是使用vim的人最能接受的快捷键了。所以，到这里基本的操作思路就很明确，当前的焦点要么在编辑区外，要么在编辑区内。而返回编辑区的操作很简单，就是Esc。当焦点在编辑区的时候，就可以随心所欲的使用vim的指令和快捷键了。 配置有了idea基本的了解之后，我们就可以着手配置自己的vim了。ideavim插件的配置通常是~/.ideavimrc，所以将自己的配置脚本写在对应的文件中即可。当然如果你已经有了自己的vim配置文件在~/.vimrc下，那只需要在ideavim的配置文件中添加一行命令即可全部读取映射过来 1source ~/.vimrc 在~目录下，创建.ideavimrc 1234567891011"设置高亮搜索set hlsearch"重命名文件nnoremap &lt;Space&gt;rf :action RenameFile&lt;CR&gt;"跳到指定类nnoremap &lt;Space&gt;gc :action GotoClass&lt;CR&gt;"查找使用nnoremap &lt;Space&gt;fu :action FindUsages&lt;CR&gt;"跳转到方法的声明nnoremap &lt;Space&gt;gs :action GotoSuperMethod&lt;CR&gt; 在vim中执行idea的action为了执行idea中的action，ideavim有两个额外的命令来支持这一操作。 :actionlist [pattern] :action {actionName} 第一个命令actionlist是帮助查看idea中有哪些命令。第二个命令是执行对应的action这样子。有了这两个命令的帮助我们就可以在ideavim的配置文件中使用map命令来映射idea的action到vim的快捷键中。 Vim 中的 remap，noremapremap是一个使映射以递归方式工作的选项 。 :map和:noremap是各种映射命令的递归和非递归版本。这意味着，如果你这样做： 123:map j gg:map Q j:noremap W j j将映射到gg 。 Q 也将映射到gg ，因为j将被扩展用于递归映射。 W将被映射到j （而不是gg ），因为j不会针对非递归映射进行扩展。 现在请记住，Vim 是一个模态编辑器 。它具有普通模式， 可视模式和其他模式。 对于这些映射集中的每一个，都有一个映射#Creating_keymaps)可以在正常，可视，选择和操作员模式下工作（ :map和:noremap ），一个在普通模式下工作（ :nmap和:nnoremap ），一个在可视模式下（ :vmap和:vnoremap ）等等。 参考https://programtip.com/zh/art-127704 https://blog.csdn.net/qq_40250122/article/details/102054921 https://my.oschina.net/funcy/blog/1832719 https://blog.csdn.net/CoderBruis/article/details/94735816 https://routinepanic.com/questions/what-is-the-difference-between-the-remap-noremap-nnoremap-and-vnoremap-mapping]]></content>
      <categories>
        <category>idea</category>
      </categories>
      <tags>
        <tag>idea</tag>
        <tag>vim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELK安装配置]]></title>
    <url>%2F2020%2F04%2F16%2FELK%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[ELK常见架构Elasticsearch + Logstash + Kibana这是一种最简单的架构。这种架构，通过logstash收集日志，Elasticsearch分析日志，然后在Kibana(web界面)中展示。这种架构虽然是官网介绍里的方式，但是往往在生产中很少使用。 Elasticsearch + Logstash + filebeat + Kibana与上一种架构相比，这种架构增加了一个filebeat模块。filebeat是一个轻量的日志收集代理，用来部署在客户端，优势是消耗非常少的资源(较logstash)， 所以生产中，往往会采取这种架构方式，但是这种架构有一个缺点，当logstash出现故障， 会造成日志的丢失。 Elasticsearch + Logstash + filebeat + redis(也可以是其他中间件，比如kafka) + Kibana这种架构是上面那个架构的完善版，通过增加中间件，来避免数据的丢失。当Logstash出现故障，日志还是存在中间件中，当Logstash再次启动，则会读取中间件中积压的日志。 filebeat安装部署 简介Filebeat是一个日志文件托运工具，在你的服务器上安装客户端后，filebeat会监控日志目录或者指定的日志文件，追踪读取这些文件（追踪文件的变化，不停的读），并且转发这些信息到elasticsearch或者logstarsh、redis中存放。 工作原理filebeat由2个主要组件构成：prospector和harvesters。这两类组件一起协同完成Filebeat的工作，从指定文件中把数据读取出来，然后发送事件数据到配置的output中。 harvesters：主要负责进行单个文件的内容收集；在运行过程中，每一个Harvester会对一个文件逐行进行内容读取，并且把读写到的内容发送到配置的output中。 Prospector负责管理Harvsters，并且找到所有需要进行读取的数据源。如果input type配置的是log类，Prospector将会去配置度路径下查找所有能匹配上的文件，然后为每一个文件创建一个Harvster。每个Prospector都运行在自己的Go routine里。 当你开启filebeat程序的时候，它会启动一个或多个探测器（prospectors）去检测你指定的日志目录或文件，对于探测器找出的每一个日志文件，filebeat启动收割进程（harvester），每一个收割进程读取一个日志文件的新内容，并发送这些新的日志数据到处理程序（spooler），处理程序会集合这些事件，最后filebeat会发送集合的数据到你指定的地点。 下载12curl -L -O https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-5.2.2-x86_64.rpmsudo rpm -vi filebeat-5.2.2-x86_64.rpm 配置12345678910111213141516171819#=========================== Filebeat prospectors =============================#filebeat.prospectors模块用来指定日志文件的来源。filebeat.prospectors:- input_type: log #input_type 指定日志类型，在这里是log， 应该也可以是json enabled: true paths: #paths指定日志文件路径。 - /root/myapp/tomcat/tomcat8/logs/*.log#================================ redis =====================================output.redis: hosts: ["152.136.233.203:6379"] password: "redis" datatype: list key: "tomcat-log" db: 1#document_type：这个字段是用来给日志打标记的。 #fields: 也是打标记，主要为了后面日志分析查找的方便，存储的时候也会根据fields分类存储，相同fields的数据存在同一个redis key中#fields_under_root: 如果该选项设置为true， 则该fields会存储在top-level中。#tail_files: 这个选项如果设置为true，则读取日志文件的新内容，而忽略原有的内容，一般要设置为true 启动1systemctl start filebeat 问题 filebeat怎么设置从头开始读取 找到registry文件的位置，如果没有单独配置那么文件路径为/var/lib/filebeat/registry，不在也没关心，可以直接find命令查找 1find / -name registry 关闭filebeat –&gt; 删掉registry文件 –&gt; 启动filebeat Filebeat插件启动失败，不能直接查找报错原因 老是在filebeat启动的这一步骤上出错，但是由于filebeat是由systemd启动的，因此原因也经常查不清楚，因此并不能直观的查出错误在哪里，所以今天教给大家两个寻找错误的根源的方法: 直接使用filebeat的启动方法，而不使用systemctl start filebeat来启动。比如： 1/usr/share/filebeat/bin/filebeat -c /etc/filebeat/filebeat.yml -path.home /usr/share/filebeat -path.config /etc/filebeat -path.data /var/lib/filebeat -path.logs /var/log/filebeat logstash 安装配置下载12wget https://artifacts.elastic.co/downloads/logstash/logstash-5.2.2.tar.gztar xf logstash-5.2.0.tar.gz -C /opt/app/ 配置1234mkdir -p /data/ls-data #创建/data/ls-data目录，用于logstash数据的存放chown -R logstash:logstash /data/ls-data #修改该目录的拥有者为logstashmkdir -p /log/ls-log #创建/data/ls-log目录，用于logstash日志的存放chown -R logstash:logstash /log/ls-log #修改该目录的拥有者为logstash 创建配置文件12mkdir -p /config/logstash/config.dvim logstash.conf 1234567891011121314151617181920input &#123; redis &#123; host =&gt; "localhost" port =&gt; "6379" db =&gt; "1" data_type =&gt; "list" key =&gt; "tomcat-log" codec =&gt; plain &#123; charset =&gt; "UTF-8" &#125; &#125;&#125;output &#123; elasticsearch &#123; hosts =&gt; ["localhost:9200"] index =&gt; "tomcat-log" &#125; stdout &#123; codec =&gt; rubydebug &#125;&#125; input是redis， 需要指定redis的host 和port以及db，还要指明数据的类型，list表示这是一个redis的list对象。key指明redis中的key名称。 output 是elasticsearch， hosts指明elasticsearch的ip和端口，index指明这个日志存在elasticsearch中的索引名称。 修改配置文件(/opt/logstash-5.2.2/config/logstash.yml)123456# 设置数据的存储路径为/data/ls-datapath.data: /data/ls-data# 设置管道配置文件路径为/etc/logstash/conf.dpath.config: /etc/logstash/conf.d# 设置日志文件的存储路径为/log/ls-logpath.logs: /log/ls-log 测试logstash12./logstash -f /config/logstash/config.d/logstash.conf --config.test_and_exit#--config.test_and_exit表示，检查测试创建的logstash.conf配置文件，是否有问题，如果没有问题，执行之后，显示Configuration OK 证明配置成功！ 启动1nohup /usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/logstash.conf &amp; elasticsearch安装下载12curl -L -O https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.2.2.tar.gztar -xvf elasticsearch-5.2.2.tar.gz -C /opt/app/ 配置1234mkdir -p /data/es-data #创建/data/es-data目录，用于elasticsearch数据的存放chown -R elasticsearch:elasticsearch /data/es-data #修改该目录的拥有者为elasticsearchmkdir -p /log/es-log #创建/data/es-log目录，用于elasticsearch日志的存放chown -R elasticsearch:elasticsearch /log/es-log #修改该目录的拥有者为elasticsearch 修改配置文件 (/opt/elasticsearch-5.2.2/config/elasticsearch.yml)1234567891011121314#设置data存放的路径为/data/es-datapath.data: /data/es-data#设置logs日志的路径为/log/es-logpath.logs: /log/es-log#设置内存不使用交换分区bootstrap.memory_lock: false#配置了bootstrap.memory_lock为true时反而会引发9200不会被监听，原因不明#设置允许所有ip可以连接该elasticsearchnetwork.host: 0.0.0.0#开启监听的端口为9200http.port: 9200#增加新的参数，为了让elasticsearch-head插件可以访问es (5.x版本，如果没有可以自己手动加)http.cors.enabled: truehttp.cors.allow-origin: "*" 启动注意，如果你使用root用户启动elasticsearch，就会报错，启动失败，这是因为elasticsearch不允许用root用户启动。可以创建一个用户，用来启动elasticsearch 1234groupadd elasticsearch #添加组useradd -g elasticsearch elasticsearch #添加用户chown -R elasticsearch:elasticsearch /opt/app/elasticsearch-5.2.2/ #设置权限/opt/app/elasticsearch-5.2.2/bin/elasticsearch -d #启动，后台运行 kibana安装配置安装12wget https://artifacts.elastic.co/downloads/kibana/kibana-5.2.2-linux-x86_64.tar.gztar -xzf kibana-5.2.2-linux-x86_64.tar.gz -C /opt/app/ 配置(/opt/app/kibana-5.2.2-linux-x86_64/config/kibana.yml)123server.host: "0.0.0.0" #指明服务运行的地址elasticsearch.url: "http://localhost:9200" #指明elasticsearch运行的地址和端口kibana.index: ".kibana" #指明kibana使用的索引，这个是自定义的。 启动1/opt/app/kibana-5.2.2-linux-x86_64/bin/kibana 参考https://www.jianshu.com/p/e7362ccfe7e3 http://www.justdojava.com/2019/08/11/elk-install/ https://blog.51cto.com/liqingbiao/2177873 https://www.cnblogs.com/FengGeBlog/p/10644170.html]]></content>
      <categories>
        <category>日志</category>
      </categories>
      <tags>
        <tag>Elasticsearch Logstash Kibana</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[类图]]></title>
    <url>%2F2020%2F04%2F06%2F%E7%B1%BB%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[集合类图 线程 IO字节输入流 FilterInputStream剖析这个类的特殊之处，就是包含了一个InputStream，使得可以在这个InputStream基础上进行多种封装，从而达到装饰的目的。 装饰者模式，顾名思义，是对原有类进行了一定的装饰，装饰后的类必须和原有的类拥有相同的方法，当然，可以在原有类的基础上进行扩展。 这里的装饰者模式通过包含一个原有的Inputstream对象，并且将InputStream原有的方法或直接暴露，或进行装饰后暴露，又或者添加了新的特性，如DataInputStream中的readInt()，BufferedInputStream中的缓存功能。 为什么InputStream选择装饰者模式，而非直接继承的方法来扩展，这就是装饰者模式VS继承。如果单纯的使用继承，就会造成类的“爆炸”式增长。 直接使用继承，可以实现“目的”和“方法”，但是每一种来源的输入流，都需要改善流读取方法，因此在使用继承时，每一个InputStream的子类都需要DataInputStream，BufferedInputStream这几个类提供的“装饰作用”的功能，因此需要的类的数目就是A*B的数目。 而直接使用装饰者模式，将InputStream的几个直接子类进一步抽象，在此基础上提供装饰作用，所需要的类的数目是A+B。使用装饰者模式使得java类的更有层次性，类的数目得到充分控制。这就是装饰者模式相比于继承的优势。 字节输出流 字符输入流 字符输入流 参考https://blog.csdn.net/zhao123h/java/article/details/52826682 https://blog.csdn.net/zhoupenglei/article/details/46312405]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>类图 java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jvm性能监控与调优]]></title>
    <url>%2F2020%2F03%2F23%2Fjvm%E6%80%A7%E8%83%BD%E7%9B%91%E6%8E%A7%E4%B8%8E%E8%B0%83%E4%BC%98%2F</url>
    <content type="text"><![CDATA[jvm参数类型 标准参数(jvm各个版本基本不变) 123456-help -server -client -version -cp -classpath X参数（非标准化参数，变化比较小） 123-Xint 解释执行-Xcomp 第一次使用就编译成本地代码-Xmixed 混合模式，JVM自己来决定是否编译成本地代码 XX参数（用的比较多，主要用来JVM调优和debug） Boolean类型 格式：-XX:[+-]表示启用或者禁用name属性 12-XX:+UseConcMarkSweepGC-XX:+UseG1GC 非Boolean类型 格式：-XX:= 表示name属性的值是value 123456-XX:MaxGCPauseMillis=500XX:GCTimeRatio=19#-Xms -Xmx 属于XX参数-Xms 等价于 -XX:InitialHeapSize-Xmx 等价于 -XX:MaxHeapSize 查看jvm运行时参数 12345-XX:+PrintFlagsInitial 查看初始值-XX:+PrintFlagsFinal 查看最终值-XX:+UnlockExperimentalVMOptions 解锁实验参数-XX:+UnlockDiagnosticVMOptions 解锁诊断参数-XX:+PrintCommandLineFlags 打印命令行参数 -XX:+PrintFlagsFinal java -XX:+PrintFlagsFinal -version =表示默认值 :=被用户或者jvm修改后的值 jinfo 12jinfo -flags 55208 打印被修改过的jvm参数值jinfo -flag &lt;name&gt; 55208 打印对应name的参数值 查看jvm统计信息 jstat 1234options: -class 类装载 -compiler JIT编译信息 -gc 垃圾回收信息 类装载 垃圾收集 S0C S1C S0U S1U : S0与S1的总量与使用量 EC EU ：Eden区总量与使用量 OC OU：Old区的总量与使用量 MC MU：Metaspace 区总量与使用量 CCSC CCSU：压缩类空间总量与使用量 YGC YGCT ：YongGC的次数与时间 FGC FGCT：FullGC的次数与时间 GCT：总的GC时间 JIT编译 导出内存映像文件 内存溢出自动导出 12-XX:+HeapDumpOnOutOfMemoryError-XX:HeapDumpPath=./ 使用jmap命令手动导出 1jmap -dump:format=b,file=heap.hprof &lt;虚拟机id&gt;]]></content>
      <categories>
        <category>jvm</category>
      </categories>
      <tags>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo使用总结]]></title>
    <url>%2F2020%2F02%2F26%2Fhexo%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[自定义域名 添加一条域名解析记录 在hexo的source文件夹中添加CNAME文件，内容为自己的域名 修改新建文章模版模版路径为hexo/scaffolds/post.md]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RabbitMQ学习]]></title>
    <url>%2F2020%2F02%2F22%2FRabbitMQ%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[安装并启动 配置yum源 1234567[rabbitmq-erlang]name=rabbitmq-erlangbaseurl=https://dl.bintray.com/rabbitmq/rpm/erlang/20/el/7gpgcheck=1gpgkey=https://dl.bintray.com/rabbitmq/Keys/rabbitmq-release-signing-key.ascrepo_gpgcheck=0enabled=1 安装 yum install -y rabbitmq-server 启动 rabbitmq-server start &amp; 安装管控台插件 rabbitmq-plugins enable rabbitmq_management 控制台 登录 浏览器输入http://ip:15672/ 用户名guest 密码guest 命令行 关闭应用 rabbitmqctl stop_app 启动应用 rabbitmqctl start_app 节点状态 rabbitmqctl status 添加用户 rabbitmqctl add username password 列出所有用户 rabbitmqctl list_users 删除用户 rabbitmqctl delete_user username 修改密码 rabbitmqctl change_password username newpassword 列出用户权限rabbitmqctl list_user_permissions username 清除用户权限 rabbitmqctl clear_permissions -p vhostpath username 设置用户权限 rabbitmqctl set_permissions -p vhostpath username &quot;.*&quot;&quot;.*&quot;&quot;.*&quot; 创建虚拟主机rabbitmqctl add_vhost vhostpath 列出所有虚拟主机 rabbitmqctl list_vhost 列出虚拟主机上所有权限 rabbitmqctl list_permissions -p vhostpath 删除虚拟主机 rabbitmqctl delete_vhost vhostpath 查看所有队列信息 rabbitmqctl list_queues 清除队列中的消息 rabbitmqctl -p vhostpath purge_queue blue 移除所有数据（要在rabbitmqctl stop_app之后使用） rabbitmqctl reset 组成集群命令rabbitmqctl join_cluster &lt;clusternode&gt; [--ram] 查看集群状态rabbitmqctl cluster_status 修改集群节点的存储形式rabbitmqctl change_cluster_node_type disc|ram 摘除节点rabbitmqctl forget_cluster_node [offline] 修改节点名称rabbitmqctl rename_cluster_node oldnode1 newnode1 [oldnode2] [newnode2] java客户端生产者12345678910111213141516171819202122232425262728293031323334package com.example.rabbitmq.quickstart;import com.rabbitmq.client.Channel;import com.rabbitmq.client.Connection;import com.rabbitmq.client.ConnectionFactory;/** * @Author: haoming * @Date: 2020/2/23 4:03 下午 * @Version 1.0 */public class Producer &#123; public static void main(String[] args) throws Exception &#123; //1.创建一个ConnectionFactory ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost("152.136.233.203"); connectionFactory.setPort(5672); connectionFactory.setVirtualHost("/"); //2.通过连接工厂创建连接 Connection connection = connectionFactory.newConnection(); //3.通过connection创建一个channel Channel channel = connection.createChannel(); //4.通过channel发送数据 for (int i = 0; i &lt; 5; i++) &#123; String msg = "hello RabbitMQ!"; channel.basicPublish("","test001",null,msg.getBytes()); //如果未指定exchange，默认使用(AMQP default) exchange,这个exchange会根据routerKey找到同名队列 &#125; //5.关闭相关的连接 channel.close(); connection.close(); &#125;&#125; 消费者123456789101112131415161718192021222324252627282930313233343536373839package com.example.rabbitmq.quickstart;import com.rabbitmq.client.*;import java.io.IOException;import java.util.concurrent.TimeoutException;/** * @Author: haoming * @Date: 2020/2/23 4:04 下午 * @Version 1.0 */public class Consumer &#123; public static void main(String[] args) throws Exception &#123; //1.创建一个ConnectionFactory ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost("152.136.233.203"); connectionFactory.setPort(5672); connectionFactory.setVirtualHost("/"); //2.通过连接工厂创建连接 Connection connection = connectionFactory.newConnection(); //3.通过connection创建一个channel Channel channel = connection.createChannel(); //4.声明（创建）一个队列 String queueName = "test001"; channel.queueDeclare(queueName,true,false,false,null); //5.创建消费者 QueueingConsumer queueingConsumer = new QueueingConsumer(channel); //6.设置channel channel.basicConsume(queueName,true,queueingConsumer); //7.获取消息 while (true)&#123; QueueingConsumer.Delivery delivery = queueingConsumer.nextDelivery(); String msg = new String(delivery.getBody()); System.out.println("msg="+msg); //Envelope envelope = delivery.getEnvelope(); &#125; &#125;&#125; 交换机作用接收消息，并根据路由键转发消息到绑定的队列。 属性 Name 交换机名称 Type 交换机类型 direct topic fanout headers（以下图片参考https://www.cnblogs.com/stefan-liu/p/5315809.html） direct（使用比较多） 所有发送到Direct Exchange的消息被转发到RouteKey中指定的Queue。 Direct模式可以使用RabbitMQ自带的Exchange:default Exchange,所以不需要将Exchange进行任何绑定（binding）操作，消息传递时，RouterKey必须完全匹配才能完全接收，否则该消息会被抛弃。 Topic 所有发送到Topic Exchange的消息被转发到所有关心RouterKey中指定Topic的Queue上。 Exchange将RouteKey和某Topic进行模糊匹配，此时队列需要绑定一个Topic。 fanout 不处理路由键，只需要简单的将队列绑定到交换机上。 发送到交换机的消息都会被转发到与该交换机绑定的所有队列上。 转发消息是最快的。 Durability 是否需要持久化，true未持久化。 Auto Delete 当最后一个绑定到Exchange上的队列删除后，自动删除该Exchange。 Internal 当前Exchange是否用于RabbitMQ内部使用，默认为false。 Arguments 扩展参数，用于扩展AMQP自制定化使用。 模糊匹配 符号 “#” 匹配一个或多个词 “log.#” 能够匹配到“log.info.oa” 符号 “*” 匹配不多不少一个词 “log.*” 只会匹配到 “log.erro” binding作用Exchange 和 Exchang、Queue之间的连接关系。 Binding中可以包含RoutingKey或者参数。 消息队列作用存储消息数据 属性 Durability:是否持久化，Durable:是；Transient:否。 Auto delete:yes:表示当最后一个监听移除之后，该Queue会自动被删除。 Message作用服务器和应用程序之间传送的数据。 本质上就是一段数据，由Properties和Payload(Body)组成。 属性 Delivery mode 送达模式 ：持久化非持久化 Headers 自定义属性 其他属性： content_type content_encoding priority 优先级 correlation_id 消息唯一id(消息幂等) reply_to expiration 消息到期时间 message_id 消息的id timestamp type user_id app_id cluster_id 123456789Map&lt;String, Object&gt; headers = new HashMap&lt;&gt;();headers.put("my1", "111");headers.put("my2", "222");AMQP.BasicProperties properties = new AMQP.BasicProperties.Builder() .deliveryMode(2) .contentEncoding("UTF-8") .expiration("10000") .headers(headers) .build(); Virtual host 虚拟主机作用虚拟地址，用于进行逻辑隔离，最上层的消息路由。 一个vhost里面可以有若干个exchange和queue。 同一个vhost里面不能有相同名称的exchange和queue。 RabbitMQ架构图 消息丢失生产端的可靠性投递 保障消息的成功发出 保障MQ节点成功接收 发送端收到MQ节点确认应答 完善的消息补偿机制 解决方案 消息落库，对消息状态进行打标 业务数据入库，消息入库，消息状态为0 发送消息到MQ 消息确认回调 更新消息状态为1 定时任务拉取一段时间内状态一直为0的消息，进行重发 超过规定的重发次数仍然没有成功，更新消息状态为2，人工介入 消息的延迟投递，做二次确认，回调检查（高并发） 业务数据入库 发送两条消息，其中一条正常发送（队列0），其中一条延迟发送（队列1） 消费者消费消息 消费者发送消费确认消息到MQ（队列2） callback服务消费确认消息（队列2），并将确认结果入库 callback服务消费延迟发送的消息（队列1），确认此消息是否成功，如果未成功，通知生产者重发 重复消费消费端-幂等性保障消费端实现幂等性，就意味着我们的消息永远不会消费多次，即使我们收到了多条一样的消息 幂等性解决方案 唯一ID+指纹码机制 ，利用数据库主键去重 select count(1) from TABLE where id = 唯一id+指纹码 好处：实现简单；坏处：高并发下有数据库写入的性能瓶颈 解决方案：根据ID进行分库分表 利用Redis的原子性去实现 需要考虑的问题 是否要进行数据的落库，如果要落库的话，要解决的问题是数据库和缓存如何做到原子性 如果不进行落库，都存储到缓存中，如何设置定时同步的策略 确认机制Confirm确认消息生产端代码1234567891011121314//指定我们的消息投递模式: 消息的确认模式 channel.confirmSelect();//添加一个确认监听channel.addConfirmListener(new ConfirmListener() &#123; @Override public void handleNack(long deliveryTag, boolean multiple) throws IOException &#123; System.err.println("-------no ack!-----------"); &#125; @Override public void handleAck(long deliveryTag, boolean multiple) throws IOException &#123; System.err.println("-------ack!-----------"); &#125;&#125;); Return消息在某些情况下，如果我们在发送消息的时候，当前的exchange不存在或者指定的路由key找不到，这个时候如果我们需要监听这种不可达的消息，就要使用Return Listener 配置项Mandatory:如果为true,则监听器会接收到路由不可达的消息，然后进行后续处理，如果为false,那么broker端自动删除该消息 生产端代码12345678910111213141516channel.addReturnListener(new ReturnListener() &#123; @Override public void handleReturn(int replyCode, String replyText, String exchange, String routingKey, BasicProperties properties, byte[] body) throws IOException &#123; System.err.println("---------handle return----------"); System.err.println("replyCode: " + replyCode); System.err.println("replyText: " + replyText); System.err.println("exchange: " + exchange); System.err.println("routingKey: " + routingKey); System.err.println("properties: " + properties); System.err.println("body: " + new String(body)); &#125; &#125;); //mandatory设置为true channel.basicPublish(exchange, routingKeyError, true, null, msg.getBytes()); 消费端自定义监听12345678910111213141516171819202122232425262728293031323334public class Consumer &#123; public static void main(String[] args) throws Exception &#123; ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost("ip"); connectionFactory.setPort(5672); connectionFactory.setVirtualHost("/"); Connection connection = connectionFactory.newConnection(); Channel channel = connection.createChannel(); String exchangeName = "test_consumer_exchange"; String routingKey = "consumer.#"; String queueName = "test_consumer_queue"; channel.exchangeDeclare(exchangeName, "topic", true, false, null); channel.queueDeclare(queueName, true, false, false, null); channel.queueBind(queueName, exchangeName, routingKey); channel.basicConsume(queueName, true, new MyConsumer(channel)); &#125;&#125;public class MyConsumer extends DefaultConsumer &#123; public MyConsumer(Channel channel) &#123; super(channel); &#125; @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; System.err.println("-----------consume message----------"); System.err.println("consumerTag: " + consumerTag); System.err.println("envelope: " + envelope); System.err.println("properties: " + properties); System.err.println("body: " + new String(body)); &#125;&#125; 消费端限流RabbitMQ提供了一种qos（服务质量保证）功能，即在非自动确认消息的前提下，如果一定数目的消息（通过基于consume或者channel设置qos的值）未被确认前，不进行消费新的消息 autoACK 一定要设置成false.不自动签收。 123456789101112131415161718192021222324252627//void basicQos(int prefetchSize, int prefetchCount, boolean global) throws IOException;//prefetchSize：0//prefetchCount：告诉RabbitMQ不要同时给一个消费者推送多于N个消息，即一旦有N个消息还没有ack,则该cousumer将block掉，直到有消息ack//global:是否将上面设置应用于channel,就是上面的限制是channel级别还是consumer级别//prefetchSize、global RabbitMQ还没有实现//1 限流方式 第一件事就是 autoAck设置为 false//消费端channel.basicQos(0, 1, false);channel.basicConsume(queueName, false, new MyConsumer(channel));public class MyConsumer extends DefaultConsumer &#123; private Channel channel ; public MyConsumer(Channel channel) &#123; super(channel); this.channel = channel; &#125; @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; System.err.println("-----------consume message----------"); System.err.println("consumerTag: " + consumerTag); System.err.println("envelope: " + envelope); System.err.println("properties: " + properties); System.err.println("body: " + new String(body)); //手动ack channel.basicAck(envelope.getDeliveryTag(), false); &#125;&#125; 消费端ack与重回队列重回队列对没有处理成功的消息，把消息重新传递给Broker 一般在实际的应用中，都会关闭重回队列，设置为false 1234567891011121314151617181920212223public class MyConsumer extends DefaultConsumer &#123; private Channel channel ; public MyConsumer(Channel channel) &#123; super(channel); this.channel = channel; &#125; @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; System.err.println("-----------consume message----------"); System.err.println("body: " + new String(body)); try &#123; Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; if((Integer)properties.getHeaders().get("num") == 0) &#123; //nack,处理失败，重回队列 channel.basicNack(envelope.getDeliveryTag(), false, true); &#125; else &#123; channel.basicAck(envelope.getDeliveryTag(), false); &#125; &#125;&#125; 死信队列（DLX Dead-Letter-Exchange）定义 当消息在一个队列中变成死信（dead message）之后，它能被重新publish到另外一个Exchange,这个Exchange就是DLX DLX也是一个正常的Exchange 消息变成死信的几种情况 消息被拒绝（basic.reject/basic.nack）并且requeue=false 消息TTL过期 队列达到最大长度 123456789101112131415161718192021222324252627282930public class Consumer &#123; public static void main(String[] args) throws Exception &#123; ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost("ip"); connectionFactory.setPort(5672); connectionFactory.setVirtualHost("/"); Connection connection = connectionFactory.newConnection(); Channel channel = connection.createChannel(); // 这就是一个普通的交换机 和 队列 以及路由 String exchangeName = "test_dlx_exchange"; String routingKey = "dlx.#"; String queueName = "test_dlx_queue"; channel.exchangeDeclare(exchangeName, "topic", true, false, null); Map&lt;String, Object&gt; agruments = new HashMap&lt;String, Object&gt;(); agruments.put("x-dead-letter-exchange", "dlx.exchange"); //这个agruments属性，要设置到声明队列上 channel.queueDeclare(queueName, true, false, false, agruments); channel.queueBind(queueName, exchangeName, routingKey); //要进行死信队列的声明: channel.exchangeDeclare("dlx.exchange", "topic", true, false, null); channel.queueDeclare("dlx.queue", true, false, false, null); channel.queueBind("dlx.queue", "dlx.exchange", "#"); channel.basicConsume(queueName, true, new MyConsumer(channel)); &#125;&#125;]]></content>
      <tags>
        <tag>消息队列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多线程笔记]]></title>
    <url>%2F2020%2F01%2F08%2F%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[CAS底层汇编命令 lock cmpxchg volatile 线程内存可见 防止指令重排序 cache line 缓存行：当cpu去内存读数据时，会把附近的数据读出来，一般是64字节,是cpu同步的基本单位 缓存行隔离会比缓存行伪共享效率高 MESI 伪共享 合并写 cpu内部的4个字节buffer 指令重排序]]></content>
  </entry>
  <entry>
    <title><![CDATA[rsync使用笔记]]></title>
    <url>%2F2019%2F12%2F29%2Frsync%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[rsync特征 速度：第一次的rsync同步源的整个目录到目标位置。之后，rsync只传递改变的块或字节到目标位置，传输非常快。 安全：rsync在传输中允许使用ssh协议加密数据。 更少的带宽：rsync分别在发送和接收端对数据进行压缩和解压缩。 特权：不需要特殊权限来安装和执行rsync。 rsync语法12$ rsync options source destination#source和destination可以是本地或远程。在远程的情况下，需要指定登录名、远程服务器的名称和位置。 rsync使用示例 在本地服务器上同步两个目录 12345$ rsync -zvr 源目录 目标目录# -z 允许压缩# -v 详细输出# -r 递归# rsync 不会同步源文件或目录的时间戳 用rsync –a同步时间戳 1$ rsync -azv 源目录 目标目录 只同步一个文件 1$ rsync -v /var/test.txt /root/temp/ 将本地文件同步到远程服务器 1$ rsync -avz 源目录 username@remote_server_ip:path 将远程文件同步到本地服务器 1$ rsync -avz username@remote_server_ip:path 目标目录 指定使用某一协议同步 1$ rsync -avz -e ssh username@remote_server_ip:path 目标目录 不覆盖目标位置已改变的文件 12# 如果目标位置的文件已被修改，而我们不希望旧文件覆盖它时,可以使用-u选项。$ rsync -avzu username@remote_server_ip:path /root/temp 查看rsync进度 1$ rsync -avz --progress username@remote_server_ip:path /root/temp 删除目标位置创建的文件 12#若我们希望rsync时删除在源位置不存在而在目标位置存在的文件，可以使用-delete选项。$ rsync -avz --delete username@remote_server_ip:path /root/temp 在目标位置不创建新文件 12# 使用-existing选项使得在同步时只同步目标位置存在的文件，而不创建新文件。$ rsync -avz --existing username@remote_server_ip:path /root/temp 查看源和目标的不同文件或目录 1234567891011$ rsync -avzi username@remote_server_ip:path /root/temp/receiving file list ... done&gt;f.st.... Basenames.f....og. Dirnames=====================================&gt; 代表文件已被传输到本地主机。f：代表这是个文件s：代表文件大小发生变化t：代表时间戳发生变化o：属主发生变化g：属组发生变化 在传输时指定包括或排除某些文件 1rsync -avz --include 'P*' --exclude '*' username@remote_server_ip:path /root/temp 不传输大文件 12# 使用rsync –max-size选项后，rsync将不传输大于指定大小的文件rsync -avz --max-size='100K' username@remote_server_ip:path /root/temp/ 传输整个文件 12# rsync的主要特征之一是它只传输改变的块到目标位置，而不是传输整个文件。如果网络带宽对你不是问题(CPU有)，您可以使用-w选项来传输整个文件。它会加速rsync过程，因为它不用在源和目标位置执行校验和。$ rsync -avzW username@remote_server_ip:path /root/temp]]></content>
  </entry>
  <entry>
    <title><![CDATA[zookeeper学习笔记]]></title>
    <url>%2F2019%2F12%2F03%2Fzookeeper%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[启动 启动 ./zkServer.sh start 客户端连接 ./zkCli.sh 客户端命令学习 ls / ls2 / stat 1234567891011cZxid = 0x0 #idctime = Thu Jan 01 08:00:00 CST 1970 #创建时间mZxid = 0x0 #修改idmtime = Thu Jan 01 08:00:00 CST 1970 #修改时间pZxid = 0x6 #子节点idcversion = 2 #子节点版本dataVersion = 0 #当前节点数据版本号aclVersion = 0 #当前节点权限的版本号ephemeralOwner = 0x0 #是否是临时节点dataLength = 0 #数据长度numChildren = 2 #子节点数量 session过期，则临时节点znode会被抛弃 create命令 set命令 delete命令 watcher 机制watcher 针对每个节点的操作，都会有一个监督者-&gt;watcher 当监控的某个对象(znode)发生变化,则触发watcher事件 zk中的watcher是一次性的，触发后立即销毁 watcher事件类型1 创建父节点触发：NodeCreated 修改父节点数据触发：NodeDataChanged 删除父节点触发：NodeDeleted watcher事件类型2 ls为父节点设置watcher,创建子节点触发：NodeChildrenChanged ls为父节点设置watcher,删除子节点触发：NodeChildrenChanged ls为父节点设置watcher,修改子节点不触发事件 ACL权限控制定义针对节点设置相关读写权限，目的是为了保证数据安全 acl命令 getAcl:获取某个节点的acl权限信息 setAcl:设置某个节点的acl权限信息 addauth:输入认证授权信息，注册时输入明文密码(登录)，但在zk系统中，密码是以加密的形式存在的 acl构成 zk的acl通过[scheme:id:permisions]来构成权限列表 scheme:代表采用的某种权限机制 world:world下只有一个id,即只有一个用户，也就是anyone,组合写法world:anyone[permissions] auth:代表认证登录，需要注册用户有权限就可以，形式为auth:user:password:[permissions] digest:需要对密码加密才能访问，组合形式为：digest:username:BASE64(SHA1(password))[permissions] ip:此时现在ip访问，比如：ip:192.168.1.1:[permissions] super:代表超级管理员，拥有所有权限 Id:代表允许访问的用户 permisions:权限组合字符串 CREATE：创建子节点 READ：获取节点、子节点 WRITE：设置节点数据 DELETE：删除子节点 ADMIN：设置权限 四字命令定义 zk可以通过它自身提供的简写命令来和服务器进行交互 需要使用到nc命令，安装 yum install nc echo [commond] | nc [ip][port] 命令 [stat]:查看zk的状态信息，以及是否mode [ruok]:查看当前zkserver是否启动，返回imok [dump]:列出未经使用的会话和临时节点 [conf]:查看服务器配置 [cons]:连接到服务端的客户端信息 [envi]:环境变量 [mntr]:监控zk健康信息 [wchs]:展示watch的信息 [wchc]与[wchp] :session 与watch 及path 与watch 的信息]]></content>
  </entry>
  <entry>
    <title><![CDATA[浏览器缓存]]></title>
    <url>%2F2019%2F10%2F28%2F%E6%B5%8F%E8%A7%88%E5%99%A8%E7%BC%93%E5%AD%98%2F</url>
    <content type="text"><![CDATA[缓存过程浏览器第一次向服务器发起该请求后拿到请求结果，会根据响应报文中HTTP头的缓存标识，决定是否缓存结果，是则将请求结果和缓存标识存入浏览器缓存中，简单的过程如下图： 浏览器每次发起请求，都会先在浏览器缓存中查找该请求的结果以及缓存标识 浏览器每次拿到返回的请求结果都会将该结果和缓存标识存入浏览器缓存中 只要我们再理解浏览器缓存的使用规则，那么所有的问题就迎刃而解了。当浏览器向服务器发起请求时，服务器会将缓存规则放入HTTP响应报文的HTTP头中和请求结果一起返回给浏览器。 缓存规则强制缓存 规则：控制强制缓存的字段分别是Expires和Cache-Control，其中Cache-Control优先级比Expires高。 Expires： Expires是HTTP/1.0控制网页缓存的字段，其值为服务器返回该请求结果缓存的到期时间，即再次发起该请求时，如果客户端的时间小于Expires的值时，直接使用缓存结果。 到了HTTP/1.1，Expire已经被Cache-Control替代，原因在于Expires控制缓存的原理是使用客户端的时间与服务端返回的时间做对比，那么如果客户端与服务端的时间因为某些原因（例如时区不同；客户端和服务端有一方的时间不准确）发生误差，那么强制缓存则会直接失效，这样的话强制缓存的存在则毫无意义。 Cache-Control 在HTTP/1.1中，Cache-Control是最重要的规则，主要用于控制网页缓存，主要取值为： public：所有内容都将被缓存（客户端和代理服务器都可缓存） private：所有内容只有客户端可以缓存，Cache-Control的默认取值 no-cache：客户端缓存内容，但是是否使用缓存则需要经过协商缓存来验证决定 no-store：所有内容都不会被缓存，即不使用强制缓存，也不使用协商缓存 max-age=xxx (xxx is numeric)：缓存内容将在xxx秒后失效 例子(google首页) HTTP响应报文中expires的时间值，是一个绝对值 HTTP响应报文中Cache-Control为max-age=600，是相对值 由于Cache-Control的优先级比expires，那么直接根据Cache-Control的值进行缓存，意思就是说在31536000秒内再次发起该请求，则会直接使用缓存结果，强制缓存生效。 在无法确定客户端的时间是否与服务端的时间同步的情况下，Cache-Control相比于expires是更好的选择，所以同时存在时，只有Cache-Control生效。 缓存存放位置 状态码为灰色的请求则代表使用了强制缓存，请求对应的Size值则代表该缓存存放的位置，分别为from memory cache 和 from disk cache。from memory cache代表使用内存中的缓存，from disk cache则代表使用的是硬盘中的缓存，浏览器读取缓存的顺序为memory –&gt; disk。 虽然我已经直接把结论说出来了，但是相信有不少人对此不能理解，那么接下来我们一起详细分析一下缓存读取问题，这里仍让以我的博客为例进行分析： 过程如下 访问https://haominglfs.github.io/ 关闭博客的标签页 重新打开https://haominglfs.github.io/ 刷新 内存缓存(from memory cache)和硬盘缓存(from disk cache)的特点： 内存缓存(from memory cache)：内存缓存具有两个特点，分别是快速读取和时效性： 快速读取：内存缓存会将编译解析后的文件，直接存入该进程的内存中，占据该进程一定的内存资源，以方便下次运行使用时的快速读取。 时效性：一旦该进程关闭，则该进程的内存则会清空。 硬盘缓存(from disk cache)：硬盘缓存则是直接将缓存写入硬盘文件中，读取缓存需要对该缓存存放的硬盘文件进行I/O操作，然后重新解析该缓存内容，读取复杂，速度比内存缓存慢。 在浏览器中，浏览器会在js和图片等文件解析执行后直接存入内存缓存中，那么当刷新页面时只需直接从内存缓存中读取(from memory cache)；而css文件则会存入硬盘文件中，所以每次渲染页面都需要从硬盘读取缓存(from disk cache)。 协商缓存协商缓存就是强制缓存失效后，浏览器携带缓存标识向服务器发起请求，由服务器根据缓存标识决定是否使用缓存的过程。 规则：控制协商缓存的字段分别有：Last-Modified / If-Modified-Since和Etag / If-None-Match，其中Etag / If-None-Match的优先级比Last-Modified / If-Modified-Since高。 Last-Modified / If-Modified-SinceLast-Modified是服务器响应请求时，返回该资源文件在服务器最后被修改的时间，如下： If-Modified-Since则是客户端再次发起该请求时，携带上次请求返回的Last-Modified值，通过此字段值告诉服务器该资源上次请求返回的最后被修改时间。服务器收到该请求，发现请求头含有If-Modified-Since字段，则会根据If-Modified-Since的字段值与该资源在服务器的最后被修改时间做对比，若服务器的资源最后被修改时间大于If-Modified-Since的字段值，则重新返回资源，状态码为200；否则则返回304，代表资源无更新，可继续使用缓存文件，如下： Etag / If-None-MatchEtag是服务器响应请求时，返回当前资源文件的一个唯一标识(由服务器生成)，如下： If-None-Match是客户端再次发起该请求时，携带上次请求返回的唯一标识Etag值，通过此字段值告诉服务器该资源上次请求返回的唯一标识值。服务器收到该请求后，发现该请求头中含有If-None-Match，则会根据If-None-Match的字段值与该资源在服务器的Etag值做对比，一致则返回304，代表资源无更新，继续使用缓存文件；不一致则重新返回资源文件，状态码为200，如下： 总结强制缓存优先于协商缓存进行，若强制缓存(Expires和Cache-Control)生效则直接使用缓存，若不生效则进行协商缓存(Last-Modified / If-Modified-Since和Etag / If-None-Match)，协商缓存由服务器决定是否使用缓存，若协商缓存失效，那么代表该请求的缓存失效，重新获取请求结果，再存入浏览器缓存中；生效则返回304，继续使用缓存，主要过程如下： 其他 为什么要有Etag 你可能会觉得使用Last-Modified已经足以让浏览器知道本地的缓存副本是否足够新，为什么还需要Etag呢？HTTP1.1中Etag的出现（也就是说，ETag是新增的，为了解决之前只有If-Modified的缺点）主要是为了解决几个Last-Modified比较难解决的问题： 一些文件也许会周期性的更改，但是他的内容并不改变(仅仅改变的修改时间)，这个时候我们并不希望客户端认为这个文件被修改了，而重新GET； 某些文件修改非常频繁，比如在秒以下的时间内进行修改，(比方说1s内修改了N次)，If-Modified-Since能检查到的粒度是s级的，这种修改无法判断(或者说UNIX记录MTIME只能精确到秒)； 某些服务器不能精确的得到文件的最后修改时间。 强缓存与协商缓存的区别可以用下表来表示： | 缓存类型 | 获取资源形式 | 状态码 | 发送请求到服务器 || :——: | :———-: | :—————: | :——————————: || 强缓存 | 从缓存取 | 200(from cache) | 否，直接从缓存取 || 协商缓存 | 从缓存取 | 304(not Modified) | 是，通过服务器来告知缓存是否可用 | 用户行为对缓存的影响 | 用户操作 | Expires/Cache-Control | Last-Modifed/Etag || :————-: | :——————-: | :—————: || 地址栏回车 | 有效 | 有效 || 页面链接跳转 | 有效 | 有效 || 新开窗口 | 有效 | 有效 || 前进回退 | 有效 | 有效 || F5刷新 | 无效 | 有效 || Ctrl+F5强制刷新 | 无效 | 无效 | F5 会 跳过强缓存规则，直接走协商缓存；Ctrl+F5 ，跳过所有缓存规则，和第一次请求一样，重新获取资源。 no-cache 如果request headers中，Cache-Control为no-cache。表示不管服务端有没有设置Cache-Control，都必须从重新去获取请求。 max-age=0 max-age=0表示不管response怎么设置，在重新获取资源之前，先检验ETag/Last-Modified 不管是max-age=0还是no-cache，都会返回304（资源无修改的情况下），no-store才是真正的不进行缓存。 Cache-Control与Expires Cache-Control与Expires的作用一致，都是指明当前资源的有效期，控制浏览器是否直接从浏览器缓存取数据还是重新发请求到服务器取数据。只不过Cache-Control的选择更多，设置更细致，如果同时设置的话，其优先级高于Expires。 Last-Modified/ETag与Cache-Control/Expires 配置Last-Modified/ETag的情况下，浏览器再次访问统一URI的资源，还是会发送请求到服务器询问文件是否已经修改，如果没有，服务器会只发送一个304回给浏览器，告诉浏览器直接从自己本地的缓存取数据；如果修改过那就整个数据重新发给浏览器； Cache-Control/Expires则不同，如果检测到本地的缓存还是有效的时间范围内，浏览器直接使用本地副本，不会发送任何请求。两者一起使用时，Cache-Control/Expires的优先级要高于Last-Modified/ETag。即当本地副本根据Cache-Control/Expires发现还在有效期内时，则不会再次发送请求去服务器询问修改时间（Last-Modified）或实体标识（Etag）了。 一般情况下，使用Cache-Control/Expires会配合Last-Modified/ETag一起使用，因为即使服务器设置缓存时间, 当用户点击“刷新”按钮时，浏览器会忽略缓存继续向服务器发送请求，这时Last-Modified/ETag将能够很好利用304，从而减少响应开销。 参考https://heyingye.github.io/2018/04/16/彻底理解浏览器的缓存机制/ https://juejin.im/post/5c417993f265da61285a6075 https://www.zhoulujun.cn/html/theory/network/2018_0306_8078.html]]></content>
      <tags>
        <tag>浏览器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL优化]]></title>
    <url>%2F2019%2F10%2F23%2FSQL%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[数据结构 3层Btree可以存放上百万条数据 Btree一般指的是B+树，数据全部存放在叶子节点中。 B+树中查询任意的数据次数：n次（B+树的高度） 分类： 单值索引 单列的索引，一个表可以有多个单值索引 唯一索引 不能重复 可以为null 符合索引 多个列构成的索引 主键索引 不能重复，不能为null SQL性能问题 分析sql的执行计划（explain）,可以模拟SQL优化器执行sql语句 Mysql查询优化会干扰我们的优化。 explain 参数解析： id:编号 id值相同，从上往下顺序执行; id值越大越优先查询 (本质：在嵌套子查询时，先查内层 再查外层) select_type 查询类型 PRIMARY:包含子查询SQL中的 主查询 （最外层） SUBQUERY：包含子查询SQL中的 子查询 （非最外层） simple:简单查询（不包含子查询、union） derived:衍生查询(使用到了临时表) 在from子查询中只有一张表 explain select cr.cname from ( select * from course where tid in (1,2) ) cr ; 在from子查询中， 如果有table1 union table2 ，则table1 就是derived,table2就是union explain select cr.cname from ( select from course where tid = 1 union select from course where tid = 2 ) cr ; table partitions type 类型（索引类型） system&gt;const&gt;eq_ref&gt;ref&gt;range&gt;index&gt;all 其中：system,const只是理想情况；实际能达到 ref&gt;range system（忽略）: 只有一条数据的系统表 ；或 衍生表只有一条数据的主查询 const:仅仅能查到一条数据的SQL ,用于Primary key 或unique索引 （类型 与索引类型有关） eq_ref:唯一性索引：对于每个索引键的查询，返回匹配唯一行数据（有且只有1个，不能多 、不能0）;常见于唯一索引 和主键索引 非唯一性索引，对于每个索引键的查询，返回匹配的所有行（0，多） possible_keys 预测用到的索引 key 实际使用的索引 key_len 实际使用的索引的长度 ref 表之间的引用 rows 通过索引查询到的数据量 filtered Extra 额外的信息 索引分类 聚集索引：页节点包含了完整的数据记录。（innoDB的主键索引） 非聚集索引 （myISAM的主键索引） Q&amp;A 为什么InnoDB表必须有主键，并且推荐使用整形的自增主键。 InnoDB的索引和数据存在同一个表名.ibd文件中 自增：可以减少B+树的分裂。]]></content>
  </entry>
  <entry>
    <title><![CDATA[springboot学习1]]></title>
    <url>%2F2019%2F10%2F08%2Fspringboot%E5%AD%A6%E4%B9%A01%2F</url>
    <content type="text"></content>
      <tags>
        <tag>springboot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[macosX挂载NFS]]></title>
    <url>%2F2019%2F09%2F30%2FmacosX%E6%8C%82%E8%BD%BDNFS%2F</url>
    <content type="text"><![CDATA[Mac OS X使用automounter，也称为 autofs 来挂载NFS输出卷。Autofs包含以下程序和daemons: autofsd autofsd 执行 automount 之后 ，就会等待网络配置修改事件以及类似的事件发生。如果发生这样的事件，重新运行一次 automount 来更新挂载以反映当前automounter映射。也可以使用automount_reread 来运行 automount 。 automountd automountd 是一个响应从 autofs 发出的请求的服务，用来挂载或卸载网络文件系统，并且提供目录的内容，基于automounter映射的内容。这个 automountd 是通过 launchd 来启动的。 automount automount 是实际的挂载管理器。使用一些映射文件和配置文件来管理挂载和卸载远程资源。这些配置文件使用 /etc/autofs.conf 和 /etc/auto_master。 automount_reread automount_reread 可以触发针对 autofs 的网络变更事件。 检查autofs相关服务 ps -ef | grep auto | grep -v grep ​ 0 95 1 0 六12上午 ?? 0:00.03 autofsd 可以看到系统运行了一个autofsd 检查一个服务是否通过 launchd 启动 sudo launchctl list | grep -E &#39;automo|autof&#39; 95 0 com.apple.autofsd13290 0 com.apple.automountd Autofs映射，autofs 有两种映射方式 Direct Map 直接映射是直接列出目录的文件系统位置，关键字是完整的目录名字，例如 12/usr/local eng4:/export/local/src eng4:/export/src Indirect Map 非直接映射是为了用于将大量的对象和一个单一目录关联的情况。每个映射入口是一个目录入口的简化名字。一个非常好的案例是 auto_home 映射，可以检测所有在 /home 目录下的入口,例如： 123bill argon:/export/home/billbrent depot:/export/home/brentguy depot:/export/home/guy 创建AutoFS的Indirect Map 先在Windows主机（win7）上设置共享目录Mac。 编辑 /etc/auto_master配置文件添加如下： 123456789## Automounter master map#+auto_master # Use directory service/net -hosts -nobrowse,hidefromfinder,nosuid/home auto_home -nobrowse,hidefromfinder/Network/Servers -fstab/- -static/Users/haominglfs/win7 autofs_win7 以上配置告知 OS X 任何位于 /Users/haominglfs/win7 目录下的入口都通过 /etc/autofs_win7配置文件来配置。 在auto_master前面的”+”符号表示让OS X查看目录服务（例如Open Directory，LDAP等等）是否有自动挂载记录，如果从目录服务找到自动挂载记录就使用其进行挂载。 /home目录被设置成 auto_home ，但是这个并不是一个完全目录，而是指 /etc/auto_home。这是一个非直接映射(indirect map)的例子。定义本地目录的挂载点，而远程挂载则是 /etc/auto_home 映射文件定义。网络用户登录将使用 /etc/auto_home 中定义的目录挂载到 /home 上。 同样在 /etc/auto_home 配置文件中也可以看到 +auto_home 的配置表示查找使用目录服务的auto_home记录。 编辑 /etc/autofs_win7内容如下 mac -fstype=nfs 192.168.1.242:/e/mac 执行命令showmount -e 192.168.1.242 查看可挂载的NFS目录。 执行 automount 命令挂载 sudo automount -vc ​ ​ ​]]></content>
      <tags>
        <tag>nfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cec邮件调用示例]]></title>
    <url>%2F2019%2F09%2F30%2Fcec%E9%82%AE%E4%BB%B6%E8%B0%83%E7%94%A8%E7%A4%BA%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[调用邮件的服务为一个定时任务，定时扫描指定文件，若存在待发送的邮件，则以javaMail的方式调用邮件服务，调用示例如下(发件人邮箱SMTP服务器地址：mail.cec.com.cn)： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113 /** * JavaMail 版本: 1.6.0 * JDK 版本: JDK 1.7 以上（必须） */ public class sendMailService &#123; /** * * @param myEmailAccount 发件人邮箱账号 * @param myEmailPassword 发件人邮箱密码 * @param myEmailSMTPHost 发件人邮箱SMTP服务器地址 * @param mainSendNameAccounts 收件人主送名称及账号 * @param copySendNameAccounts 收件人抄送名称及账号 * @param secretSendNameAccounts 收件人暗送名称及账号 * @param sendMsg 发送邮件主体信息 * @return */ public static String sendMail(String myEmailAccount, String myEmailPassword,String senderName,String myEmailSMTPHost,Map&lt;String, String&gt; mainSendNameAccounts,Map&lt;String, String&gt; copySendNameAccounts,Map&lt;String, String&gt; secretSendNameAccounts,Map sendMsg)throws Exception &#123; Properties props = new Properties(); // 参数配置 props.setProperty("mail.transport.protocol", "smtp"); // 使用的协议（JavaMail规范要求） props.setProperty("mail.smtp.host", myEmailSMTPHost); // 发件人的邮箱的 SMTP 服务器地址 props.setProperty("mail.smtp.auth", "false"); // 需要请求认证 // 2. 根据配置创建会话对象, 用于和邮件服务器交互 Session session = Session.getInstance(props); session.setDebug(true); //开启日志 // 3. 创建一封邮件 MimeMessage message; message = createMimeMessage(session, myEmailAccount, senderName,mainSendNameAccounts,copySendNameAccounts,secretSendNameAccounts,sendMsg); // 4. 根据 Session 获取邮件传输对象 Transport transport = session.getTransport(); // 5. 使用 邮箱账号 和 密码 连接邮件服务器, 这里认证的邮箱必须与 message 中的发件人邮箱一致, 否则报错 transport.connect(myEmailAccount, myEmailPassword); // 6. 发送邮件, 发到所有的收件地址, message.getAllRecipients() 获取到的是在创建邮件对象时添加的所有收件人, 抄送人, 密送人 transport.sendMessage(message, message.getAllRecipients()); transport.close(); return Action.SUCCESS; &#125; /** * 创建邮件(带附件) * * @param session * @param myEmailAccount 发件人账号 * @param mainSendAccount 主送人员账号 * @param copySendAccount 抄送人员账号 * @param secretSendAccount 暗送人员账号 * @param sendMsg 发送信息主体 * @return * @throws Exception */ private static MimeMessage createMimeMessage(Session session, String myEmailAccount,String senderName, Map&lt;String, String&gt; mainSendNameAccounts ,Map&lt;String, String&gt; copySendNameAccounts,Map&lt;String, String&gt; secretSendNameAccounts,Map sendMsg) throws Exception &#123; IUser sUser = WebBaseUtil.getCurrentUser(); String subject = (String)sendMsg.get("subject"); String content = (String)sendMsg.get("content"); String attachPath = (String)sendMsg.get("attachPath"); // 1. 创建一封邮件 MimeMessage message = new MimeMessage(session); // 2. From: 发件人（昵称有广告嫌疑，避免被邮件服务器误认为是滥发广告以至返回失败，请修改昵称） message.setFrom(new InternetAddress(myEmailAccount,senderName, "UTF-8")); // 3. To: 收件人（可以增加多个收件人、抄送、密送） //设置主送人员 Set mainSendNameSet = mainSendNameAccounts.keySet();//主送人员姓名 List mailSendList = new ArrayList(); for (Object mainSendName : mainSendNameSet) &#123; mailSendList.add(new InternetAddress(mainSendNameAccounts.get(mainSendName))); &#125; InternetAddress[] mainAddress =(InternetAddress[])mailSendList.toArray(new InternetAddress[mailSendList.size()]); message.setRecipients(MimeMessage.RecipientType.TO,mainAddress);//当邮件有多个收件人时，用逗号隔开 //设置抄送人员 if(!copySendNameAccounts.isEmpty())&#123; Set copySendNameSet = copySendNameAccounts.keySet();//主送人员姓名 List copylist = new ArrayList(); for (Object copySendName : copySendNameSet) &#123; copylist.add(new InternetAddress(copySendNameAccounts.get(copySendName))); &#125; InternetAddress[] copyAddress =(InternetAddress[])copylist.toArray(new InternetAddress[copylist.size()]); message.setRecipients(MimeMessage.RecipientType.CC,copyAddress);//当邮件有多个收件人时，用逗号隔开 &#125; //设置暗送人员 if(!secretSendNameAccounts.isEmpty())&#123; Set secretSendNameSet = secretSendNameAccounts.keySet();//主送人员姓名 List secretlist = new ArrayList(); for (Object secretSendName : secretSendNameSet) &#123; secretlist.add(new InternetAddress(secretSendNameAccounts.get(secretSendName))); &#125; InternetAddress[] secretAddress =(InternetAddress[])secretlist.toArray(new InternetAddress[secretlist.size()]); message.setRecipients(MimeMessage.RecipientType.CC,secretAddress);//当邮件有多个收件人时，用逗号隔开 &#125; // 4. Subject: 邮件主题（标题有广告嫌疑，避免被邮件服务器误认为是滥发广告以至返回失败，请修改标题） message.setSubject(subject, "UTF-8"); //5. 创建正文文本"节点" MimeBodyPart text = new MimeBodyPart(); text.setContent(content,"text/html;charset=UTF-8"); // 6. 创建附件"节点" MimeMultipart mm = new MimeMultipart(); mm.addBodyPart(text); if(StringHelper.isNotEmpty(attachPath))&#123; MimeBodyPart attachment = new MimeBodyPart(); DataHandler dh = new DataHandler(new FileDataSource(attachPath)); attachment.setDataHandler(dh); attachment.setFileName(MimeUtility.encodeText(dh.getName())); mm.addBodyPart(attachment); // 如果有多个附件，可以创建多个多次添加 mm.setSubType("mixed"); // 混合关系 &#125; message.setContent(mm); // 6. 设置发件时间 message.setSentDate(new Date()); // 7. 保存设置 message.saveChanges(); return message; &#125; &#125;]]></content>
      <tags>
        <tag>工作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[windows7配置nfs]]></title>
    <url>%2F2019%2F09%2F30%2Fwindows7%E9%85%8D%E7%BD%AEnfs%2F</url>
    <content type="text"><![CDATA[安装haneWIN NFS SERVER 下载地址 打开nfs客户端，配置如下 编辑要共享的目录后，重启服务器，列表中就会显示共享的服务器。如果不生效，则打开win7的服务管理器重启nfsd服务。 问题 保存配置文件时显示没有权限保存文件，需要以管理员身份运行nfs客户端。如何打开win7的管理员账户，可以参考win7系统设置用户帐户为最高权限的操作方法]]></content>
      <tags>
        <tag>nfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cas单点登录]]></title>
    <url>%2F2019%2F09%2F26%2Fcas%E5%8D%95%E7%82%B9%E7%99%BB%E5%BD%95%2F</url>
    <content type="text"><![CDATA[cas登出源码解析]]></content>
  </entry>
  <entry>
    <title><![CDATA[js原型]]></title>
    <url>%2F2019%2F09%2F24%2Fjs%E5%8E%9F%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[obj这个对象本质上是被Object函数创建的，因此obj.__proto__=== Object.prototype。我们可以用一个图来表示。 即，每个对象都有一个__proto__属性，指向创建该对象的函数的prototype。 自定义函数的prototype本质上就是和 var obj = {} 是一样的，都是被Object创建，所以它的__proto__指向的就是Object.prototype。但是Object.prototype确实一个特例——它的__proto__指向的是null。 函数也是一种对象，函数是由Function，所以Object.__proto__ === Function.prototype 1234function fn(x,y)&#123; return x+y;&#125;var fn = new Function('x','y','return x+y');]]></content>
  </entry>
  <entry>
    <title><![CDATA[iframe问题总结]]></title>
    <url>%2F2019%2F09%2F11%2Fiframe%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[iframe内部内容被添加了&lt;pre&gt;标签今天在解决iframe上传文件的跨域问题时，遇到一个奇怪的问题，后台返回的json数据，放到iframe中时，莫名加上了&lt;pre&gt;标签，通过查询，最后在stackoverflow上找到这么一段话 Assuming that the user POST the request in a form setting the target to an iframe. The JSON response will be sent back to the user on his/her iframe with content type set as “text/html”. It is set as “text/html” instead of “application/json” because I want to avoid having a “pre” tag injected by the browser around the JSON response. Anyway, how does the user read that JSON response if the iframe and the parent window have different domain? There is going to be a cross domain policy issue. 大概意思就是，在form表单提交的是后，返回的json数据会根据form表单设置的target属性放到对应的iframe中，将返回的头信息改成text/html而不是默认的application/json就能避免返回的数据被包裹在&lt;pre&gt;标签中。]]></content>
  </entry>
  <entry>
    <title><![CDATA[websocket使用总结]]></title>
    <url>%2F2019%2F09%2F05%2Fwebsocket%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[背景公司门户系统有一个显示待办消息的需求，要求其他系统产生的待办消息要及时的在门户系统中展示，网上查找了解到有ajax轮询和websocket两种主要方式，为了及时性，最终选择了websocket方式。 整体思路一图胜千言 代码 前端 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374&lt;!-- websocket--&gt; &lt;script type="text/javascript"&gt; var msgTypes = &#123; 1:'db', 2:'dy', 3:'yj', 4:'gwdb' &#125; userId = '&lt;ww:property value="#session.sUser.userId" /&gt;'; //定义websocket var ws = new WebSocket("ws://localhost:8888/msg/ws"); //维持心跳 var heartCheck = &#123; timeout: 19000,//19s timeoutObj: null, reset: function()&#123; clearInterval(this.timeoutObj); this.start(); &#125;, start: function()&#123; this.timeoutObj = setInterval(function()&#123; if(ws.readyState==1)&#123; ws.send("HeartBeat"); &#125; &#125;, this.timeout) &#125; &#125;; //连接websocket ws.onopen = function() &#123; var msg = 'userid='+userId; console.log("open and send message："+msg); // 发送消息 ws.send(msg); heartCheck.start(); &#125;; //接收消息 ws.onmessage = function(evt) &#123; console.log("get message:"+evt.data); heartCheck.reset(); if(evt.data.startsWith('您'))&#123;//提示消息 $("#msg_tip").text(evt.data); console.log('显示tip') $("#msg_tip").fadeIn(); &#125;else&#123; var typeId = 'span#'+msgTypes[evt.data.split(':')[0]]; $(typeId).html(evt.data.split(':')[1]); if($navTab.getTab('dirMsg')!= '')&#123; debugger; //$navTab.refreshCurrentTabForm('dirMsg'); $forms = $("#dirMsg").find('.table-form'); for(var i=0;i&lt;$forms.length;i++)&#123; $navTab.submitForm($forms[i]); &#125; &#125; &#125; &#125;; //关闭连接 ws.onclose = function(evt) &#123; console.log("WebSocketClosed!"); &#125;; //发生异常 ws.onerror = function(evt) &#123; console.log("WebSocketError!"); ws = null; &#125;; window.onunload = function(event)&#123; if(ws)&#123; ws=null; &#125; &#125;; 后端（tomcat8) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687package com.css.apps.msg.websocket;/** * @Author: haoming * @Date: 2019/5/8 11:22 PM * @Version 1.0 */import com.css.apps.msg.constant.MsgType;import com.css.db.query.QueryCache;import org.apache.commons.logging.Log;import org.apache.commons.logging.LogFactory;import javax.websocket.*;import javax.websocket.server.ServerEndpoint;import java.io.IOException;import java.util.HashMap;import java.util.Map;/** * @Author: haoming * @Date: 2019/5/6 7:07 PM * @Version 1.0 */@ServerEndpoint(value = "/ws")public class WsServlet &#123; private static Log log = LogFactory.getLog(WsServlet.class); //设置Map,存放每个用户的连接 public static Map&lt;String,WsServlet&gt; webSocketSet = new HashMap&lt;String,WsServlet&gt;(); //浏览器与服务端的回话，浏览器每new一个WebSocket就创建一个session，关闭或刷新浏览器，session关闭 private Session session; //代表浏览器 private String userid; /** * 推送消息接口 * 外部可以进行调用 * @param sendMsg * @throws IOException */ public void sendMsg(String sendMsg) throws IOException &#123; log.info(this.userid+"发送消息:"+sendMsg); this.session.getBasicRemote().sendText(sendMsg); &#125; @OnOpen public void onOpen(Session session) throws IOException &#123; this.session = session; log.info(this+"有新连接,session="+session+";userid="+userid); &#125; @OnClose public void onClose() &#123; webSocketSet.remove(this.userid); log.info(this.userid+"；连接关闭"); &#125; @OnMessage public void onMessage(String info) throws IOException &#123; log.info(this.userid+"；来自客户端的消息:" + info); String msg = "服务端接收到了来自客户端的消息："+info; if(info.contains("userid"))&#123; this.userid = info.split("userid=")[1]; log.info("this.userid="+this.userid); webSocketSet.put(userid, this); //发送初始待办数量 sendMsg("1:"+getMsgNum(userid, MsgType.DAIBAN)); sendMsg("2:"+getMsgNum(userid, MsgType.DAIYUE)); sendMsg("3:"+getMsgNum(userid, MsgType.YOUJIAN)); sendMsg("4:"+getMsgNum(userid, MsgType.GWDAIBAN)); &#125; &#125; @OnError public void onError(Throwable error) &#123; log.error(this.userid+"；发生错误",error); &#125; private String getMsgNum(String userId,Integer msgType)&#123; String sql = ""; sql = "select count(a.uuid) from Msg a where a.receiver =:userId " + "and a.msgType =:msgType and a.msgStatus = 1"; QueryCache qc = new QueryCache(sql); qc.setParameter("userId",userId); qc.setParameter("msgType",msgType); return qc.uniqueResult().toString(); &#125;&#125; 总结(问题) 前端添加心跳机制防止连接超时断开，但发送心跳的时间网络上未找到具体的应该根据什么来设置？。 利用如下代码测试最大连接数 123456789101112131415161718192021222324for (var i = 0; i &lt; 300; i++) &#123; sleep(5) websocket(); &#125; function websocket() &#123; var wsUri = "ws://localhost:8888/msg/ws"; var ws = new WebSocket(wsUri); ws.onopen = function () &#123; ws.send("User connected"); &#125;; ws.onmessage = function (e) &#123; console.log(e.data) &#125;; ws.onclose = function () &#123; console.log("User disconnected"); &#125;; &#125; function sleep(n) &#123; var start = new Date().getTime(); while (true) if (new Date().getTime() - start &gt; n) break; &#125; 发现浏览器最大连接数达到200时，其他连接无法连上，修改tomcat的连接数等配置，会有小幅度增加，达到了256，但不理想?，tomcat配置如下： 12&lt;Executor maxThreads="500" minSpareThreads="20" name="tomcatThreadPool" namePrefix="catalina-exec-"/&gt; &lt;Connector URIEncoding="UTF-8" acceptCount="300" connectionTimeout="20000" enableLookups="false" executor="tomcatThreadPool" port="8080" protocol="org.apache.coyote.http11.Http11NioProtocol" redirectPort="8443"/&gt;]]></content>
  </entry>
  <entry>
    <title><![CDATA[kindeditor跨域问题解决]]></title>
    <url>%2F2019%2F09%2F05%2Fkindeditor%E8%B7%A8%E5%9F%9F%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3%2F</url>
    <content type="text"><![CDATA[问题背景因为在A系统中需要嵌入B系统的页面，两个系统有相同的父域名，所以使用了iframe+domain的方式解决跨域问题，在A系统的页面中加入了document.domain = &#39;exame.com.cn&#39;;;但是在嵌入kindeditor富文本编辑器后，上传图片出现跨域问题。 解决思路通过分析kindeditor的图片上传代码，如下 123456789101112&lt;div class="tab2" style=""&gt;&lt;iframe name="kindeditor_upload_iframe_1567676701755" style="display:none;"&gt;&lt;/iframe&gt; &lt;form class="ke-upload-area ke-form" method="post" enctype="multipart/form-data" target="kindeditor_upload_iframe_1567676701755" action="/portal/kindeditor/uploadImg?dir=image"&gt; &lt;div class="ke-dialog-row"&gt;&lt;label style="width:60px;"&gt;上传文件&lt;/label&gt;&lt;input type="text" name="localUrl" class="ke-input-text" tabindex="-1" style="width:200px;" readonly="true"&gt; &amp;nbsp;&lt;div class="ke-inline-block ke-upload-button"&gt; &lt;div class="ke-upload-area"&gt;&lt;span class="ke-button-common"&gt;&lt;input type="button" class="ke-button-common ke-button" value="浏览..."&gt;&lt;/span&gt;&lt;input type="file" class="ke-upload-file" name="uploadFile" style="width: 60px;"&gt;&lt;/div&gt; &lt;/div&gt;&lt;input type="button" class="ke-upload-button" value="浏览..." style="display: none;"&gt;&lt;/div&gt; &lt;/form&gt; &lt;/div&gt; 上传后的返回值会放入iframe中，所以只要在iframe中加入document.domain = &#39;exame.com.cn&#39;;就能解决跨域问题。 解决方法参照kindeditor包中的uplod_json.jsp，重新写了一个UploadImgServlet，最终代码如下 前端代码(正常写，不需要特殊处理) 12345678910111213editor = KindEditor.create('textarea[rel="ehContentOnlyOne"]',&#123; resizeType : 1, allowPreviewEmoticons : false, allowImageUpload : true,//上传图片框本地上传的功能，false为隐藏，默认为true allowImageRemote : false,//上传图片框网络图片的功能，false为隐藏，默认为true cssPath:['cssui/main/editor.css'], filePostName: "uploadFile", uploadJson : '/portal/kindeditor/uploadImg',// 上传图片接口 items : [ 'source','preview','code','|','fontname', 'fontsize', '|', 'forecolor', 'hilitecolor', 'bold', 'italic', 'underline', 'removeformat', '|', 'justifyleft', 'justifycenter', 'justifyright', 'insertorderedlist', 'insertunorderedlist', '|', 'emoticons','image', 'link'] &#125;); 后端代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142package com.css.bbs.bbs.action;import net.sf.json.JSONObject;import org.apache.commons.fileupload.FileItem;import org.apache.commons.fileupload.FileItemFactory;import org.apache.commons.fileupload.FileUploadException;import org.apache.commons.fileupload.disk.DiskFileItemFactory;import org.apache.commons.fileupload.servlet.ServletFileUpload;import javax.servlet.ServletException;import javax.servlet.http.HttpServlet;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import java.io.File;import java.io.IOException;import java.io.PrintWriter;import java.text.SimpleDateFormat;import java.util.*;/** * @Author: haoming * @Date: 2019/9/3 10:36 AM * @Version 1.0 */public class UploadImgServlet extends HttpServlet &#123; protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; //设置Response响应的编码 response.setContentType("text/html; charset=UTF-8"); PrintWriter out = response.getWriter(); //文件保存目录路径 String savePath = request.getServletContext().getRealPath("/") + "attached/"; //文件保存目录URL String saveUrl = request.getContextPath() + "/attached/"; //定义允许上传的文件扩展名 HashMap&lt;String, String&gt; extMap = new HashMap&lt;String, String&gt;(); extMap.put("image", "gif,jpg,jpeg,png,bmp"); extMap.put("flash", "swf,flv"); extMap.put("media", "swf,flv,mp3,wav,wma,wmv,mid,avi,mpg,asf,rm,rmvb"); extMap.put("file", "doc,docx,xls,xlsx,ppt,htm,html,txt,zip,rar,gz,bz2"); //最大文件大小 long maxSize = 1000000; if(!ServletFileUpload.isMultipartContent(request))&#123; out.println(getError("请选择文件。")); return; &#125; //检查目录 File uploadDir = new File(savePath); if(!uploadDir.isDirectory())&#123; uploadDir.mkdirs(); &#125; //检查目录写权限 if(!uploadDir.canWrite())&#123; out.println(getError("上传目录没有写权限。")); return; &#125; String dirName = request.getParameter("dir"); if (dirName == null) &#123; dirName = "image"; &#125; if(!extMap.containsKey(dirName))&#123; out.println(getError("目录名不正确。")); return; &#125; //创建文件夹 savePath += dirName + "/"; saveUrl += dirName + "/"; File saveDirFile = new File(savePath); if (!saveDirFile.exists()) &#123; saveDirFile.mkdirs(); &#125; SimpleDateFormat sdf = new SimpleDateFormat("yyyyMMdd"); String ymd = sdf.format(new Date()); savePath += ymd + "/"; saveUrl += ymd + "/"; File dirFile = new File(savePath); if (!dirFile.exists()) &#123; dirFile.mkdirs(); &#125; FileItemFactory factory = new DiskFileItemFactory(); ServletFileUpload upload = new ServletFileUpload(factory); upload.setHeaderEncoding("UTF-8"); List items = null; try &#123; items = upload.parseRequest(request); &#125; catch (FileUploadException e) &#123; out.println(getError("上传文件失败。")); return; &#125; Iterator itr = items.iterator(); while (itr.hasNext()) &#123; FileItem item = (FileItem) itr.next(); String fileName = item.getName(); long fileSize = item.getSize(); if (!item.isFormField()) &#123; //检查文件大小 if (item.getSize() &gt; maxSize) &#123; out.println(getError("上传文件大小超过限制。")); return; &#125; //检查扩展名 String fileExt = fileName.substring(fileName.lastIndexOf(".") + 1).toLowerCase(); if (!Arrays.&lt;String&gt;asList(extMap.get(dirName).split(",")).contains(fileExt)) &#123; out.println(getError("上传文件扩展名是不允许的扩展名。\n只允许" + extMap.get(dirName) + "格式。")); return; &#125; SimpleDateFormat df = new SimpleDateFormat("yyyyMMddHHmmss"); String newFileName = df.format(new Date()) + "_" + new Random().nextInt(1000) + "." + fileExt; try &#123; File uploadedFile = new File(savePath, newFileName); item.write(uploadedFile); &#125; catch (Exception e) &#123; out.println(getError("上传文件失败。")); return; &#125; JSONObject obj = new JSONObject(); obj.put("error", 0); obj.put("url", saveUrl + newFileName); //解决跨域问题 out.println(obj.toString()+"&lt;script&gt;document.domain='cec.com.cn';&lt;/script&gt;"); &#125; &#125; //将writer对象中的内容输出 out.flush(); //关闭writer对象 out.close(); &#125; protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; doPost(request,response); &#125; private String getError(String message) &#123; JSONObject obj = new JSONObject(); obj.put("error", 1); obj.put("message", message); return obj.toString(); &#125;&#125; web.xml配置 123456789&lt;servlet&gt; &lt;servlet-name&gt;uploadImg&lt;/servlet-name&gt; &lt;servlet-class&gt;com.css.bbs.bbs.action.UploadImgServlet&lt;/servlet-class&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;uploadImg&lt;/servlet-name&gt; &lt;url-pattern&gt;/kindeditor/uploadImg&lt;/url-pattern&gt;&lt;/servlet-mapping&gt;]]></content>
  </entry>
  <entry>
    <title><![CDATA[tomcat源码4-Pipeline-Value管道]]></title>
    <url>%2F2019%2F09%2F02%2Ftomcat%E6%BA%90%E7%A0%814-Pipeline-Value%E7%AE%A1%E9%81%93%2F</url>
    <content type="text"><![CDATA[每个容器（Engine/Host/Context/Wrap）包含一个pipeline，每个pipeline包含一个valve集合，位于前面的valve做完业务处理后将调用后面的valve做业务处理，而容器的缺省valve位于集合的最后一个位置，负责调用下层容器的pipeline的第一个valve做请求处理。调用会从Engine的第一个valve调用开始，一直执行到调用Wrapper的缺省valve：StandardWrapperValve，而filter与servlet的处理就是在这个valve中进行的 。Engine的第一个valve是由Adapter调用的，在connector章节中也看到CoyoteAdapter在处理完request以后会执行connector.getContainer().getPipeline().getFirst().invoke(request, response)。 123456789101112131415&lt;&lt;ContainerBase&gt;&gt;/** * The Pipeline object with which this Container is associated. */ protected final Pipeline pipeline = new StandardPipeline(this); @Override protected synchronized void startInternal() throws LifecycleException &#123; ....... // Start the Valves in our pipeline (including the basic), if any if (pipeline instanceof Lifecycle) &#123; ((Lifecycle) pipeline).start(); &#125; ....... &#125;]]></content>
  </entry>
  <entry>
    <title><![CDATA[tomcat源码3-container]]></title>
    <url>%2F2019%2F08%2F29%2Ftomcat%E6%BA%90%E7%A0%813-container%2F</url>
    <content type="text"><![CDATA[Container 的4 个子容器 Container 的子容器Engine 、Host 、Context 、Wrapper 是逐层包含的关系，其中Engine是最顶层，每个service 最多只能有一个Engine, Engine 里面可以有多个Host ，每个Host 下可以有多个Context ，每个Context 下可以有多个Wrapper，它们的装配关系如下图所示。 Engine ：引擎，用来管理多个站点， 一个Service 最多只能有一个Engine。 Host ：代表一个站点，也可以叫虚拟主机，通过配置Host 就可以添加站点。 Context ：代表一个应用程序，对应着平时开发的一套程序，或者一个WEB-INF 目录以及下面的web.xml 文件。 Wrapper ：每个Wrapper 封装着一个servlet。 ​ Context 和Host 的区别是Context 表示一个应用，比如，默认配置下webapps 下的每个目录都是一个应用，其中ROOT目录中存放着主应用，其他目录存放着别的子应用，而整个webapps 是一个站点。假如www.haominglfs.com 域名对应着webapps 目录所代表的站点，其中的ROOT 目录里的应用就是主应用，访问时直接使用域名就可以，而webapps/test 目录存放的是test 子应用，访问时需要www.excelib.com/test ，每一个应用对应一个Context ，所有webapps 下的应用都属于www.haominglfs.com 站点，而blog.haominglfs.com 则是另外一个站点，属于另外一个Host。 配置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!--定义了－个Server ，在8005 端口监听关闭命令“ SHUTDOWN ”；--&gt;&lt;Server port="8005" shutdown="SHUTDOWN"&gt; &lt;Listener className="org.apache.catalina.startup.VersionLoggerListener" /&gt; &lt;!-- Security listener. Documentation at /docs/config/listeners.html &lt;Listener className="org.apache.catalina.security.SecurityListener" /&gt; --&gt; &lt;!--APR library loader. Documentation at /docs/apr.html --&gt; &lt;Listener className="org.apache.catalina.core.AprLifecycleListener" SSLEngine="on" /&gt; &lt;!-- Prevent memory leaks due to use of particular java/javax APIs--&gt; &lt;Listener className="org.apache.catalina.core.JreMemoryLeakPreventionListener" /&gt; &lt;Listener className="org.apache.catalina.mbeans.GlobalResourcesLifecycleListener" /&gt; &lt;Listener className="org.apache.catalina.core.ThreadLocalLeakPreventionListener" /&gt; &lt;!-- Global JNDI resources Documentation at /docs/jndi-resources-howto.html --&gt; &lt;GlobalNamingResources&gt; &lt;!-- Editable user database that can also be used by UserDatabaseRealm to authenticate users --&gt; &lt;Resource name="UserDatabase" auth="Container" type="org.apache.catalina.UserDatabase" description="User database that can be updated and saved" factory="org.apache.catalina.users.MemoryUserDatabaseFactory" pathname="conf/tomcat-users.xml" /&gt; &lt;/GlobalNamingResources&gt; &lt;!-- A "Service" is a collection of one or more "Connectors" that share a single "Container" Note: A "Service" is not itself a "Container", so you may not define subcomponents such as "Valves" at this level. Documentation at /docs/config/service.html --&gt; &lt;!-- 定义了一个名为Catalina的Service --&gt; &lt;Service name="Catalina"&gt; &lt;Connector port="8080" protocol="HTTP/1.1" connectionTimeout="20000" redirectPort="8443" /&gt; &lt;!-- Define an AJP 1.3 Connector on port 8009 --&gt; &lt;!-- AJP 主要用于集成（如与Apache 集成） --&gt; &lt;Connector port="8009" protocol="AJP/1.3" redirectPort="8443" /&gt; &lt;!-- You should set jvmRoute to support load-balancing via AJP ie : &lt;Engine name="Catalina" defaultHost="localhost" jvmRoute="jvm1"&gt; --&gt; &lt;!--定义了一个名为Catalina 的Engine defaultHost 属性，它表示接收到请求的域名如果在所有的Host 的name 和Alias 中都找不到时使用的默认Host --&gt; &lt;Engine name="Catalina" defaultHost="localhost"&gt; &lt;!-- Use the LockOutRealm to prevent attempts to guess user passwords via a brute-force attack --&gt; &lt;Realm className="org.apache.catalina.realm.LockOutRealm"&gt; &lt;!-- This Realm uses the UserDatabase configured in the global JNDI resources under the key "UserDatabase". Any edits that are performed against this UserDatabase are immediately available for use by the Realm. --&gt; &lt;Realm className="org.apache.catalina.realm.UserDatabaseRealm" resourceName="UserDatabase"/&gt; &lt;/Realm&gt; &lt;!--定义了一个名为localhost 的Host name属性代表域名，所以上面定义的站点可以通过localhost 访问 appBase 属性指定站点的位置，比如，上面定义的站点就是默认的webapps 目录， unpackWARs 属性表示是否自动解压war 文件， autoDeploy 属性表示是否自动部署，如果autoDeploy 为true 那么Tomcat 在运行过程中在webapps 目录中加入新的应用将会自动部署并启动。另外Host 还有一个Alias 子标签，可以通过这个标签来定义别名，如果有多个域名访问同一个站点就可以这么定义 --&gt; &lt;Host name="localhost" appBase="webapps" unpackWARs="true" autoDeploy="true"&gt; &lt;!-- Access log processes all example. Documentation at: /docs/config/valve.html Note: The pattern used is equivalent to using pattern="common" --&gt; &lt;Valve className="org.apache.catalina.valves.AccessLogValve" directory="logs" prefix="localhost_access_log" suffix=".txt" pattern="%h %l %u %t &amp;quot;%r&amp;quot; %s %b" /&gt; &lt;/Host&gt; &lt;/Engine&gt; &lt;/Service&gt;&lt;/Server&gt; Context 通过文件配置的方式一共有5 个位置可以配置： conf/server.xml 文件中的Context 标签。 conf/[ enginename ]/[hostname ］／目录下以应用命名的xml 文件。 应用自己的／META-INF/context.xml 文件。 conf/context.xml 文件。 conf/[ enginename ]/[hostname ]/context.xml.default 文件。 其中前三个位置用于配置单独的应用，后两个配置的Context 是共享的， conf/context.xml文件中配置的内容在整个Tomcat 中共享;第5 种配置的内容在对应的站点（ Host ）中共享。另外，因为conf/server.xrnl 文件只有在Tomcat 重启的时候才会重新加载，所以第一种配置方法不推荐使用。 Wrapper 的配置就是我们在web.xml 中配置的Servlet ， 一个Servlet 对应一个Wrapper。另外也可以在conf/web.xml 文件中配置全局的Wrapper，处理Jsp 的JspServlet 就配置在这里，所以不需要自己配置Jsp 就可以处理Jsp 请求了。 Container 的启动Container 的启动是通过init 和start 方法来完成的，在前面分析过这两个方法会在Tomcat启动时被Service 调用。Container 也是按照Tomcat 的生命周期来管理的， init 和start 方法也会调用initlntemal 和startintemal 方法来具体处理，不过Container 和前面讲的Tomcat 整体结构启动的过程稍微有点不一样，主要有三点区别： Container 的4 个子容器有一个共同的父类ContainerBase ，这里定义了Container 容器的initlntemal和startlnternal 方法通用处理内容，具体容器还可以添加向己的内容； 除了最顶层容器的init 是被Service 调用的,子容器的init 方法并不是在容器中逐层循环调用的，而是在执行start 方法的时候通过状态判断还没有初始化才会调用； start 方法除了在父容器的startlnternal 方法中调用，还会在父容器的添加子容器的addChild 方法中调用，这主要是因为Context 和Wrapper 是动态添加的，我们在站点目录下放一个应用的文件夹或者war 包就可以添加一个Context ，在web.xml 文件中配置一个Servlet 就可以添加一个Wrapper ，所以Context 和Wrapper 是在容器启动的过程中才动态查找出来添加到相应的父容器中的。 ContainerBase123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198 &lt;&lt;ContainerBase&gt;&gt; @Override protected void initInternal() throws LifecycleException &#123; //初始化ThreadPoolExecutor 类型的startStopExecutor属性，用于管理启动和关闭的线程 BlockingQueue&lt;Runnable&gt; startStopQueue = new LinkedBlockingQueue&lt;&gt;(); startStopExecutor = new ThreadPoolExecutor( getStartStopThreadsInternal(), getStartStopThreadsInternal(), 10, TimeUnit.SECONDS, startStopQueue, new StartStopThreadFactory(getName() + "-startStop-")); startStopExecutor.allowCoreThreadTimeOut(true); super.initInternal(); &#125; //ContainerBase 的startlntemal 方法主要做了5件事： //如果有Cluster 和Realm 则调用其start方法； //调用所有子容器的start方法启动子容器； //调用管道中Value的start方法来启动管道； //启动完成后将生命周期状态设置为LifecycleState.STARTING状态； //启用后台线程定时处理一些事情。 @Override protected synchronized void startInternal() throws LifecycleException &#123; // Start our subordinate components, if any logger = null; getLogger(); //Cluster用于配置集群,它的作用就是同步Session Cluster cluster = getClusterInternal(); if (cluster instanceof Lifecycle) &#123; ((Lifecycle) cluster).start(); &#125; //Realm是Tomcat的安全域，可以用来管理资源的访问权限 Realm realm = getRealmInternal(); if (realm instanceof Lifecycle) &#123; ((Lifecycle) realm).start(); &#125; //子容器是使用startStopExecutor调用新线程来启动的，这样可以用多个线程来同时启动，效率更高 //遍历Future 主要有两个作用：①其get方法是阻塞的，只有线程处理完之后才会向下走，这就保证了管道Pipeline 启动之前容器已经启动完成了；②可以处理启动过程中遇到的异常。 // Start our child containers, if any Container children[] = findChildren(); List&lt;Future&lt;Void&gt;&gt; results = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; children.length; i++) &#123; results.add(startStopExecutor.submit(new StartChild(children[i]))); &#125; MultiThrowable multiThrowable = new MultiThrowable(); for (Future&lt;Void&gt; result : results) &#123; try &#123; result.get(); &#125; catch (Throwable e) &#123; log.error(sm.getString("containerBase.threadedStartFailed"), e); multiThrowable.add(e); &#125; &#125; if (multiThrowable.size() &gt; 0) &#123; throw new LifecycleException(sm.getString("containerBase.threadedStartFailed"), multiThrowable.getThrowable()); &#125; //启动容器的管道 // Start the Valves in our pipeline (including the basic), if any if (pipeline instanceof Lifecycle) &#123; ((Lifecycle) pipeline).start(); &#125; setState(LifecycleState.STARTING); // Start our thread threadStart(); &#125;/** * Start the background thread that will periodically check for * session timeouts. */ //其实这个私有方法是start()方法中最重要的方法 protected void threadStart() &#123; if (thread != null) return; //如果backgroundProcessorDelay 不大于0，那么方法就停止,默认值为 -1。 //子容器中，只有StandardEngine设置这个值为10，其他三个容器默认为-1，说明只有StandardEngine在start()方法调用的时候才会走这个方法，其他容器这个方法是走不到下面代码的 if (backgroundProcessorDelay &lt;= 0) return; threadDone = false; String threadName = "ContainerBackgroundProcessor[" + toString() + "]"; thread = new Thread(new ContainerBackgroundProcessor(), threadName); thread.setDaemon(true); thread.start(); &#125;// -------------------------------------- ContainerExecuteDelay Inner Class /** * Private thread class to invoke the backgroundProcess method * of this container and its children after a fixed delay. */ protected class ContainerBackgroundProcessor implements Runnable &#123; //ContainerBackgroundProcessor是个Runnable接口的实现类，查看其run方法 @Override public void run() &#123; Throwable t = null; String unexpectedDeathMessage = sm.getString( "containerBase.backgroundProcess.unexpectedThreadDeath", Thread.currentThread().getName()); try &#123; while (!threadDone) &#123; try &#123; Thread.sleep(backgroundProcessorDelay * 1000L); &#125; catch (InterruptedException e) &#123; // Ignore &#125; if (!threadDone) &#123; processChildren(ContainerBase.this); &#125; &#125; &#125; catch (RuntimeException|Error e) &#123; t = e; throw e; &#125; finally &#123; if (!threadDone) &#123; log.error(unexpectedDeathMessage, t); &#125; &#125; &#125; protected void processChildren(Container container) &#123; ClassLoader originalClassLoader = null; try &#123; if (container instanceof Context) &#123; Loader loader = ((Context) container).getLoader(); // Loader will be null for FailedContext instances if (loader == null) &#123; return; &#125; // Ensure background processing for Contexts and Wrappers // is performed under the web app's class loader originalClassLoader = ((Context) container).bind(false, null); &#125; container.backgroundProcess(); //获取所有的子容器，然后遍历每个子容器来调用他们的processChildren()方法 Container[] children = container.findChildren(); for (int i = 0; i &lt; children.length; i++) &#123; if (children[i].getBackgroundProcessorDelay() &lt;= 0) &#123; processChildren(children[i]); &#125; &#125; &#125; catch (Throwable t) &#123; ExceptionUtils.handleThrowable(t); log.error("Exception invoking periodic operation: ", t); &#125; finally &#123; if (container instanceof Context) &#123; ((Context) container).unbind(false, originalClassLoader); &#125; &#125; &#125; &#125;/** * Execute a periodic task, such as reloading, etc. This method will be * invoked inside the classloading context of this container. Unexpected * throwables will be caught and logged. */ @Override public void backgroundProcess() &#123; if (!getState().isAvailable()) return; //Catalina的集群组件，通过cluster组件可以实现：集群应用部署，即多个Tomcat实例，不需要每个都分别部署应用，只需要在某个实例上部署，整个集群中的各个实例都会自动同步应用进行部署。那么他的backgroundProcess()方法主要的功能是监听指定文件夹下有没有新增的war包或者文件是新增的还是修改的已决定来重新部署和通知其他tomcat集群成员。 Cluster cluster = getClusterInternal(); if (cluster != null) &#123; try &#123; cluster.backgroundProcess(); &#125; catch (Exception e) &#123; log.warn(sm.getString("containerBase.backgroundProcess.cluster", cluster), e); &#125; &#125; //Catalina中的安全组件，backgroundProcess()方法主要的功能是，貌似是跟servlet的安全校验有关。 Realm realm = getRealmInternal(); if (realm != null) &#123; try &#123; realm.backgroundProcess(); &#125; catch (Exception e) &#123; log.warn(sm.getString("containerBase.backgroundProcess.realm", realm), e); &#125; &#125; //容器的pipeline组件，这里是遍历整个pipeline链表，分别调用backgroundProcess()方法 Valve current = pipeline.getFirst(); while (current != null) &#123; try &#123; current.backgroundProcess(); &#125; catch (Exception e) &#123; log.warn(sm.getString("containerBase.backgroundProcess.valve", current), e); &#125; current = current.getNext(); &#125; fireLifecycleEvent(Lifecycle.PERIODIC_EVENT, null); &#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&lt;&lt;StandardContext&gt;&gt;@Override public void backgroundProcess() &#123; if (!getState().isAvailable()) return; //Catalina在启动过程中创建的classloader的实例,backgroundProcess()方法主要的功能是查看Context容器是否需要重新加载,热部署就是利用这个机制来完成的 Loader loader = getLoader(); if (loader != null) &#123; try &#123; loader.backgroundProcess(); &#125; catch (Exception e) &#123; log.warn(sm.getString( "standardContext.backgroundProcess.loader", loader), e); &#125; &#125; //Catalina中的Session管理器，backgroundProcess()方法主要的功能是将过期会话(session)置为无效 Manager manager = getManager(); if (manager != null) &#123; try &#123; manager.backgroundProcess(); &#125; catch (Exception e) &#123; log.warn(sm.getString( "standardContext.backgroundProcess.manager", manager), e); &#125; &#125; WebResourceRoot resources = getResources(); if (resources != null) &#123; try &#123; resources.backgroundProcess(); &#125; catch (Exception e) &#123; log.warn(sm.getString( "standardContext.backgroundProcess.resources", resources), e); &#125; &#125; InstanceManager instanceManager = getInstanceManager(); if (instanceManager instanceof DefaultInstanceManager) &#123; try &#123; ((DefaultInstanceManager)instanceManager).backgroundProcess(); &#125; catch (Exception e) &#123; log.warn(sm.getString( "standardContext.backgroundProcess.instanceManager", resources), e); &#125; &#125; super.backgroundProcess(); &#125; 123456789101112&lt;&lt;StandardWrapper&gt;&gt; @Override public void backgroundProcess() &#123; super.backgroundProcess(); if (!getState().isAvailable()) return; if (getServlet() instanceof PeriodicEventListener) &#123; ((PeriodicEventListener) getServlet()).periodicEvent(); &#125; &#125; threadStart 方法启动的后台线程是一个while 循环，内部会定期调用backgroundProcess 方法做一些事情，间隔时间的长短是通过ContainerBase 的backgroundProcessor Delay 属性来设置的，单位是秒，如果小于0 就不启动后台线程了，不过其backgroundProcess 方法会在父容器的后台线程中调用。backgroundProcess 方法是Container 接口中的一个方法， 一共有3 个实现，分别在ContainerBase 、StandardContext 和StandardWrapper 中， ContainerBase 中提供了所有容器共同的处理过程， StandardContext 和StandardWrapper 的backgroundProcess 方法除了处理自己相关的业务，也调用ContainerBase 中的处理。ContainerBase 的backgroundProcess 方法中调用了Cluster 、Realm 和管道的backgroundProcess 方法； StandardContext 的backgroundProcess方法中对Session 过期和资源变化进行了处理； StandardWrapper 的backgroundProcess方法会对Jsp 生成的Servlet 定期进行检查。 Engine1234567891011121314151617181920&lt;&lt;StandardEngine&gt;&gt;@Override protected void initInternal() throws LifecycleException &#123; // Ensure that a Realm is present before any attempt is made to start // one. This will create the default NullRealm if necessary. //如果没有配置Realm ，则使用一个默认的NullRealm getRealm(); super.initInternal(); &#125; @Override protected synchronized void startInternal() throws LifecycleException &#123; // Log our server identification information if(log.isInfoEnabled()) log.info( "Starting Servlet Engine: " + ServerInfo.getServerInfo()); // Standard container startup super.startInternal(); &#125; Host1234567891011121314151617181920212223242526272829303132333435363738&lt;&lt;StandardHost&gt;&gt;//没有重写initInternal方法//这里的代码看起来虽然比较多，但功能却非常简单，就是检查Host的管道中有没有指定的Value ，如果没有则添加进去。检查的方法是遍历所有的Value然后通过名字判断的，检查的Value的类型通过getErrorReportValveClass方法获取，它返回errorReportValveClass属性，可以配置，默认值是org.apache.catalina.valves.ErrorReportValve @Override protected synchronized void startInternal() throws LifecycleException &#123; // Set error report valve String errorValve = getErrorReportValveClass(); if ((errorValve != null) &amp;&amp; (!errorValve.equals(""))) &#123; try &#123; boolean found = false; Valve[] valves = getPipeline().getValves(); for (Valve valve : valves) &#123; if (errorValve.equals(valve.getClass().getName())) &#123; found = true; break; &#125; &#125; if(!found) &#123; Valve valve = (Valve) Class.forName(errorValve).getConstructor().newInstance(); getPipeline().addValve(valve); &#125; &#125; catch (Throwable t) &#123; ExceptionUtils.handleThrowable(t); log.error(sm.getString( "standardHost.invalidErrorReportValveClass", errorValve), t); &#125; &#125; super.startInternal(); &#125; private String errorReportValveClass = "org.apache.catalina.valves.ErrorReportValve";public String getErrorReportValveClass() &#123; return (this.errorReportValveClass); &#125; 注册监听器 123456789101112131415161718192021222324252627282930313233343536373839&lt;&lt;Catalina&gt;&gt;/** * Create and configure the Digester we will be using for startup. * @return the main digester to parse server.xml */ protected Digester createStartDigester() &#123; long t1=System.currentTimeMillis(); // Initialize the digester Digester digester = new Digester(); digester.setValidating(false); digester.setRulesValidation(true); HashMap&lt;Class&lt;?&gt;, List&lt;String&gt;&gt; fakeAttributes = new HashMap&lt;&gt;(); .......... // Add RuleSets for nested elements digester.addRuleSet(new NamingRuleSet("Server/GlobalNamingResources/")); digester.addRuleSet(new EngineRuleSet("Server/Service/")); digester.addRuleSet(new HostRuleSet("Server/Service/Engine/")); digester.addRuleSet(new ContextRuleSet("Server/Service/Engine/Host/")); addClusterRuleSet(digester, "Server/Service/Engine/Host/Cluster/"); digester.addRuleSet(new NamingRuleSet("Server/Service/Engine/Host/Context/")); .......... &lt;&lt;HostRuleSet&gt;&gt; @Override public void addRuleInstances(Digester digester) &#123; digester.addObjectCreate(prefix + "Host", "org.apache.catalina.core.StandardHost", "className"); digester.addSetProperties(prefix + "Host"); digester.addRule(prefix + "Host", new CopyParentClassLoaderRule()); //添加hostConfig监听器 digester.addRule(prefix + "Host", new LifecycleListenerRule ("org.apache.catalina.startup.HostConfig", "hostConfigClass")); ......... &#125; Host 的启动除了startlntemal 方法，还有HostConfig 中相应的方法， HostConfig 继承自LifecycleListener 的监听器（ Engine 也有对应的EngineConfig 监昕器，不过里面只是简单地做了日志记录），在接收到Lifecycle.START_EVENT 事件时会调用start 方法来启动， HostConfig 的start 方法会检查配置的Host 站点配置的位置是否存在以及是不是目录，最后调用deployApps 方法部署应用， deployApps 方法代码如下 123456789101112&lt;&lt;HostConfig&gt;&gt;protected void deployApps() &#123; File appBase = host.getAppBaseFile(); File configBase = host.getConfigBaseFile(); String[] filteredAppPaths = filterAppPaths(appBase.list()); // Deploy XML descriptors from configBase deployDescriptors(configBase, configBase.list()); // Deploy WARs deployWARs(appBase, filteredAppPaths); // Deploy expanded folders deployDirectories(appBase, filteredAppPaths); &#125; 一共有三种部署方式：通过XML 描述文件、通过WAR 文件和通过文件夹部署。XML文件指的是conf/[enginename ]/[hostname ]/* .xml 文件， WAR 文件和文件夹是Host 站点目录下的WAR 文件和文件夹，这里会自动找出来并部署上，所以我们如果要添加应用只需要直接放在Host 站点的目录下就可以了。部署完成后，会将部署的Context 通过StandardHost 的add Child 方法添加到Host 里面。StandardHost 的addChild 方法会调用父类ContainerBase 的addChild 方法， 其中会调用子类（这里指Context ）的start 方法来启动子容器。 Context123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334&lt;&lt;StandardContext&gt;&gt;@Override protected synchronized void startInternal() throws LifecycleException &#123; if(log.isDebugEnabled()) log.debug("Starting " + getBaseName()); // Send j2ee.state.starting notification if (this.getObjectName() != null) &#123; Notification notification = new Notification("j2ee.state.starting", this.getObjectName(), sequenceNumber.getAndIncrement()); broadcaster.sendNotification(notification); &#125; setConfigured(false); boolean ok = true; // Currently this is effectively a NO-OP but needs to be called to // ensure the NamingResources follows the correct lifecycle if (namingResources != null) &#123; namingResources.start(); &#125; // Post work directory postWorkDirectory(); // Add missing components as necessary if (getResources() == null) &#123; // (1) Required by Loader if (log.isDebugEnabled()) log.debug("Configuring default Resources"); try &#123; setResources(new StandardRoot(this)); &#125; catch (IllegalArgumentException e) &#123; log.error(sm.getString("standardContext.resourcesInit"), e); ok = false; &#125; &#125; if (ok) &#123; resourcesStart(); &#125; if (getLoader() == null) &#123; WebappLoader webappLoader = new WebappLoader(getParentClassLoader()); webappLoader.setDelegate(getDelegate()); setLoader(webappLoader); &#125; // An explicit cookie processor hasn't been specified; use the default if (cookieProcessor == null) &#123; cookieProcessor = new Rfc6265CookieProcessor(); &#125; // Initialize character set mapper getCharsetMapper(); // Validate required extensions boolean dependencyCheck = true; try &#123; dependencyCheck = ExtensionValidator.validateApplication (getResources(), this); &#125; catch (IOException ioe) &#123; log.error(sm.getString("standardContext.extensionValidationError"), ioe); dependencyCheck = false; &#125; if (!dependencyCheck) &#123; // do not make application available if dependency check fails ok = false; &#125; // Reading the "catalina.useNaming" environment variable String useNamingProperty = System.getProperty("catalina.useNaming"); if ((useNamingProperty != null) &amp;&amp; (useNamingProperty.equals("false"))) &#123; useNaming = false; &#125; if (ok &amp;&amp; isUseNaming()) &#123; if (getNamingContextListener() == null) &#123; NamingContextListener ncl = new NamingContextListener(); ncl.setName(getNamingContextName()); ncl.setExceptionOnFailedWrite(getJndiExceptionOnFailedWrite()); addLifecycleListener(ncl); setNamingContextListener(ncl); &#125; &#125; // Standard container startup if (log.isDebugEnabled()) log.debug("Processing standard container startup"); // Binding thread ClassLoader oldCCL = bindThread(); try &#123; if (ok) &#123; // Start our subordinate components, if any Loader loader = getLoader(); if (loader instanceof Lifecycle) &#123; ((Lifecycle) loader).start(); &#125; // since the loader just started, the webapp classloader is now // created. setClassLoaderProperty("clearReferencesRmiTargets", getClearReferencesRmiTargets()); setClassLoaderProperty("clearReferencesStopThreads", getClearReferencesStopThreads()); setClassLoaderProperty("clearReferencesStopTimerThreads", getClearReferencesStopTimerThreads()); setClassLoaderProperty("clearReferencesHttpClientKeepAliveThread", getClearReferencesHttpClientKeepAliveThread()); setClassLoaderProperty("clearReferencesObjectStreamClassCaches", getClearReferencesObjectStreamClassCaches()); // By calling unbindThread and bindThread in a row, we setup the // current Thread CCL to be the webapp classloader unbindThread(oldCCL); oldCCL = bindThread(); // Initialize logger again. Other components might have used it // too early, so it should be reset. logger = null; getLogger(); Realm realm = getRealmInternal(); if(null != realm) &#123; if (realm instanceof Lifecycle) &#123; ((Lifecycle) realm).start(); &#125; // Place the CredentialHandler into the ServletContext so // applications can have access to it. Wrap it in a "safe" // handler so application's can't modify it. CredentialHandler safeHandler = new CredentialHandler() &#123; @Override public boolean matches(String inputCredentials, String storedCredentials) &#123; return getRealmInternal().getCredentialHandler().matches(inputCredentials, storedCredentials); &#125; @Override public String mutate(String inputCredentials) &#123; return getRealmInternal().getCredentialHandler().mutate(inputCredentials); &#125; &#125;; context.setAttribute(Globals.CREDENTIAL_HANDLER, safeHandler); &#125; // Notify our interested LifecycleListeners fireLifecycleEvent(Lifecycle.CONFIGURE_START_EVENT, null); // Start our child containers, if not already started for (Container child : findChildren()) &#123; if (!child.getState().isAvailable()) &#123; child.start(); &#125; &#125; // Start the Valves in our pipeline (including the basic), // if any if (pipeline instanceof Lifecycle) &#123; ((Lifecycle) pipeline).start(); &#125; // Acquire clustered manager Manager contextManager = null; Manager manager = getManager(); if (manager == null) &#123; if (log.isDebugEnabled()) &#123; log.debug(sm.getString("standardContext.cluster.noManager", Boolean.valueOf((getCluster() != null)), Boolean.valueOf(distributable))); &#125; if ( (getCluster() != null) &amp;&amp; distributable) &#123; try &#123; contextManager = getCluster().createManager(getName()); &#125; catch (Exception ex) &#123; log.error("standardContext.clusterFail", ex); ok = false; &#125; &#125; else &#123; contextManager = new StandardManager(); &#125; &#125; // Configure default manager if none was specified if (contextManager != null) &#123; if (log.isDebugEnabled()) &#123; log.debug(sm.getString("standardContext.manager", contextManager.getClass().getName())); &#125; setManager(contextManager); &#125; if (manager!=null &amp;&amp; (getCluster() != null) &amp;&amp; distributable) &#123; //let the cluster know that there is a context that is distributable //and that it has its own manager getCluster().registerManager(manager); &#125; &#125; if (!getConfigured()) &#123; log.error(sm.getString("standardContext.configurationFail")); ok = false; &#125; // We put the resources into the servlet context if (ok) getServletContext().setAttribute (Globals.RESOURCES_ATTR, getResources()); if (ok ) &#123; if (getInstanceManager() == null) &#123; javax.naming.Context context = null; if (isUseNaming() &amp;&amp; getNamingContextListener() != null) &#123; context = getNamingContextListener().getEnvContext(); &#125; Map&lt;String, Map&lt;String, String&gt;&gt; injectionMap = buildInjectionMap( getIgnoreAnnotations() ? new NamingResourcesImpl(): getNamingResources()); setInstanceManager(new DefaultInstanceManager(context, injectionMap, this, this.getClass().getClassLoader())); &#125; getServletContext().setAttribute( InstanceManager.class.getName(), getInstanceManager()); InstanceManagerBindings.bind(getLoader().getClassLoader(), getInstanceManager()); &#125; // Create context attributes that will be required if (ok) &#123; getServletContext().setAttribute( JarScanner.class.getName(), getJarScanner()); &#125; // Set up the context init params mergeParameters(); // Call ServletContainerInitializers for (Map.Entry&lt;ServletContainerInitializer, Set&lt;Class&lt;?&gt;&gt;&gt; entry : initializers.entrySet()) &#123; try &#123; entry.getKey().onStartup(entry.getValue(), getServletContext()); &#125; catch (ServletException e) &#123; log.error(sm.getString("standardContext.sciFail"), e); ok = false; break; &#125; &#125; // Configure and call application event listeners //配置listeners if (ok) &#123; if (!listenerStart()) &#123; log.error(sm.getString("standardContext.listenerFail")); ok = false; &#125; &#125; // Check constraints for uncovered HTTP methods // Needs to be after SCIs and listeners as they may programmatically // change constraints if (ok) &#123; checkConstraintsForUncoveredMethods(findConstraints()); &#125; try &#123; // Start manager Manager manager = getManager(); if (manager instanceof Lifecycle) &#123; ((Lifecycle) manager).start(); &#125; &#125; catch(Exception e) &#123; log.error(sm.getString("standardContext.managerFail"), e); ok = false; &#125; // Configure and call application filters //配置filters if (ok) &#123; if (!filterStart()) &#123; log.error(sm.getString("standardContext.filterFail")); ok = false; &#125; &#125; // Load and initialize all "load on startup" servlets //配置load-on-startup if (ok) &#123; if (!loadOnStartup(findChildren()))&#123; log.error(sm.getString("standardContext.servletFail")); ok = false; &#125; &#125; // Start ContainerBackgroundProcessor thread super.threadStart(); &#125; finally &#123; // Unbinding thread unbindThread(oldCCL); &#125; // Set available status depending upon startup success if (ok) &#123; if (log.isDebugEnabled()) log.debug("Starting completed"); &#125; else &#123; log.error(sm.getString("standardContext.startFailed", getName())); &#125; startTime=System.currentTimeMillis(); // Send j2ee.state.running notification if (ok &amp;&amp; (this.getObjectName() != null)) &#123; Notification notification = new Notification("j2ee.state.running", this.getObjectName(), sequenceNumber.getAndIncrement()); broadcaster.sendNotification(notification); &#125; // The WebResources implementation caches references to JAR files. On // some platforms these references may lock the JAR files. Since web // application start is likely to have read from lots of JARs, trigger // a clean-up now. getResources().gc(); // Reinitializing if something went wrong if (!ok) &#123; setState(LifecycleState.FAILED); &#125; else &#123; setState(LifecycleState.STARTING); &#125; &#125; listenerStart 、filterStart 和loadOnStartup 方法分别调用配置在Listener 的contextlnitialized 方法以及Filter 和配置了load-on-startup 的Servlet 的init 方法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224&lt;&lt;ContextConfig&gt;&gt;/** * Process a "contextConfig" event for this Context. */ protected synchronized void configureStart() &#123; // Called from StandardContext.start() if (log.isDebugEnabled()) &#123; log.debug(sm.getString("contextConfig.start")); &#125; if (log.isDebugEnabled()) &#123; log.debug(sm.getString("contextConfig.xmlSettings", context.getName(), Boolean.valueOf(context.getXmlValidation()), Boolean.valueOf(context.getXmlNamespaceAware()))); &#125; webConfig(); if (!context.getIgnoreAnnotations()) &#123; applicationAnnotationsConfig(); &#125; if (ok) &#123; validateSecurityRoles(); &#125; // Configure an authenticator if we need one if (ok) &#123; authenticatorConfig(); &#125; // Dump the contents of this pipeline if requested if (log.isDebugEnabled()) &#123; log.debug("Pipeline Configuration:"); Pipeline pipeline = context.getPipeline(); Valve valves[] = null; if (pipeline != null) &#123; valves = pipeline.getValves(); &#125; if (valves != null) &#123; for (int i = 0; i &lt; valves.length; i++) &#123; log.debug(" " + valves[i].getClass().getName()); &#125; &#125; log.debug("======================"); &#125; // Make our application available if no problems were encountered if (ok) &#123; context.setConfigured(true); &#125; else &#123; log.error(sm.getString("contextConfig.unavailable")); context.setConfigured(false); &#125; &#125; /** * Scan the web.xml files that apply to the web application and merge them * using the rules defined in the spec. For the global web.xml files, * where there is duplicate configuration, the most specific level wins. ie * an application's web.xml takes precedence over the host level or global * web.xml file. */ protected void webConfig() &#123; /* * Anything and everything can override the global and host defaults. * This is implemented in two parts * - Handle as a web fragment that gets added after everything else so * everything else takes priority * - Mark Servlets as overridable so SCI configuration can replace * configuration from the defaults */ /* * The rules for annotation scanning are not as clear-cut as one might * think. Tomcat implements the following process: * - As per SRV.1.6.2, Tomcat will scan for annotations regardless of * which Servlet spec version is declared in web.xml. The EG has * confirmed this is the expected behaviour. * - As per http://java.net/jira/browse/SERVLET_SPEC-36, if the main * web.xml is marked as metadata-complete, JARs are still processed * for SCIs. * - If metadata-complete=true and an absolute ordering is specified, * JARs excluded from the ordering are also excluded from the SCI * processing. * - If an SCI has a @HandlesType annotation then all classes (except * those in JARs excluded from an absolute ordering) need to be * scanned to check if they match. */ WebXmlParser webXmlParser = new WebXmlParser(context.getXmlNamespaceAware(), context.getXmlValidation(), context.getXmlBlockExternal()); Set&lt;WebXml&gt; defaults = new HashSet&lt;&gt;(); defaults.add(getDefaultWebXmlFragment(webXmlParser)); WebXml webXml = createWebXml(); // Parse context level web.xml InputSource contextWebXml = getContextWebXmlSource(); if (!webXmlParser.parseWebXml(contextWebXml, webXml, false)) &#123; ok = false; &#125; ServletContext sContext = context.getServletContext(); // Ordering is important here // Step 1. Identify all the JARs packaged with the application and those // provided by the container. If any of the application JARs have a // web-fragment.xml it will be parsed at this point. web-fragment.xml // files are ignored for container provided JARs. Map&lt;String,WebXml&gt; fragments = processJarsForWebFragments(webXml, webXmlParser); // Step 2. Order the fragments. Set&lt;WebXml&gt; orderedFragments = null; orderedFragments = WebXml.orderWebFragments(webXml, fragments, sContext); // Step 3. Look for ServletContainerInitializer implementations if (ok) &#123; processServletContainerInitializers(); &#125; if (!webXml.isMetadataComplete() || typeInitializerMap.size() &gt; 0) &#123; // Step 4. Process /WEB-INF/classes for annotations and // @HandlesTypes matches Map&lt;String,JavaClassCacheEntry&gt; javaClassCache = new HashMap&lt;&gt;(); if (ok) &#123; WebResource[] webResources = context.getResources().listResources("/WEB-INF/classes"); for (WebResource webResource : webResources) &#123; // Skip the META-INF directory from any JARs that have been // expanded in to WEB-INF/classes (sometimes IDEs do this). if ("META-INF".equals(webResource.getName())) &#123; continue; &#125; processAnnotationsWebResource(webResource, webXml, webXml.isMetadataComplete(), javaClassCache); &#125; &#125; // Step 5. Process JARs for annotations and // @HandlesTypes matches - only need to process those fragments we // are going to use (remember orderedFragments includes any // container fragments) if (ok) &#123; processAnnotations( orderedFragments, webXml.isMetadataComplete(), javaClassCache); &#125; // Cache, if used, is no longer required so clear it javaClassCache.clear(); &#125; if (!webXml.isMetadataComplete()) &#123; // Step 6. Merge web-fragment.xml files into the main web.xml // file. if (ok) &#123; ok = webXml.merge(orderedFragments); &#125; // Step 7. Apply global defaults // Have to merge defaults before JSP conversion since defaults // provide JSP servlet definition. webXml.merge(defaults); // Step 8. Convert explicitly mentioned jsps to servlets if (ok) &#123; convertJsps(webXml); &#125; // Step 9. Apply merged web.xml to Context if (ok) &#123; configureContext(webXml); &#125; &#125; else &#123; webXml.merge(defaults); convertJsps(webXml); configureContext(webXml); &#125; if (context.getLogEffectiveWebXml()) &#123; log.info("web.xml:\n" + webXml.toXml()); &#125; // Always need to look for static resources // Step 10. Look for static resources packaged in JARs if (ok) &#123; // Spec does not define an order. // Use ordered JARs followed by remaining JARs Set&lt;WebXml&gt; resourceJars = new LinkedHashSet&lt;&gt;(); for (WebXml fragment : orderedFragments) &#123; resourceJars.add(fragment); &#125; for (WebXml fragment : fragments.values()) &#123; if (!resourceJars.contains(fragment)) &#123; resourceJars.add(fragment); &#125; &#125; processResourceJARs(resourceJars); // See also StandardContext.resourcesStart() for // WEB-INF/classes/META-INF/resources configuration &#125; // Step 11. Apply the ServletContainerInitializer config to the // context if (ok) &#123; for (Map.Entry&lt;ServletContainerInitializer, Set&lt;Class&lt;?&gt;&gt;&gt; entry : initializerClassMap.entrySet()) &#123; if (entry.getValue().isEmpty()) &#123; context.addServletContainerInitializer( entry.getKey(), null); &#125; else &#123; context.addServletContainerInitializer( entry.getKey(), entry.getValue()); &#125; &#125; &#125; &#125; Context 和 Host 一样也有一个LifecycleListener 类型的监听器ContextConfig ， 其中configureStart 方法用来处理CONFTGURE_START_EVENT 事件，这个方法里面调用webConfig方法， webConfig 方法解析了web.xml文件，相应地创建了Wrapper 并使用addChild 添加到了Context 里面。 Wrapper12345678910111213141516171819202122232425262728&lt;&lt;Wrapper&gt;&gt;//没有重写initlntemal 方法@Override protected synchronized void startInternal() throws LifecycleException &#123; // Send j2ee.state.starting notification if (this.getObjectName() != null) &#123; Notification notification = new Notification("j2ee.state.starting", this.getObjectName(), sequenceNumber++); broadcaster.sendNotification(notification); &#125; // Start up this component super.startInternal(); setAvailable(0L); // Send j2ee.state.running notification if (this.getObjectName() != null) &#123; Notification notification = new Notification("j2ee.state.running", this.getObjectName(), sequenceNumber++); broadcaster.sendNotification(notification); &#125; &#125; 这里主要做了三件事情： 用broadcaster 发送通知，主要用于JMX; 调用了父类ContainerBase 中的startlntemal 方法； 调用setAvailable 方法让Servlet 有效。 这里的setAvailable 方法是Wrapper 接口中的方法，其作用是设置Wrapper 所包含的Servlet 有效的起始时间，如果所设置的时间为将来的时间，那么调用所对应的Servlet 就会产生错误，直到过了所设置的时间之后才可以正常调用，它的类型是long，如果设置为Long.MAX VALUE 就一直不可以调用了。Wrapper 没有别的容器那种XXXConfig 样式的LifecycleListener 监听器。]]></content>
  </entry>
  <entry>
    <title><![CDATA[tomcat源码2]]></title>
    <url>%2F2019%2F08%2F27%2FTomcat%E6%BA%90%E7%A0%812%2F</url>
    <content type="text"><![CDATA[Tomcat拾遗–BootStrap类的静态代码块和反射调用Catalina的意义是什么首先我们需要知道一个潜规则：即如果我们在A类中调用B类，如果B类没有被classloader加载或者就算加载了 但是该classloader和A类的classloader属于平行的，即我们在A的classloader中找不到B类的class，那么A会使用自己的classloader去加载B。 反射调用Catalina的意义因为Bootstrap这个类在Tomcat打包发布时是放在bin\bootstrap.jar中， 而Catalina类是放在lib\catalina.jar中,两个jar是用不同的ClassLoader加载的， 所以不能在Bootstrap类中直接引用Catalina类，只能通过反射。 这也意味着 后续我们在tomcat的Catalina类里面启动的类默认都是使用catalinaLoader（除了我们的context使用webappclassloader去加载的），进而tomcat使用的类只能被tomcat自己使用，而不会被其他应用使用 组件图 多个 Connector 和一个 Container 就形成了一个 Service，Service 的概念大家都很熟悉了，有了 Service 就可以对外提供服务了，但是 Service 还要一个生存的环境，必须要有人能够给她生命、掌握其生死大权，那就非 Server 莫属了。所以整个 Tomcat 的生命周期由 Server 控制。 connector 在tomcat中，connector负责接收来自客户端的连接，并交由后续的代码进行处理。connector对象持有ProtocolHandler对象；ProtocolHandler对象持有AbstractEndpoint对象。AbstractEndpoint负责创建服务器套接字，并绑定到监听端口；同时还创建accepter线程来接收客户端的连接以及poller线程来处理连接中的读写请求。其结构如上图所示。 1234567891011121314151617181920212223242526272829303132333435363738394041424344public Connector(String protocol) &#123; //设置协议 setProtocol(protocol); // Instantiate protocol handler ProtocolHandler p = null; try &#123; //反射生成ProtocolHandler实例 Class&lt;?&gt; clazz = Class.forName(protocolHandlerClassName); p = (ProtocolHandler) clazz.getConstructor().newInstance(); &#125; catch (Exception e) &#123; log.error(sm.getString( "coyoteConnector.protocolHandlerInstantiationFailed"), e); &#125; finally &#123; this.protocolHandler = p; &#125; if (Globals.STRICT_SERVLET_COMPLIANCE) &#123; uriCharset = StandardCharsets.ISO_8859_1; &#125; else &#123; uriCharset = StandardCharsets.UTF_8; &#125; &#125;public void setProtocol(String protocol) &#123; boolean aprConnector = AprLifecycleListener.isAprAvailable() &amp;&amp; AprLifecycleListener.getUseAprConnector(); if ("HTTP/1.1".equals(protocol) || protocol == null) &#123; if (aprConnector) &#123; setProtocolHandlerClassName("org.apache.coyote.http11.Http11AprProtocol"); &#125; else &#123; setProtocolHandlerClassName("org.apache.coyote.http11.Http11NioProtocol"); &#125; &#125; else if ("AJP/1.3".equals(protocol)) &#123; if (aprConnector) &#123; setProtocolHandlerClassName("org.apache.coyote.ajp.AjpAprProtocol"); &#125; else &#123; setProtocolHandlerClassName("org.apache.coyote.ajp.AjpNioProtocol"); &#125; &#125; else &#123; setProtocolHandlerClassName(protocol); &#125; &#125; Connector的构造函数带有协议属性，该协议属性是server.xml中Connector标签的protocol的属性值。Tomcat 8中默认值为HTTP/1.1，因此在Connector的构造函数中生成的是Http11NioProtocol对象。在setProtocol()方法中可以看到，tomcat8还包括其他几个协议处理器。协议处理器中带有Apr命名的都是使用Apr库来处理http请求的。通过使用APR库，Tomcat将使用JNI的方式来读取文件以及进行网络传输，可以大大提升Tomcat对静态文件的处理性能，同时如果你使用了HTTPS方式传输的话，也可以提升SSL的处理性能。AJP/1.3协议是Http服务器和应用服务器之间数据交互的协议，比如Apache服务器或IIS服务器与tomcat服务器之间进行数据交互。Http11NioProtocol是非阻塞模式的Http1.1协议处理器，使用java的nio包来实现非阻塞。可以看到，在tomcat 8中，默认使用的是非阻塞IO。 1234567public AbstractHttp11Protocol(AbstractEndpoint&lt;S&gt; endpoint) &#123; super(endpoint); setConnectionTimeout(Constants.DEFAULT_CONNECTION_TIMEOUT); ConnectionHandler&lt;S&gt; cHandler = new ConnectionHandler&lt;&gt;(this); setHandler(cHandler); getEndpoint().setHandler(cHandler); &#125; 在创建Http11NioProtocol实例的时候，会创建NioEndpoint、ConnectionHandler实例。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061//&lt;&lt;AbstractEndpoint&gt;&gt;public void init() throws Exception &#123; if (bindOnInit) &#123; bind();//调用子类bind方法初始化 bindState = BindState.BOUND_ON_INIT; &#125; if (this.domain != null) &#123; // Register endpoint (as ThreadPool - historical name) oname = new ObjectName(domain + ":type=ThreadPool,name=\"" + getName() + "\""); Registry.getRegistry(null, null).registerComponent(this, oname, null); for (SSLHostConfig sslHostConfig : findSslHostConfigs()) &#123; registerJmx(sslHostConfig); &#125; &#125; &#125;//&lt;&lt;NioEndpoint&gt;&gt;/** * Initialize the endpoint. */ @Override public void bind() throws Exception &#123; if (!getUseInheritedChannel()) &#123; // 打开serverSocketChannel serverSock = ServerSocketChannel.open(); // 设置socket属性 socketProperties.setProperties(serverSock.socket()); InetSocketAddress addr = (getAddress()!=null?new InetSocketAddress(getAddress(),getPort()):new InetSocketAddress(getPort())); // 绑定监听端口 serverSock.socket().bind(addr,getAcceptCount()); &#125; else &#123; // Retrieve the channel provided by the OS Channel ic = System.inheritedChannel(); if (ic instanceof ServerSocketChannel) &#123; serverSock = (ServerSocketChannel) ic; &#125; if (serverSock == null) &#123; throw new IllegalArgumentException(sm.getString("endpoint.init.bind.inherited")); &#125; &#125; // 设为阻塞模式 //这里为什么要设置成阻塞呢，Tomcat的设计初衷主要是为了操作方便。这样这里就跟BIO模式下一样了。只不过在BIO下这里返回的是Socket，NIO下这里返回的是SocketChannel。 serverSock.configureBlocking(true); //mimic APR behavior // Initialize thread count defaults for acceptor, poller if (acceptorThreadCount == 0) &#123; // FIXME: Doesn't seem to work that well with multiple accept threads acceptorThreadCount = 1; &#125; if (pollerThreadCount &lt;= 0) &#123; //minimum one poller thread pollerThreadCount = 1; &#125; setStopLatch(new CountDownLatch(pollerThreadCount)); // Initialize SSL if needed initialiseSsl(); // 打开阻塞模式的selector selectorPool.open(); &#125; 在bind()方法中，首先打开serverSocketChannel，并绑定到监听端口，此处将其该channel设置为阻塞模式。对于SSL部分，此处略过不讲。在最后的 selectorPool.open()执行语句中，会先获得共享的selector，并且创建线程在该selector上检测事件。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748//&lt;&lt;AbstractEndPoint&gt;&gt;public final void start() throws Exception &#123; if (bindState == BindState.UNBOUND) &#123; bind(); bindState = BindState.BOUND_ON_START; &#125; startInternal();//调用子类startInternal方法初始化启动 &#125;//&lt;&lt;NioEndpoint&gt;&gt;/** * Start the NIO endpoint, creating acceptor, poller threads. */ @Override public void startInternal() throws Exception &#123; if (!running) &#123; running = true; paused = false; processorCache = new SynchronizedStack&lt;&gt;(SynchronizedStack.DEFAULT_SIZE, socketProperties.getProcessorCache()); eventCache = new SynchronizedStack&lt;&gt;(SynchronizedStack.DEFAULT_SIZE, socketProperties.getEventCache()); nioChannels = new SynchronizedStack&lt;&gt;(SynchronizedStack.DEFAULT_SIZE, socketProperties.getBufferPool()); // Create worker collection if ( getExecutor() == null ) &#123; createExecutor(); &#125; // 初始化计数器Latch initializeConnectionLatch(); // 创建Poller线程 // Start poller threads pollers = new Poller[getPollerThreadCount()]; for (int i=0; i&lt;pollers.length; i++) &#123; pollers[i] = new Poller(); Thread pollerThread = new Thread(pollers[i], getName() + "-ClientPoller-"+i); pollerThread.setPriority(threadPriority); pollerThread.setDaemon(true); pollerThread.start(); &#125; // 创建Acceptor线程 startAcceptorThreads(); &#125; &#125; 在startInternal()方法中，最重要的是创建Poller和Acceptor线程。Acceptor线程处理serverSocketChannel的请求接收事件；Poller处理serverSocketChannel的读写事件。此时可以预想到，Acceptor线程专门负责接收客户端连接socketChannel，然后将socketChannel交给Poller线程读写。在实际中，Poller线程将socketChannel再次封装之后又开启另一个线程进行实际的数据处理。这样设计的目的是避免当某一个请求出现阻塞的时候，影响到整个服务器的接收、处理能力。 按接收请求，处理请求的逻辑，我们先观察Acceptor线程。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140// --------------------------------------------------- Acceptor Inner Class /** * The background thread that listens for incoming TCP/IP connections and * hands them off to an appropriate processor. */ protected class Acceptor extends AbstractEndpoint.Acceptor &#123; @Override public void run() &#123; int errorDelay = 0; // 一直循环直到接收停止命令 // Loop until we receive a shutdown command while (running) &#123; // Loop if endpoint is paused while (paused &amp;&amp; running) &#123; state = AcceptorState.PAUSED; try &#123; Thread.sleep(50); &#125; catch (InterruptedException e) &#123; // Ignore &#125; &#125; if (!running) &#123; break; &#125; state = AcceptorState.RUNNING; try &#123; //通过同步计数器来限制连接数目 //当连接数目超过上限时，则等待 //其中同步计算器是通过继承AQS实现的 //默认的最大连接数是10000 //if we have reached max connections, wait countUpOrAwaitConnection(); SocketChannel socket = null; try &#123; // Accept the next incoming connection from the server // socket //接收连接，此处并不是使用selector实现,在前面的代码中已知serverSock是阻塞模式的。 socket = serverSock.accept(); &#125; catch (IOException ioe) &#123; // We didn't get a socket countDownConnection(); if (running) &#123; // Introduce delay if necessary errorDelay = handleExceptionWithDelay(errorDelay); // re-throw throw ioe; &#125; else &#123; break; &#125; &#125; // Successful accept, reset the error delay errorDelay = 0; // Configure the socket if (running &amp;&amp; !paused) &#123; // setSocketOptions() will hand the socket off to // an appropriate processor if successful // 在setSocketOptions中将接收到的socket传给poller线程进行处理 if (!setSocketOptions(socket)) &#123; closeSocket(socket); &#125; &#125; else &#123; closeSocket(socket); &#125; &#125; catch (Throwable t) &#123; ExceptionUtils.handleThrowable(t); log.error(sm.getString("endpoint.accept.fail"), t); &#125; &#125; state = AcceptorState.ENDED; &#125; private void closeSocket(SocketChannel socket) &#123; countDownConnection(); try &#123; socket.socket().close(); &#125; catch (IOException ioe) &#123; if (log.isDebugEnabled()) &#123; log.debug(sm.getString("endpoint.err.close"), ioe); &#125; &#125; try &#123; socket.close(); &#125; catch (IOException ioe) &#123; if (log.isDebugEnabled()) &#123; log.debug(sm.getString("endpoint.err.close"), ioe); &#125; &#125; &#125; &#125;protected boolean setSocketOptions(SocketChannel socket) &#123; // Process the connection try &#123; //disable blocking, APR style, we are gonna be polling it // 设置为非阻塞模式 socket.configureBlocking(false); Socket sock = socket.socket(); socketProperties.setProperties(sock); // 从NioChannel容器中获得一个NioChannel // NioChannel可以理解为socketChannel的代理类，提供更多的功能 NioChannel channel = nioChannels.pop(); if (channel == null) &#123; // SocketBufferHandler维护了在处理过程中的读写缓存 SocketBufferHandler bufhandler = new SocketBufferHandler( socketProperties.getAppReadBufSize(), socketProperties.getAppWriteBufSize(), socketProperties.getDirectBuffer()); if (isSSLEnabled()) &#123; channel = new SecureNioChannel(socket, bufhandler, selectorPool, this); &#125; else &#123; // 将socket、bufHandler封装到NioChannel中 channel = new NioChannel(socket, bufhandler); &#125; &#125; else &#123; channel.setIOChannel(socket); channel.reset(); &#125; // 将niochannel注册到poller线程中进行处理 getPoller0().register(channel); &#125; catch (Throwable t) &#123; ExceptionUtils.handleThrowable(t); try &#123; log.error("",t); &#125; catch (Throwable tt) &#123; ExceptionUtils.handleThrowable(tt); &#125; // Tell to close the socket return false; &#125; return true; &#125; poller和pollerEvent待补充 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198public void run() &#123; // Loop until destroy() is called // 循环直到destroy()方法被调用 while (true) &#123; boolean hasEvents = false; try &#123; if (!close) &#123; hasEvents = events(); // wakeupCounter &gt; 0，表示有事件，故直接用selectNow，否则用 select(selectorTimeout)以阻塞一段时间等待事件到来 if (wakeupCounter.getAndSet(-1) &gt; 0) &#123; //if we are here, means we have other stuff to do //do a non blocking select keyCount = selector.selectNow(); &#125; else &#123; keyCount = selector.select(selectorTimeout); &#125; wakeupCounter.set(0); &#125; // 关闭 if (close) &#123; events(); timeout(0, false); try &#123; selector.close(); &#125; catch (IOException ioe) &#123; log.error(sm.getString("endpoint.nio.selectorCloseFail"), ioe); &#125; break; &#125; &#125; catch (Throwable x) &#123; ExceptionUtils.handleThrowable(x); log.error("",x); continue; &#125; //either we timed out or we woke up, process events first // 执行队列中的PollerEvent事件，注册读或写， // hasEvents表示是否有读写事件注册 if ( keyCount == 0 ) hasEvents = (hasEvents | events()); Iterator&lt;SelectionKey&gt; iterator = keyCount &gt; 0 ? selector.selectedKeys().iterator() : null; // Walk through the collection of ready keys and dispatch // any active event. while (iterator != null &amp;&amp; iterator.hasNext()) &#123; SelectionKey sk = iterator.next(); NioSocketWrapper attachment = (NioSocketWrapper)sk.attachment(); // Attachment may be null if another thread has called // cancelledKey() if (attachment == null) &#123; iterator.remove(); &#125; else &#123; iterator.remove(); // 将sk和attachtment包装，交由后续线程继续处理 processKey(sk, attachment); &#125; &#125;//while //process timeouts timeout(keyCount,hasEvents); &#125;//while getStopLatch().countDown(); &#125;//在events()方法中，通过调用PollerEvent的run()方法将socket注册到selector中public boolean events() &#123; boolean result = false; PollerEvent pe = null; // 从队列中获得PollerEvent事件 for (int i = 0, size = events.size(); i &lt; size &amp;&amp; (pe = events.poll()) != null; i++ ) &#123; result = true; try &#123; // 调用PollerEvent的run()方法执行事件注册 pe.run(); pe.reset(); if (running &amp;&amp; !paused) &#123; eventCache.push(pe); &#125; &#125; catch ( Throwable x ) &#123; log.error("",x); &#125; &#125; return result; &#125;public void run() &#123; // 如果是注册，则把socket注册到selector中 if (interestOps == OP_REGISTER) &#123; try &#123; socket.getIOChannel().register( socket.getPoller().getSelector(), SelectionKey.OP_READ, socketWrapper); &#125; catch (Exception x) &#123; log.error(sm.getString("endpoint.nio.registerFail"), x); &#125; &#125; else &#123; final SelectionKey key = socket.getIOChannel().keyFor(socket.getPoller().getSelector()); try &#123; if (key == null) &#123; // The key was cancelled (e.g. due to socket closure) // and removed from the selector while it was being // processed. Count down the connections at this point // since it won't have been counted down when the socket // closed. socket.socketWrapper.getEndpoint().countDownConnection(); ((NioSocketWrapper) socket.socketWrapper).closed = true; &#125; else &#123; final NioSocketWrapper socketWrapper = (NioSocketWrapper) key.attachment(); if (socketWrapper != null) &#123; //we are registering the key to start with, reset the fairness counter. int ops = key.interestOps() | interestOps; socketWrapper.interestOps(ops); key.interestOps(ops); &#125; else &#123; socket.getPoller().cancelledKey(key); &#125; &#125; &#125; catch (CancelledKeyException ckx) &#123; try &#123; socket.getPoller().cancelledKey(key); &#125; catch (Exception ignore) &#123;&#125; &#125; &#125; &#125;//processKey()方法处理准备完毕的事件protected void processKey(SelectionKey sk, NioSocketWrapper attachment) &#123; try &#123; // 如果close，则取消sk if ( close ) &#123; cancelledKey(sk); &#125; else if ( sk.isValid() &amp;&amp; attachment != null ) &#123; if (sk.isReadable() || sk.isWritable() ) &#123; if ( attachment.getSendfileData() != null ) &#123; // 处理文件 processSendfile(sk,attachment, false); &#125; else &#123; unreg(sk, attachment, sk.readyOps()); boolean closeSocket = false; // Read goes before write if (sk.isReadable()) &#123;// 处理可读 if (!processSocket(attachment, SocketEvent.OPEN_READ, true)) &#123; closeSocket = true; &#125; &#125; if (!closeSocket &amp;&amp; sk.isWritable()) &#123; //处理可写 if (!processSocket(attachment, SocketEvent.OPEN_WRITE, true)) &#123; closeSocket = true; &#125; &#125; if (closeSocket) &#123; cancelledKey(sk); &#125; &#125; &#125; &#125; else &#123; //invalid key cancelledKey(sk); &#125; &#125; catch ( CancelledKeyException ckx ) &#123; cancelledKey(sk); &#125; catch (Throwable t) &#123; ExceptionUtils.handleThrowable(t); log.error("",t); &#125; &#125;// 将selectionKey包装为SocketProcessor public boolean processSocket(SocketWrapperBase&lt;S&gt; socketWrapper, SocketEvent event, boolean dispatch) &#123; try &#123; if (socketWrapper == null) &#123; return false; &#125; SocketProcessorBase&lt;S&gt; sc = processorCache.pop(); if (sc == null) &#123; sc = createSocketProcessor(socketWrapper, event); &#125; else &#123; sc.reset(socketWrapper, event); &#125; Executor executor = getExecutor(); if (dispatch &amp;&amp; executor != null) &#123; // 交给线程池处理或直接运行 executor.execute(sc); &#125; else &#123; sc.run(); &#125; &#125; catch (RejectedExecutionException ree) &#123; getLog().warn(sm.getString("endpoint.executor.fail", socketWrapper) , ree); return false; &#125; catch (Throwable t) &#123; ExceptionUtils.handleThrowable(t); // This means we got an OOM or similar creating a thread, or that // the pool and its queue are full getLog().error(sm.getString("endpoint.process.fail"), t); return false; &#125; return true; &#125; LifeCycle接口]]></content>
      <tags>
        <tag>tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[solr配置总结]]></title>
    <url>%2F2019%2F03%2F24%2Fsolr%E9%85%8D%E7%BD%AE%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[环境solr版本：4.10.4 tomcat7 jdk8 solr文件内容介绍 bin：solr的运行脚本 contrib：solr的一些扩展jar包，用于增强solr的功能。 dist：该目录包含build过程中产生的war和jar文件，以及相关的依赖文件。 docs：solr的API文档 example：solr工程的例子目录： l example/solr： ​ 该目录是一个标准的SolrHome，它包含一个默认的SolrCore l example/multicore： ​ 该目录包含了在Solr的multicore中设置的多个Core目录。 l example/webapps： ​ 该目录中包括一个solr.war，该war可作为solr的运行实例工程。 licenses：solr相关的一些许可信息 SolrCore配置 SolrHome是Solr服务运行的主目录，该目录中包括了多个SolrCore目录。SolrCore目录中包含了运行Solr实例所有的配置文件和数据文件，Solr实例就是SolrCore。 每个SolrCore提供单独的搜索和索引服务。 创建SolrCore]]></content>
  </entry>
  <entry>
    <title><![CDATA[vue学习]]></title>
    <url>%2F2019%2F03%2F18%2Fvue%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[mvvm Model 模型，数据对象(data) view 视图模板页面 viewModel 视图模型(vue的实例) 表达式和指令 “Mustache”语法 (双大括号) 的文本插值 1&lt;span&gt;Message: &#123;&#123; msg &#125;&#125;&lt;/span&gt; Mustache 语法不能作用在 HTML 特性上，遇到这种情况应该使用 v-bind 指令： 1234&lt;div v-bind:id="dynamicId"&gt;&lt;/div&gt;&lt;div :id="dynamicId"&gt;&lt;/div&gt; &lt;!--简写形式--&gt;&lt;!--在布尔特性的情况下，它们的存在即暗示为true--&gt;&lt;button v-bind:disabled="isButtonDisabled"&gt;Button&lt;/button&gt; 使用js表达式 1234&#123;&#123; number + 1 &#125;&#125;&#123;&#123; ok ? 'YES' : 'NO' &#125;&#125;&#123;&#123; message.split('').reverse().join('') &#125;&#125;&lt;div v-bind:id="'list-' + id"&gt;&lt;/div&gt; 事件绑定 1234567&lt;a v-on:click="doSomething"&gt;...&lt;/a&gt;&lt;!-- 修饰符 .prevent 修饰符告诉 v-on 指令对于触发的事件调用 event.preventDefault()--&gt;&lt;form v-on:submit.prevent="onSubmit"&gt;...&lt;/form&gt;&lt;!-- 缩写 --&gt;&lt;a @click="doSomething"&gt;...&lt;/a&gt;&lt;!--传参 --&gt;&lt;a @click="doSomething('abc')"&gt;...&lt;/a&gt; 计算属性和监视 计算属性 12345678910111213141516171819202122232425262728293031var vm = new Vue(&#123; el: '#example', data: &#123; firstName: 'Hello', lastName:'123' &#125;, computed: &#123; // 计算属性的 getter 方法的返回值作为属性值 //什么时候执行：初始化显示、相关data属性发生改变 fullName() &#123; // `this` 指向 vm 实例 return this.firstName +' '+ lastName &#125; &#125;&#125;)//计算属性的 settercomputed: &#123; fullName: &#123; // getter 读取当前属性值时根据相关的数据回调 //计算属性存在缓存 get: function () &#123; return this.firstName + ' ' + this.lastName &#125;, // setter 当属性值发送改变时回调 set: function (newValue) &#123; var names = newValue.split(' ') this.firstName = names[0] this.lastName = names[names.length - 1] &#125; &#125;&#125; 监视 12345678910111213141516var vm = new Vue(&#123; el: '#demo', data: &#123; firstName: 'Foo', lastName: 'Bar', fullName: 'Foo Bar' &#125;, watch: &#123; firstName: function (val) &#123; this.fullName = val + ' ' + this.lastName &#125;, lastName: function (val) &#123; this.fullName = this.firstName + ' ' + val &#125; &#125;&#125;) class与style绑定 class绑定 class=’xxx’ xxx 字符串 xxx 对象 （用的比较多） 1&lt;div v-bind:class="&#123; active: isActive &#125;"&gt;&lt;/div&gt; xxx 数组 style绑定 1&lt;div v-bind:style="&#123; color: activeColor, fontSize: fontSize + 'px' &#125;"&gt;&lt;/div&gt; 1234data: &#123; activeColor: 'red', fontSize: 30&#125; 条件渲染]]></content>
  </entry>
  <entry>
    <title><![CDATA[nginx笔记]]></title>
    <url>%2F2019%2F03%2F06%2Fnginx%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[安装123brew install nginx#linux安装yum install nginx 基本参数使用安装路径123rpm -ql nginx #查看安装路径nginx -t -c nginx.conf #检查配置文件nginx -s reload -c nginx.conf #使配置文件生效 1234567891011121314151617181920212223242526272829303132333435/etc/logrotate.d/nginx nginx日志轮转，用于logrotate服务的日志切割/etc/nginx/etc/nginx/nginx.conf/etc/nginx/conf.d/etc/nginx/conf.d/default.conf nginx 配置文件/etc/nginx/fastcgi_params/etc/nginx/scgi_params/etc/nginx/uwsgi_params cgi配置/etc/nginx/koi-utf/etc/nginx/koi-win/etc/nginx/win-utf 编码转换映射转化文件/etc/nginx/mime.types 设置http协议的Content-Type 与扩展名对应关系/etc/sysconfig/nginx/etc/sysconfig/nginx-debug/usr/lib/systemd/system/nginx-debug.service/usr/lib/systemd/system/nginx.service 配置系统守护进程管理器管理方式/etc/nginx/modules/usr/lib64/nginx/modules 模块目录/usr/sbin/nginx/usr/sbin/nginx-debug 启动管理终端命令/usr/share/doc/nginx-1.16.1/usr/share/doc/nginx-1.16.1/COPYRIGHT/usr/share/man/man8/nginx.8.gz 手册和帮助文件/var/cache/nginx 开启缓存后的缓存目录/var/log/nginx 日志目录 编译参数12345678910111213141516171819202122nginx -V--prefix=/etc/nginx --sbin-path=/usr/sbin/nginx --modules-path=/usr/lib64/nginx/modules --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --pid-path=/var/run/nginx.pid --lock-path=/var/run/nginx.lock 安装路径--http-client-body-temp-path=/var/cache/nginx/client_temp --http-proxy-temp-path=/var/cache/nginx/proxy_temp --http-fastcgi-temp-path=/var/cache/nginx/fastcgi_temp --http-uwsgi-temp-path=/var/cache/nginx/uwsgi_temp --http-scgi-temp-path=/var/cache/nginx/scgi_temp 执行对应模块时，nginx所保留的临时性文件--user=nginx --group=nginx 设定nginx进程启动的用户和用户组--with-cc-opt gcc编译的额外参数--with-ld-opt 设置附加参数，链接系统库 配置默认配置1234567891011121314151617181920212223242526272829303132333435363738394041424344#设置nginx服务的系统使用用户#如果希望所有用户都可以启动Nginx进程，有两种办法：一是将此指令行注释掉;一种是设置为nobody#user nobody nobodyuser nginx; #工作进程数,从理论上来说，worker process的值越大，可以支持的并发处理量也越多# worker_processes number | auto(自动检测)worker_processes 1; #错误日志#error_log file | stderr [debug | info | notice ｜warn｜ error|crit｜ alert | emerg];error_log /var/log/nginx/error.log warn; #服务启动时pidpid /var/run/nginx.pid; events &#123; #在《UNIX网络编程》第1卷里提到过一个叫“惊群”的问题（Thundering herd problem），大致意思是，当某一时刻只有一个网络连接到来时，多个睡眠进程会被同时叫醒，但只有一个进程可获得连接。如果每次唤醒的进程数目太多，会影响一部分系统性能。在Nginx服务器的多进程下，就有可能出现这样的问题。为了解决这样的问题，Nginx配置中包含了这样一条指令accept_mutex，当其设置为开启的时候，将会对多个Nginx进程接收连接进行序列化，防止多个进程对连接的争抢 accept mutex on | off; #每个Nginx服务器的worker process都有能力同时接收多个新到达的网络连接，但是这需要在配置文件中进行设置 multi_accept on | off; #Nginx服务器提供了多种事件驱动模型来处理网络消息其中，method可选择的内容有：select、poll、kqueue、epoll、rtsig、/dev/poll以及eventport use method; #每个进程允许的最大连接数 worker_connections 1024; &#125;#http模块location /img/ &#123; alias /var/www/image/;&#125;#若按照上述配置的话，则访问/img/目录里面的文件时，ningx会自动去/var/www/image/目录找文件location /img/ &#123; root /var/www/image;&#125;#若按照这种配置的话，则访问/img/目录下的文件时，nginx会去/var/www/image/img/目录下找文件。alias是一个目录别名的定义，root则是最上层目录的定义。一直以为root是指的/var/www/image目录下，应该 是 /var/www/image/img/ 还有一个重要的区别是alias后面必须要用“/”结束，否则会找不到文件的。。。而root则可有可无可以是绝对目录，也可以是相对目录(相对于nginx的安装目录)uri解析= 精确匹配,如果已经匹配成功，就停止继续向下搜索并立即处理此请求^～ 匹配以某个uri开头的请求，不支持正则表达式#如果uri包含正则表达式，就必须要使用“～”或者“～*”标识。～ 用于表示uri包含正则表达式，并且区分大小写。～* 用于表示uri包含正则表达式，并且不区分大小写。 nginx变量1234561.http请求变量 arg_PARAMETER #请求参数 http_HEADER #请求头 sent_http_HEADER #响应头2.内置变量3.自定义变量 nginx模块nginx官方模块 http_stub_status_module nginx的客户端状态 http_random_index_module 目录中随机选择一个主页 http_sub_module http内容替换 limit_conn_module 连接频率限制 limit_req_module 请求频率限制 http_access_module 基于ip的访问控制 局限性：如果中间有代理的话，使用$remote_addr 就会不准确 方法： 采用别的http头信息控制，如HTTP_X_FORWARD_FOR 结合geo模块 通过http自定义变量传递 http_auth_basic_module 基于用户的信任登录 局限： 用户信息依赖文件方式 操作管理机械，效率低下 解决方案： 结合lua Nginx和ldap打通，利用nginx-auth-ldap模块 nginx第三方模块nginx静态服务器文件读取：sendfile tcp_nopush sendfile 开启的情况下，提高网络包的传输效率 tcp_nodelay keepalive下，提高网络包的传输实时性 gzip 压缩 gzip_comp_level 压缩比 http_gzip_static_module 扩展压缩模块，预读gzip功能 跨域访问：Access-Control-Allow- 防盗链：http_referer 1234valid_referers none blocked 116.62.103.228if($valid_referers)&#123; return 403&#125; 代理服务代理区别：代理的对象不一样 正向代理代理的对象是客户端 1234resolver 8.8.8.8location / &#123; proxy_pass http://$http_host$request_uri&#125; 反向代理代理的对象是服务端 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748proxy_pass http(https)://hostname:port/app#在使用该指令的过程中还要注意，URL中是否包含有URI，Nginx服务器的处理方式是不同的。如果URL中不包含URI，Nginx服务器不会改变原地址的URI；但是如果包含了URI，Nginx服务器将会使用新的URI替代原来的URI。我们举例来说明。例子如下server&#123; listen 80; server name www.myweb. name; location /server/ &#123; proxy_pass http://192.168.1.1; &#125;# 如果客户端使用“http://www.myweb.name/server”发起请求，该请求被配置中显示的location块进行处理，由于proxy_pass指令的URL变量不含有URI，所以转向的地址为“http://192.168.1.1/server”server&#123; listen 80; server name www.myweb. name; location /server/ &#123; proxy_pass http://192.168.1.1/loc/; &#125;#在该配置实例中，proxy_pass指令的URL包含了URI“/loc/”。如果客户端仍然使用“http://www. myweb.name/server/”发起请求，Nginx服务器将会把地址转向“http://192.168.1.1/loc/”#通过上面的实例，我们可以总结出，在使用proxy_pass指令时，如果不想改变原地址中的URI，就不要在URL变量中配置URI#明白了上面这两个例子的用法，我们来解释大家经常讨论的一个问题，就是proxy_pass指令的URL变量末尾是否加斜杠“/”的问题。#配置1: proxy_pass http://192.168.1.1;#配置2: proxy_pass http://192.168.1.1/;#配置1和配置2的区别在于，配置2中proxy_pass指令的URL变量末尾添加了斜杠“/”，这意味着配置2中proxy_pass指令的URL变量包含了URI “/”，而配置1中proxy_pass指令的URL变量不包含URI。理解了这一点，我们就可以解释下面的实例和现象了。大家注意各例子之间的对比。server&#123; listen 80; server name www.myweb. name; location / #注意location的uri变量 &#123; #配置1: proxy_pass http://192.168.1.1; #配置2: proxy_pass http://192.168.1.1/; &#125;#在该配置中，location块使用“/”作为uri变量的值来匹配不包含URI的请求URL。由于请求URL中不包含URI，因此配置1和配置2的效果是一样的。比如，客户端的请求URL为“http://www.myweb.name/index.htm”，其将会被实例1中的location块匹配成功并进行处理。不管使用配置1还是配置2，转向的URL都为：“http://192.168.1.1/index.htm”server&#123; listen 80; server name www.myweb. name; location /server/ #注意location的uri变量 &#123; #配置1: proxy_pass http://192.168.1.1; #配置2: proxy_pass http://192.168.1.1/; &#125; #在该配置中，location块使用“/server/”作为uri变量的值来匹配包含URI “/server/”的请求URL。这时，使用配置1和配置2的转向结果就不相同了。使用配置1的时候，proxy_pass指令中的URL变量不包含URI，Nginx服务器将不改变原地址的URI；使用配置2的时候，proxy_pass指令中的URL变量包含URI “/”，Nginx服务器会将原地址的URI替换为“/”。 #比如，客户端的请求URL为“http://www.myweb.name/server/index.htm”，将会被实例2中的location块匹配成功并进行处理。使用配置1的时候，转向的URL为“http://192.168.1.1/server/index.htm”，原地址的URI “/server/”未被改变；但使用配置2时，转向的URL为“http://192.168.1.1/index.htm”，可以看到，原地址的URI “/server/”被替换为“/”。 补充： 123456proxy_buffering #缓冲区proxy_redirect #跳转重定向proxy_set_header field value #设置请求头proxy_connect_timeout time #连接超时时间proxy_read_timeout #读超时proxy_send_timeout #发送给客户端超时 负载均衡123456789101112131415161718upstream name&#123; server http://ip:port/app server状态&#125;#server 状态down #当前的server暂时不参与负载均衡backup #预留的备份服务器max_fails #允许请求失败的次数fail_timeout #经过max_fails 失败后，服务暂停的时间max_conns #限制最大接收的连接数#调度算法轮询：按时间顺序逐一分配到不同的后端服务器加权轮询：weight值越大，分配到的访问几率越高ip_hash： 每个请求按访问ip的hash结果分配，这样来自同一个ip的固定访问一个后端服务器url_hash: 按照访问的url的hash结果来分配请求，每个url定向到同一个后端服务器 hash $request_urileast_conn：最少连接数，那个机器连接数少就分发hash关键数值: hash自定义的key 缓存服务123456proxy_cache_path #缓存路径levels=1:2keys_zone=缓存名称:缓存大小max_size=10g #缓存最大空间inactive=60m #如果60分钟未使用，则会淘汰use_temp_path=off #临时目录(建议关闭) 动静分离 rewrite 123456789rewrite regex replacement [flag]flag: last #停止rewirte监测 (进入location后会重新请求一次) break #停止rewirte监测 （进入location后不会出来） redirect #返回302临时重定向，地址栏会显示跳转后的地址 permanent #返回301永久重定向，地址栏会显示跳转后的地址 (只发送一次请求，浏览器会记住重定向的地址，以后的请求不会再请求后端服务) 优先级： server&gt;location]]></content>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络知识笔记]]></title>
    <url>%2F2019%2F03%2F02%2F%E7%BD%91%E7%BB%9C%E7%9F%A5%E8%AF%86%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[网络层的四个协议 ARP IP 0-126 A类地址 128-191 B类地址 192-223 C类地址 127.0.0.1 本地回环地址 保留的私网地址 10.0.0.0 172.16.0.0–172.31.0.0 192.168.0.0–192.168.255.0 ICMP IGMP]]></content>
      <tags>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[类加载器-classLoader]]></title>
    <url>%2F2019%2F02%2F26%2F%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%99%A8-classLoader%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[docker配置]]></title>
    <url>%2F2019%2F02%2F25%2Fdocker%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[docker开机启动1systemctl enable docker.service docker-compose开机启动容器123vim /etc/rc.d/rc.local/usr/local/bin/docker-compose -f /www/docker/trace_fecshop/docker-compose.yml up -d#/www/docker/trace_fecshop 是你的docker-compose的目录 docker-compose-volumes说明1234567891011121314151617181920212223#docker-compose里两种设置方式都是可以持久化的#第一种情况路径直接挂载到本地，比较直观，但需要管理本地的路径，而第二种使用卷标的方式，比较简洁，但你不知道#数据存在本地什么位置，下面说明如何查看docker的卷标#1.ghost: image: ghost volumes: - ./ghost/config.js:/var/lib/ghost/config.js #yml文件所在路径 #2.卷标services: mysql: image: mysql container_name: mysql volumes: - mysql:/var/lib/mysql...volumes: mysql: #查看所有卷标docker volume ls #查看具体的volume对应的真实地址docker volume inspect vagrant_mysql]]></content>
  </entry>
  <entry>
    <title><![CDATA[centos7无法连接wifi]]></title>
    <url>%2F2019%2F02%2F25%2Fcentos7%E6%97%A0%E6%B3%95%E8%BF%9E%E6%8E%A5wifi%2F</url>
    <content type="text"><![CDATA[今天在联想笔记本安装centos7,安装完成后无法连接wifi，最后发现是联想电脑的问题，以下为解决方法： 解决方法：把影响无线wifi开关的ideapad_laptop加入黑名单12345sudo vi /etc/modprobe.d/ideapad.conf文件内容：blacklist ideapad_laptop#保存并关闭后再执行sudo modprobe -r ideapad_laptop#重启之后，wifi就可以使用了。 1234567891011121314151617# 查询内核日志，查看是否需要安装无线网卡的固件dmesg | grep firmware# 正常：iwlwifi loaded firmware version ....# 错误：IOCSIFFLAGS: No such file or directory，此时需要安装固件# 错误：firmware: requesting iwlwifi-5000-1.ucode# 安装firmware，需要查看网卡型号，先安装工具yum -y install pciutils*# 查看无线网卡型号lspci# Ethernet controller: Realtek Semiconductor Co., Ltd. .....有线网卡# Intel Corporation Dual Band Wireless-AC 3165.......无线网卡# 查找并安装yum list | grep "3165"yum -y install iwl3945-firmware 12345678910111213141516171819202122232425262728# 安装配置工具，安装net-tools后，可以使用ifconfigyum install iwyum install wpa_supplicantyum install net-tools# 查看无线网接口iw dev# interface wlp3s0 ... addr ... type...# 有channel 1 (2412 MHz)....表示已连接# 查看接口连接信息iw wlp3s0 link# Not connectted. 未连接# Connected to ... SSID:test... 已连接# 查看网络接口/网卡状态ifconfig# 注：未连接wifi前，/etc/sysconfig/network-scripts没有发现wlp3s0的配置，# 连接成功之后，出现同wifi的SSID相同名称的配置# 查看网络接口/网卡状态ip addr # 会显示已获取的IPip link # 显示网卡# 启用/禁用wlp3s0接口，两种方法等同。up时需要数秒ifconfig wlp3s0 up/down # ping提示：connect: Network is unreachableip link set dev wlp3s0 up/down # ping提示：Name or service not known 123rfkill list #查看是否无线卡被锁住sudo modprobe -r ideapad-laptopsudo rfkill unblock all nmcli使用1234567nmcli device wifi //扫描//连接 WIFI 网络nmcli dev wifi con "wifi name" password "wifi password"// 查看所有连接// nmcli con show // 查看所有连接// nmcli con show -a // 活动的连接 --active// nmcli con show tun0 // 详细信息]]></content>
  </entry>
  <entry>
    <title><![CDATA[node.js]]></title>
    <url>%2F2019%2F02%2F13%2Fnode-js%2F</url>
    <content type="text"><![CDATA[node知识点 console __dirname 当前文件所在的目录。 __filename 文件的全路径。 eventLoop 宏任务、微任务 123456789101112131415161718//宏任务 主体script setTimeout setInterval (new Promise)//微任务 Promise.then process.nextTick()setTimeout(()=&gt;&#123; //宏任务 new Promise(resolve =&gt;&#123; console.log("promise") resolve() &#125;).then(()=&gt;&#123; //微任务 console.log("then") &#125;) //宏任务 console.log(1) //宏任务 setTimeout(()=&gt;&#123; console.log(222) &#125;)&#125;) 模块化（commonjs规范） 一个文件就是一个独立的模块。每个模块都有自己的独立作用域—模块作用域 模块加载采用同步模式。 通过require导入,exports导出。module.exports == exports 返回true,但是使用上有一定的注意事项： 每一个模块中都有一个内置的对象：module,该对象包括当前模块文件的一些信息 id 当前模块的唯一标识，默认id为当前文件的绝对路径 filename 当前模块的文件 parent children loaded paths 模块类型 文件 文件夹 当我们导入的模块是一个文件夹的时候，就会引入文件夹模块导入机制：1.读取该文件夹下的package.json文件。2.导入文件中main选项指定的文件。3.如果不存在，则导入文件夹下的index.js。 node_modules文件夹：如果我们的模块在node_modules目录下(一般用于第三方库) 123456789101112//let m2 = require('./node_modules/m2')let m2 = require('m2')//和上面的效果一样//如果模块的加载是以./ ../ /开头的，就是路径模块加载模式//不以./ ../ /开头的，按照另外一种加载机制，非路径加载模式，按照以下规则查找： //在module对象有一个属性paths,里面保存的是非路径模式下的查找列表。paths: [ '/Users/haominglfs/Documents/es6/node_modules', '/Users/haominglfs/Documents/node_modules', '/Users/haominglfs/node_modules', '/Users/node_modules', '/node_modules' ] global 文件夹:所有项目都可以使用的模块,在node安装目录下node_modules文件夹。 核心模块：node内置模块: 12let fs = require('fs')//如果自己定义的模块和核心模块冲突，则默认加载核心模块。 模块文件后缀处理机制： .js&gt;.json&gt;.node es6模块化 123456789//导出export var a = 10;//每个模块只能存在一个defaultexport default 100//导入import &#123;a&#125; from './m1'import * as m1 from './m1'//导入默认,直接赋值import a from './m1' 内置对象 events 1234567891011121314151617181920212223242526const eventEmitter = require('events')class Person extends eventEmitter&#123; constructor(name)&#123; super() this.name = name; this.age = 0; this.growup(); &#125; growup()&#123; setInterval(() =&gt; &#123; this.age++; this.emit('growup') //触发事件 &#125;, 1000); &#125;&#125;const p1 = new Person('郝明');p1.addListener('growup',function()&#123; //注册监听器 console.log('growup 1')&#125;)console.log(p1.eventNames()) process(全局对象，不需要引入) 123456789101112//process.argv 属性返回一个数组，其中包含当启动 Node.js 进程时传入的命令行参数。console.log(process.argv)//环境变量console.log(process.env)//标准输入输出process.stdout.write('hello')process.stdin.on('data',(e)=&gt;&#123; console.log('用户输入'+e.toString())&#125;) stream 四种基本类型 Writable 可写入数据的流。 fs.createWriteStream() Readable 可读取数据的流。 fs.createReadStream() Duplex 可读又可写的流。 Net.Socket() Transform 在读写过程中可以修改后转换 buffer filesystem 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566const fs = require('fs')//First Error :node 中的一种约定，如果一个回调可能有错误发生，那么约定回调函数的第一个参数用来做//错误对象// fs.writeFile('./1.txt','你好我好',(err)=&gt;&#123;// if(err) throw err// console.log('文件已保存')// &#125;)// let res = fs.writeFileSync('./2.txt','你好','utf8')// console.log(res)//返回bufferfs.readFile('./1.txt',(err,data)=&gt;&#123; if(err) throw err console.log(data.toString())&#125;)let content = fs.readFileSync('./2.txt')console.log(content)try&#123; let st = fs.statSync('./1.txt') console.log(st) console.log(st.isFile()) console.log(st.isDirectory())&#125;catch(e)&#123; console.log(e)&#125;//删除//fs.unlinkSync('./1.txt')//文件夹//fs.mkdirSync('./a')//不能递归创建文件夹//fs.mkdirSync('./a/b/c')//不能删除非空文件夹fs.rmdirSync('./a');let dir = './a';// let files = fs.readdirSync(dir);// files.forEach(child=&gt;&#123;// fs.unlinkSync(dir+'/'+child);// &#125;)rmdir(dir)//递归删除文件夹function rmdir(dirpath)&#123; let files = fs.readdirSync(dirpath); files.forEach(child=&gt;&#123; let childPath = dirpath+'/'+child; let st = fs.statSync(childPath) if(st.isDirectory())&#123; rmdir(childPath) &#125;else&#123; fs.unlinkSync(childPath); &#125; &#125;) fs.rmdirSync(dirpath)&#125;//监听文件或目录const fs = require('fs')fs.watchFile('./2.txt',(e)=&gt;&#123; console.log(e)&#125;) npm 能不安装全局就不安装全局。 命令行工具(第三方模块)： commander chalk Inquirer ​]]></content>
  </entry>
  <entry>
    <title><![CDATA[es6]]></title>
    <url>%2F2019%2F02%2F07%2Fes6%2F</url>
    <content type="text"><![CDATA[数据类型 Number、Boolean、String、Symbol、Null、Undefined、Object Symbol:es6新增数据类型-基本类型，值是由Symbol函数调用产生的 作用域 let、const 解构赋值 数组解构 let [a,b,c] = [1,2,3] 对象解构 let {foo,bar} = {foo:”aaa”,bar:”bbb”};//左侧变量的名称解构顺序无关 let {foo:f,bar:b} = {foo:”aaa”,bar:”bbb”};//取别名 let {foo:[a,b]} = {foo:[10,20],bar:”bbb”}//多重解构 扩展运算符 … 把数组、对象转成参数序列。 var arr = [1,7,3,6,5]; var arr2 = [‘a’,’b’,’c’]; Math.max(…arr); var arr3 = […arr,…arr2]; 模板字符串（保持格式、表达式解析） 反引号 ${} 对象简洁表示法 当对象的key与对应的属性所引用的变量或函数同名的时候，可以简写成一个。 12345678910111213var a = 1;var fn = function()&#123;&#125;var obj = &#123; a:a, fn:fn&#125;//简写方式var obj = &#123; a:, fn&#125; 属性名表达式 1234var x = 'hh'var obj = &#123; [x]:1 //key可以为动态的&#125; 迭代 12345678var attrs = ['a','b','c'];for(var attr in attrs)&#123; console.log(attr) // 0 1 2&#125;for(var attr of attrs)&#123; console.log(attr) // a b c&#125; 迭代器 12345678910111213141516171819202122232425262728var obj = &#123; left :100, top :200 &#125;obj[Symbol.iterator] = function()&#123; let keys = Object.keys(obj) //['left','top'] let len = keys.length; let n = 0; return &#123; next:function()&#123; if(n&lt;len)&#123; return&#123; value:&#123;k:keys[n],v:obj[keys[n++]]&#125;, done:false &#125; &#125;else&#123; return&#123; done:true &#125; &#125; &#125; &#125;&#125;for(var &#123;k,v&#125; of obj)&#123; console.log(k,v)&#125; 函数扩展 参数默认值 1function fn2(x=0,y=2)&#123;&#125; rest参数 1function arrayPush(arr,...newData)&#123;&#125;//从第二个参数开始，后面的数据全部赋值给newData参数 newData是一个数组，剩余参数只能写在形参的最后面 箭头函数 注意事项： 内部this对象指向创建期上下文对象，在函数创建时就绑定好了。普通函数的this指向是在函数的执行期绑定的 不能作为构造函数，不能new 没有arguements 不能作为生成器函数 Array 1234567891011121314151617181920212223242526console.log(['a','b','c'].includes('c'))//every 如果都为真，返回真，否则为假 做全选操作var arr4 = [1,5,8,19]var res = arr4.every(v=&gt;&#123;return v&gt;5&#125;)console.log(res) //false//some 只要有一个为真，就为真var arr4 = [1,5,8,19]var res = arr4.some(v=&gt;&#123;return v&gt;5&#125;)console.log(res) //true//filtervar arr4 = [1,5,8,19]var res = arr4.filter(v=&gt;&#123;return v&gt;5&#125;)console.log(res) //[8,19]//map 映射var arr4 = [1,5,8,19]var res = arr4.map(v=&gt;&#123;return v*5&#125;)console.log(res) //[5, 25, 40, 95]//reduce 拆分 购物车计算总和var arr4 = [1,5,8,19]var res = arr4.reduce((prev,current)=&gt;&#123;return prev+current&#125;,0)console.log(res) //33 Object 123456789//只改变第一个参数，如果有相同的会覆盖第一个参数的值；浅拷贝let obj1 = Object.assign(&#123;&#125;,&#123;x:1,y:12&#125;)console.log(obj1)//定义属性 configurable/enumerable/writable 默认都为falseobj4 = &#123;x:1&#125;obj4.y = 12;Object.defineProperty(obj4,'z',&#123;value:88&#125;)console.log(obj4) 集合 weakMap 类似map,但是key必须是对象，特点是key是弱引用的，GC机制不考虑回收问题。 异步 promise 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182//Promise 构造函数，接受一个参数 callback,把要执行的异步任务放置在callback中//promise内部会维护一个状态 默认是pending 成功：resolved 失败：rejectedlet p = new Promise((resolve,reject)=&gt;&#123; //当promise被实例化的时候，callback就会执行 setTimeout(() =&gt; &#123; //可以通过传入的resolve,reject两个函数，去改变当前Promise任务的状态 //调用resolve函数，把状态改成resolved.调用reject函数，把状态改成rejected console.log("1"); //promise对象下有一个方法：then,该方法在promise状态发生改变时，触发then的回调 reject() &#125;, 1000);&#125;);//then会接受两个参数：这两个参数都是回调，当对应的promise对象的状态变成resolved,那么then的//第一个callback就会被执行，如果状态变成了rejected,那么then的第二个callback就会被执行。// let p2 = p.then(()=&gt;&#123;// console.log("成功")// &#125;,()=&gt;&#123;// console.log("失败")// &#125;);//then执行后，无论执行的那个回调函数，都会返回一个新的成功状态的promise对象p.then(()=&gt;&#123; console.log("2")&#125;,()=&gt;&#123; console.log("a") // return new Promise((resolve,reject)=&gt;&#123; // reject() // &#125;) //上面的简写 return Promise.reject();&#125;).then(()=&gt;&#123; console.log("3")&#125;,()=&gt;&#123; console.log("b")&#125;).then(()=&gt;&#123; console.log("4")&#125;).catch(err=&gt;&#123; //catch方法和then一样，也会返回一个resolved状态的promise对象 console.log('错了')&#125;).then(()=&gt;&#123; console.log("5")&#125;)let ps1 = new Promise((resolve,reject)=&gt;&#123; setTimeout(() =&gt; &#123; //传递参数给then console.log("1") //reject('出错了') resolve(100) &#125;, 1000);&#125;).then(data=&gt;&#123; console.log(data)&#125;,err=&gt;&#123; console.log(err)&#125;);let ps2 = new Promise((resolve,reject)=&gt;&#123; setTimeout(() =&gt; &#123; //传递参数给then console.log("1") //reject('出错了') resolve(100) &#125;, 1000);&#125;).then(data=&gt;&#123; console.log(data) return Promise.resolve(300)&#125;,err=&gt;&#123; console.log(err)&#125;);//[]中放promise对象，都完成后执行thenPromise.all([ps1,ps2]) .then(data=&gt;&#123; console.log(data)//返回的data为所有执行结果的数组&#125;,err=&gt;&#123; console.log(err)&#125;);//谁先跑完触发then,另一个不管了Promise.race([]) generator 123456789101112131415function* fn()&#123; console.log("1") yield getData(); console.log("3");&#125;function getData()&#123; setTimeout(() =&gt; &#123; console.log(2) f.next() &#125;, 1000);&#125;//返回一个迭代器函数let f = fn();f.next(); async、await 1234567891011121314151617function getData()&#123; return new Promise((resolve,reject)=&gt;&#123; //resolve(100) reject('err') &#125;)&#125;async function fn1()&#123; console.log(222) try&#123; let v = await getData(); console.log(v); console.log(333) &#125;catch(e)&#123; console.log(e) &#125;&#125;fn1(); ​ ​ ​]]></content>
  </entry>
  <entry>
    <title><![CDATA[tomcat源码--启动脚本]]></title>
    <url>%2F2019%2F02%2F03%2Ftomcat%E6%BA%90%E7%A0%81-%E5%90%AF%E5%8A%A8%E8%84%9A%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[startup.bat1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859@echo offrem Licensed to the Apache Software Foundation (ASF) under one or morerem contributor license agreements. See the NOTICE file distributed withrem this work for additional information regarding copyright ownership.rem The ASF licenses this file to You under the Apache License, Version 2.0rem (the &quot;License&quot;); you may not use this file except in compliance withrem the License. You may obtain a copy of the License atremrem http://www.apache.org/licenses/LICENSE-2.0remrem Unless required by applicable law or agreed to in writing, softwarerem distributed under the License is distributed on an &quot;AS IS&quot; BASIS,rem WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.rem See the License for the specific language governing permissions andrem limitations under the License.rem ---------------------------------------------------------------------------rem Start script for the CATALINA Serverrem ---------------------------------------------------------------------------#临时修改系统变量setlocal#设置CATALINA_HOME环境变量rem Guess CATALINA_HOME if not definedset &quot;CURRENT_DIR=%cd%&quot;if not &quot;%CATALINA_HOME%&quot; == &quot;&quot; goto gotHomeset &quot;CATALINA_HOME=%CURRENT_DIR%&quot;if exist &quot;%CATALINA_HOME%\bin\catalina.bat&quot; goto okHomecd ..set &quot;CATALINA_HOME=%cd%&quot;cd &quot;%CURRENT_DIR%&quot;:gotHomeif exist &quot;%CATALINA_HOME%\bin\catalina.bat&quot; goto okHomeecho The CATALINA_HOME environment variable is not defined correctlyecho This environment variable is needed to run this programgoto end:okHome#验证CATALINA_HOME变量目录下的catalina.bat文件是否存在，不存在则批处理直接结束set &quot;EXECUTABLE=%CATALINA_HOME%\bin\catalina.bat&quot;rem Check that target executable existsif exist &quot;%EXECUTABLE%&quot; goto okExececho Cannot find &quot;%EXECUTABLE%&quot;echo This file is needed to run this programgoto end:okExec#如果设置了其他参数，将参数保存到 CMD_LINE_ARGS 变量中rem Get remaining unshifted command line arguments and save them in theset CMD_LINE_ARGS=:setArgsif &quot;&quot;%1&quot;&quot;==&quot;&quot;&quot;&quot; goto doneSetArgsset CMD_LINE_ARGS=%CMD_LINE_ARGS% %1shiftgoto setArgs:doneSetArgs#执行 catalina.bat 批处理文件，注意，该行后面跟着两个参数，第1个参数是 startcall &quot;%EXECUTABLE%&quot; start %CMD_LINE_ARGS%:end catalina.battomcat类加载设计 整个Tomcat的classLoader分为了两条线，左边的一条线为catalinaLoader，这个是Tomcat服务器专用的，用于加载Tomcat服务器本身的class，右边的一条线则为web应用程序用的，每一个web应用程序都有自己专用的WebappClassLoader，用于加载属于自己应用程序的资源，例如/web-inf/lib下面的jar包，classes里面的class文件。]]></content>
      <tags>
        <tag>tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[javaweb获取类路径下资源]]></title>
    <url>%2F2019%2F01%2F24%2Fjavaweb%E8%8E%B7%E5%8F%96%E7%B1%BB%E8%B7%AF%E5%BE%84%E4%B8%8B%E8%B5%84%E6%BA%90%2F</url>
    <content type="text"><![CDATA[项目结构： 类路径：指的是编译后的class文件的位置，如果是IDEA的话，一般在\项目名\out\artifacts\项目扩展名WEB-INF\classes\a.txt 编译后文件路径 获取类路径下资源的方式 ClassLoader 123456789101112@Controllerpublic class PathController &#123; @RequestMapping("/path") public void testPath() throws IOException &#123; //如果资源文件不是直接在src下，而是在其他包下面,要改成 getResourceAsStream("com/haominglfs/test/a.txt") 开头没有斜杠 InputStream resourceAsStream = this.getClass().getClassLoader().getResourceAsStream("a.txt"); BufferedReader br = new BufferedReader(new InputStreamReader(resourceAsStream,"UTF-8")); String s = br.readLine(); System.out.println(s); &#125;&#125; Class 1234//得到ClassClass c = this.getClass();//相对于当前.class文件所在目录,开头还是没有斜杆的InputStream input = c.getResourceAsStream(&quot;a.txt&quot;); 如果资源文件和.class文件不同目录 1getResourceAsStream("/a.txt"); 注意：这里加了一个斜杆]]></content>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring注解]]></title>
    <url>%2F2019%2F01%2F19%2Fspring%E6%B3%A8%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[@Configuration 告诉spring这是一个配置类（配置类==配置文件） @Bean 给容器中注册一个bean;类型为返回值的类型；id默认用方法名作为id; @ComponentScan(value=”要扫描的包”) 包扫描注解 @Scope 调整组件的作用域（默认是单实例） @Lazy 懒加载（默认单实例bean在容器创建的时候创建） @Conditional({Condition}) 按照一定的条件进行判断，满足条件给容器中注册bean; Condition实现Condition接口，实现matches方法； 放在类上，统一设置。 @Import,给容器中注册组件方式： 包扫描+组件标注注解（@Controller/@Service/@Repository/@Component）[自己写的类] @Bean[导入的第三方包里面的组件] @Import[快速给容器中导入一个组件]; @Import(要导入容器中的组件)，容器中就会自动注册这个组件，id默认是全类名。 实现ImportSelector接口:返回需要导入的组件的全类名数组，方法不要返回null。 ImportBeanDefinitionRegistrar的实现类。 使用spring提供的FactoryBean;默认获取到的是工厂bean调用getObject创建的对象。要获取工厂bean本身，需要给id前面加一个&amp; 生命周期 指定初始化和销毁方法 @Bean(init-method=””,destory-method=””) 初始化：对象创建完成并赋值好，调用初始化方法 销毁：单实例：容器关闭的时候；多实例：容器不会管理这个bean,容器不会调用销毁方法。 让bean实现接口InitializingBean、DisposableBean。 @PostConstruct:在bean创建完成并且属性赋值完成，来进行初始化方法；@PreDistory:销毁时要调用的方法。 BeanPostProcessor:bean的后置处理器，在bean初始化前后进行一些处理工作。 postProcessBeforeInitialization：bean初始化之前调用方法。 postProcessAfterInitialization：在初始化之后工作。 属性赋值 @Value赋值 基本数值 SpEL表达式：#{} ${}：取出配置文件中的值 @PropertySource(value=”classpath:”) 读取外部配置文件 自动装配 @Autowired;默认优先按照类型去容器中找对应的组件，如果找到多个相同类型的组件，再将属性的名称作为组件的id去容器中查找。 @Qualifire(“bookDao”);明确指定要装配的id @Primary:spring自动装配的时候，默认使用首选的bean;也可以使用@Qualifire明确指定。 Spring还支持使用@Resource(JSR250)和@Inject(JSR330)[java规范注解] @Resource没有支持@Primary和@Autowired(required=false)的功能。 @Inject：需要先导入javax.inject的包。 如果组件只有一个有参构造器，@Autowired可以省略。 @Bean标注的方法创建对象时，方法参数的值从容器中获取。 自定义组件使用spring容器底层的一些组件（ApplicationContext、BeanFactory…），自定义组件实现xxxAware。xxxAware的功能是使用xxxProcessor实现的： ApplicationContextAware—&gt;ApplicationContextAwareProcessor后置处理器。 @Profile spring为我们提供的可以根据当前的环境，动态的激活和切换一系列组件的功能。 指定组件在哪个环境的情况下才能注册到容器中。 @Profile(“default”) @Profile(“dev”) @Profile(“test”) @Profile(“prod”) 默认是default 使用命令行动态参数切换 （在虚拟机参数位置） -Dspring.profiles.active=test 激活test环境 使用代码的方式 1applicationContext.getEnvironment().setActiveProfiles("test") 没有标注@Profile的bean,在所有环境下都会注册到容器。]]></content>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mongodb使用]]></title>
    <url>%2F2018%2F12%2F20%2Fmongodb%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344454647db.students.find(&#123;age:20,name:'sss'&#125;)db.students.find(&#123;age:20&#125;)db.students.findOne(&#123;age:20&#125;).namedb.students.find()db.students.insertOne(&#123;name:"mmm",age:20&#125;)db.students.find(&#123;age:20&#125;).count()db.students.find(&#123;age:20&#125;).length()db.students.update(&#123;name:"hm"&#125;,&#123;age:28&#125;)//默认情况只会修改一个db.students.update(&#123;"_id" : ObjectId("5c17c5836ff3da5ffa6da9f7")&#125;,&#123;$set:&#123;name:'hm',gender:"男",address:"石家庄"&#125;&#125;)db.students.update(&#123;"_id" : ObjectId("5c17c5836ff3da5ffa6da9f7")&#125;,&#123;$unset:&#123;address:"石家庄"&#125;&#125;)db.students.remove(&#123;age:20&#125;)db.students.remove(&#123;&#125;)db.students.drop()for(var i=1;i&lt;10;i++)&#123; db.logs.insert(&#123;name:i&#125;)&#125;//var list = db.logs.find();//printjson(list)db.logs.find().sort(&#123;$natural:-1&#125;)db.logs.insert(&#123;name:undefined&#125;)//db.logs.find(&#123;name:undefined&#125;)db.logs.find(&#123;name:&#123;$type:6&#125;&#125;)db.logs.find(&#123;name:&#123;$type:"undefined"&#125;&#125;) mongoTemplate使用(kotlin)123456789//根据id查询mongoTemplate.findById(oid, Place::class.java)//保存文档对象mongoTemplate.save(place)//构造查询对象val pattern= Pattern.compile("^http", Pattern.CASE_INSENSITIVE) //匹配正则表达式val query = Query(Criteria.where("coverPicture").regex(pattern))query.with(PageRequest.of(page, 30)) //分页处理val items = mongoTemplate.find(query, Place::class.java)]]></content>
  </entry>
  <entry>
    <title><![CDATA[hibernate-HQL查询]]></title>
    <url>%2F2018%2F12%2F16%2Fhibernate-HQL%E6%9F%A5%E8%AF%A2%2F</url>
    <content type="text"><![CDATA[Hibernate提供了以下几种检索对象的方式： 导航对象图检索方式: 根据已经加载的对象导航到其他对象。 OID 检索方式: 按照对象的OID来检索对象。 HQL 检索方式: 使用面向对象的 HQL 查询语言。 QBC 检索方式: 使用QBC(QueryBy Criteria) API 来检索对象. 这种API封装了基于字符串形式的查询语句,提供了更加面向对象的查询接口。 本地SQL检索方式:使用本地数据库的SQL查询语句。 HQL(HibernateQuery Language) 是面向对象的查询语言, 它和SQL查询语言有些相似.在Hibernate提供的各种检索方式中,HQL 是使用最广的一种检索方式.它有如下功能: 在查询语句中设定各种查询条件。 支持投影查询,即仅检索出对象的部分属性。 支持分页查询。 支持连接查询。 支持分组查询,允许使用HAVING和GROUPBY 关键字。 提供内置聚集函数,如sum(),min() 和 max()。 支持子查询。 支持动态绑定参数。 能够调用用户定义的 SQL 函数或标准的SQL函数。 HQL检索方式包括以下步骤: 通过Session的createQuery()方法创建一个Query对象,它包括一个HQL查询语句.HQL 查询语句中可以包含命名参数。 动态绑定参数。 调用Query相关方法执行查询语句。 测试用例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358 @Test public void testBatch()&#123; session.doWork(new Work() &#123; @Override public void execute(Connection connection) throws SQLException &#123; //通过 JDBC 原生的 API 进行操作, 效率最高, 速度最快! &#125; &#125;); &#125; @Test public void testManageSession()&#123; //获取 Session //开启事务 Session session = HibernateUtils.getInstance().getSession(); System.out.println("--&gt;" + session.hashCode()); Transaction transaction = session.beginTransaction(); DepartmentDao departmentDao = new DepartmentDao(); Department dept = new Department(); dept.setName("ATGUIGU"); departmentDao.save(dept); departmentDao.save(dept); departmentDao.save(dept); //若 Session 是由 thread 来管理的, 则在提交或回滚事务时, 已经关闭 Session 了. transaction.commit(); System.out.println(session.isOpen()); &#125; @Test public void testQueryIterate()&#123; Department dept = (Department) session.get(Department.class, 70); System.out.println(dept.getName()); System.out.println(dept.getEmps().size()); Query query = session.createQuery("FROM Employee e WHERE e.dept.id = 80");// List&lt;Employee&gt; emps = query.list();// System.out.println(emps.size()); Iterator&lt;Employee&gt; empIt = query.iterate(); while(empIt.hasNext())&#123; System.out.println(empIt.next().getName()); &#125; &#125; @Test public void testUpdateTimeStampCache()&#123; Query query = session.createQuery("FROM Employee"); query.setCacheable(true); List&lt;Employee&gt; emps = query.list(); System.out.println(emps.size()); Employee employee = (Employee) session.get(Employee.class, 100); employee.setSalary(30000); emps = query.list(); System.out.println(emps.size()); &#125; @Test public void testQueryCache()&#123; Query query = session.createQuery("FROM Employee"); query.setCacheable(true); List&lt;Employee&gt; emps = query.list(); System.out.println(emps.size()); emps = query.list(); System.out.println(emps.size()); Criteria criteria = session.createCriteria(Employee.class); criteria.setCacheable(true); &#125; @Test public void testCollectionSecondLevelCache()&#123; Department dept = (Department) session.get(Department.class, 80); System.out.println(dept.getName()); System.out.println(dept.getEmps().size()); transaction.commit(); session.close(); session = sessionFactory.openSession(); transaction = session.beginTransaction(); Department dept2 = (Department) session.get(Department.class, 80); System.out.println(dept2.getName()); System.out.println(dept2.getEmps().size()); &#125; @Test public void testHibernateSecondLevelCache()&#123; Employee employee = (Employee) session.get(Employee.class, 100); System.out.println(employee.getName()); transaction.commit(); session.close(); session = sessionFactory.openSession(); transaction = session.beginTransaction(); Employee employee2 = (Employee) session.get(Employee.class, 100); System.out.println(employee2.getName()); &#125; @Test public void testHQLUpdate()&#123; String hql = "DELETE FROM Department d WHERE d.id = :id"; session.createQuery(hql).setInteger("id", 280) .executeUpdate(); &#125; @Test public void testNativeSQL()&#123; String sql = "INSERT INTO gg_department VALUES(?, ?)"; Query query = session.createSQLQuery(sql); query.setInteger(0, 280) .setString(1, "ATGUIGU") .executeUpdate(); &#125; @Test public void testQBC4()&#123; Criteria criteria = session.createCriteria(Employee.class); //1. 添加排序 criteria.addOrder(Order.asc("salary")); criteria.addOrder(Order.desc("email")); //2. 添加翻页方法 int pageSize = 5; int pageNo = 3; criteria.setFirstResult((pageNo - 1) * pageSize) .setMaxResults(pageSize) .list(); &#125; @Test public void testQBC3()&#123; Criteria criteria = session.createCriteria(Employee.class); //统计查询: 使用 Projection 来表示: 可以由 Projections 的静态方法得到 criteria.setProjection(Projections.max("salary")); System.out.println(criteria.uniqueResult()); &#125; @Test public void testQBC2()&#123; Criteria criteria = session.createCriteria(Employee.class); //1. AND: 使用 Conjunction 表示 //Conjunction 本身就是一个 Criterion 对象 //且其中还可以添加 Criterion 对象 Conjunction conjunction = Restrictions.conjunction(); conjunction.add(Restrictions.like("name", "a", MatchMode.ANYWHERE)); Department dept = new Department(); dept.setId(80); conjunction.add(Restrictions.eq("dept", dept)); System.out.println(conjunction); //2. OR Disjunction disjunction = Restrictions.disjunction(); disjunction.add(Restrictions.ge("salary", 6000F)); disjunction.add(Restrictions.isNull("email")); criteria.add(disjunction); criteria.add(conjunction); criteria.list(); &#125; @Test public void testQBC()&#123; //1. 创建一个 Criteria 对象 Criteria criteria = session.createCriteria(Employee.class); //2. 添加查询条件: 在 QBC 中查询条件使用 Criterion 来表示 //Criterion 可以通过 Restrictions 的静态方法得到 criteria.add(Restrictions.eq("email", "SKUMAR")); criteria.add(Restrictions.gt("salary", 5000F)); //3. 执行查询 Employee employee = (Employee) criteria.uniqueResult(); System.out.println(employee); &#125; @Test public void testLeftJoinFetch2()&#123; String hql = "SELECT e FROM Employee e INNER JOIN e.dept"; Query query = session.createQuery(hql); List&lt;Employee&gt; emps = query.list(); System.out.println(emps.size()); for(Employee emp: emps)&#123; System.out.println(emp.getName() + ", " + emp.getDept().getName()); &#125; &#125; @Test public void testLeftJoin()&#123; String hql = "SELECT DISTINCT d FROM Department d LEFT JOIN d.emps"; Query query = session.createQuery(hql); List&lt;Department&gt; depts = query.list(); System.out.println(depts.size()); for(Department dept: depts)&#123; System.out.println(dept.getName() + ", " + dept.getEmps().size()); &#125; // List&lt;Object []&gt; result = query.list(); // result = new ArrayList&lt;&gt;(new LinkedHashSet&lt;&gt;(result));// System.out.println(result); // // for(Object [] objs: result)&#123;// System.out.println(Arrays.asList(objs));// &#125; &#125; @Test public void testLeftJoinFetch()&#123;// String hql = "SELECT DISTINCT d FROM Department d LEFT JOIN FETCH d.emps"; String hql = "FROM Department d INNER JOIN FETCH d.emps"; Query query = session.createQuery(hql); List&lt;Department&gt; depts = query.list(); depts = new ArrayList&lt;&gt;(new LinkedHashSet(depts)); System.out.println(depts.size()); for(Department dept: depts)&#123; System.out.println(dept.getName() + "-" + dept.getEmps().size()); &#125; &#125; @Test public void testGroupBy()&#123; String hql = "SELECT min(e.salary), max(e.salary) " + "FROM Employee e " + "GROUP BY e.dept " + "HAVING min(salary) &gt; :minSal"; Query query = session.createQuery(hql) .setFloat("minSal", 8000); List&lt;Object []&gt; result = query.list(); for(Object [] objs: result)&#123; System.out.println(Arrays.asList(objs)); &#125; &#125; @Test public void testFieldQuery2()&#123; String hql = "SELECT new Employee(e.email, e.salary, e.dept) " + "FROM Employee e " + "WHERE e.dept = :dept"; Query query = session.createQuery(hql); Department dept = new Department(); dept.setId(80); List&lt;Employee&gt; result = query.setEntity("dept", dept) .list(); for(Employee emp: result)&#123; System.out.println(emp.getId() + ", " + emp.getEmail() + ", " + emp.getSalary() + ", " + emp.getDept()); &#125; &#125; @Test public void testFieldQuery()&#123; String hql = "SELECT e.email, e.salary, e.dept FROM Employee e WHERE e.dept = :dept"; Query query = session.createQuery(hql); Department dept = new Department(); dept.setId(80); List&lt;Object[]&gt; result = query.setEntity("dept", dept) .list(); for(Object [] objs: result)&#123; System.out.println(Arrays.asList(objs)); &#125; &#125; @Test public void testNamedQuery()&#123; Query query = session.getNamedQuery("salaryEmps"); List&lt;Employee&gt; emps = query.setFloat("minSal", 5000) .setFloat("maxSal", 10000) .list(); System.out.println(emps.size()); &#125; @Test public void testPageQuery()&#123; String hql = "FROM Employee"; Query query = session.createQuery(hql); int pageNo = 22; int pageSize = 5; List&lt;Employee&gt; emps = query.setFirstResult((pageNo - 1) * pageSize) .setMaxResults(pageSize) .list(); System.out.println(emps); &#125; @Test public void testHQLNamedParameter()&#123; //1. 创建 Query 对象 //基于命名参数. String hql = "FROM Employee e WHERE e.salary &gt; :sal AND e.email LIKE :email"; Query query = session.createQuery(hql); //2. 绑定参数 query.setFloat("sal", 7000) .setString("email", "%A%"); //3. 执行查询 List&lt;Employee&gt; emps = query.list(); System.out.println(emps.size()); &#125; @Test public void testHQL()&#123; //1. 创建 Query 对象 //基于位置的参数. String hql = "FROM Employee e WHERE e.salary &gt; ? AND e.email LIKE ? AND e.dept = ? " + "ORDER BY e.salary"; Query query = session.createQuery(hql); //2. 绑定参数 //Query 对象调用 setXxx 方法支持方法链的编程风格. Department dept = new Department(); dept.setId(80); query.setFloat(0, 6000) .setString(1, "%A%") .setEntity(2, dept); //3. 执行查询 List&lt;Employee&gt; emps = query.list(); System.out.println(emps.size()); &#125;&#125;]]></content>
      <tags>
        <tag>hibernate</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hibernate的session管理方式]]></title>
    <url>%2F2018%2F12%2F14%2Fhibernate%E7%9A%84session%E7%AE%A1%E7%90%86%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[Hibernate 自身提供了三种管理 Session对象的方法： Session 对象的生命周期与本地线程绑定。 –Session对象的生命周期与JTA事务绑定。 –Hibernate委托程序管理Session对象的生命周期。 在Hibernate的配置文件中,hibernate.current_session_context_class属性用于指定Session管理方式,可选值包括： thread:Session 对象的生命周期与本地线程绑定。 jta:Session 对象的生命周期与 JTA 事务绑定。 managed:Hibernate 委托程序来管理 Session对象的生命周期。 配置与使用 12&lt;!-- 配置管理 Session 的方式 --&gt;&lt;property name="current_session_context_class"&gt;thread&lt;/property&gt; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273public class HibernateUtils &#123; private HibernateUtils()&#123;&#125; private static HibernateUtils instance = new HibernateUtils(); public static HibernateUtils getInstance() &#123; return instance; &#125; private SessionFactory sessionFactory; public SessionFactory getSessionFactory() &#123; if (sessionFactory == null) &#123; Configuration configuration = new Configuration().configure(); ServiceRegistry serviceRegistry = new ServiceRegistryBuilder() .applySettings(configuration.getProperties()) .buildServiceRegistry(); sessionFactory = configuration.buildSessionFactory(serviceRegistry); &#125; return sessionFactory; &#125; public Session getSession()&#123; return getSessionFactory().getCurrentSession(); &#125;&#125;==============================================================public class DepartmentDao &#123; public void save(Department dept)&#123; //内部获取 Session 对象 //获取和当前线程绑定的 Session 对象 //1. 不需要从外部传入 Session 对象 //2. 多个 DAO 方法也可以使用一个事务! Session session = HibernateUtils.getInstance().getSession(); System.out.println(session.hashCode()); session.save(dept); &#125; /** * 若需要传入一个 Session 对象, 则意味着上一层(Service)需要获取到 Session 对象. * 这说明上一层需要和 Hibernate 的 API 紧密耦合. 所以不推荐使用此种方式. */ public void save(Session session, Department dept)&#123; session.save(dept); &#125;&#125;================================================================@Test public void testManageSession()&#123; //获取 Session //开启事务 Session session = HibernateUtils.getInstance().getSession(); System.out.println("--&gt;" + session.hashCode()); Transaction transaction = session.beginTransaction(); DepartmentDao departmentDao = new DepartmentDao(); Department dept = new Department(); dept.setName("ATGUIGU"); departmentDao.save(dept); departmentDao.save(dept); departmentDao.save(dept); //若 Session 是由 thread 来管理的, 则在提交或回滚事务时, 已经关闭 Session 了. transaction.commit(); System.out.println(session.isOpen()); &#125; 如果把 Hibernate 配置文件的 hibernate.current_session_context_class 属性值设为 thread, Hibernate 就会按照与本地线程绑定的方式来管理 Session，Hibernate 按一下规则把 Session 与本地线程绑定： 当一个线程(threadA)第一次调用SessionFactory对象的getCurrentSession()方法时,该方法会创建一个新的Session(sessionA)对象,把该对象与threadA绑定,并将sessionA返回。 当threadA再次调用SessionFactory对象的getCurrentSession()方法时,该方法将返回sessionA对象。 当 threadA提交sessionA对象关联的事务时,Hibernate 会自动flushsessionA对象的缓存,然后提交事务,关闭sessionA对象.当threadA撤销sessionA对象关联的事务时,也会自动关闭sessionA对象。 若 threadA再次调用SessionFactory对象的getCurrentSession()方法时,该方法会又创建一个新的Session(sessionB)对象,把该对象与threadA绑定,并将sessionB返回。 批量操作 推荐使用原生的JDBC方式。]]></content>
  </entry>
  <entry>
    <title><![CDATA[hibernate缓存]]></title>
    <url>%2F2018%2F12%2F13%2Fhibernate%E7%BC%93%E5%AD%98%2F</url>
    <content type="text"><![CDATA[缓存级别 第一级别的缓存是Session级别的缓存，它是属于事务范围的缓存。这一级别的缓存由hibernate管理的。 第二级别的缓存是SessionFactory级别的缓存，它是属于进程范围的缓存，sessionFactory级别的缓存分为两类： 内置缓存: Hibernate 自带的, 不可卸载.通常在Hibernate的初始化阶段,Hibernate 会把映射元数据和预定义的 SQL语句放到SessionFactory的缓存中,映射元数据是映射文件中数据（.hbm.xml文件中的数据）的复制.该内置缓存是只读的。 外置缓存(二级缓存):一个可配置的缓存插件.在默认情况下,SessionFactory不会启用这个缓存插件.外置缓存中的数据是数据库数据的复制,外置缓存的物理介质可以是内存或硬盘。 二级缓存的适用场景 适合放入二级缓存中的数据： 很少被修改 不是很重要的数据,允许出现偶尔的并发问题 不适合放入二级缓存中的数据： 经常被修改 财务数据,绝对不允许出现并发问题 与其他应用程序共享的数据 管理二级缓存，二级缓存是可配置的的插件,Hibernate 允许选用以下类型的缓存插件: EHCache: 可作为进程范围内的缓存,存放数据的物理介质可以是内存或硬盘,对Hibernate的查询缓存提供了支持。 其他看官方文档。 使用二级缓存的步骤: 加入二级缓存插件的 jar 包及配置文件: 复制 \hibernate-release-4.2.4.Final\lib\optional\ehcache*.jar 到当前 Hibrenate 应用的类路径下 复制 \hibernate-release-4.2.4.Final\project\etc\ehcachexml 到当前 WEB 应用的类路径下 配置 hibernate.cfg.xml 12345678910111213141516171819202122&lt;!-- 启用二级缓存 --&gt;&lt;property name="cache.use_second_level_cache"&gt;true&lt;/property&gt;&lt;!-- 配置使用的二级缓存的产品 --&gt;&lt;property name="hibernate.cache.region.factory_class"&gt;org.hibernate.cache.ehcache.EhCacheRegionFactory&lt;/property&gt;&lt;!-- 配置对哪些类使用 hibernate 的二级缓存 (实际上也可以在 .hbm.xml 文件中配置对哪些类使用二级缓存, 及二级缓存的策略是什么)--&gt;&lt;class-cache usage="read-write" class="com.atguigu.hibernate.entities.Employee"/&gt;===============================集合的二级缓存&lt;collection-cache usage="read-write" collection="com.atguigu.hibernate.entities.Department.emps"/&gt;&lt;!--也可以在 .hbm.xml 文件中进行配置--&gt;&lt;set name="emps" table="GG_EMPLOYEE" inverse="true" lazy="true"&gt; &lt;cache usage="read-write"/&gt; &lt;key&gt; &lt;column name="DEPT_ID" /&gt; &lt;/key&gt; &lt;one-to-many class="com.atguigu.hibernate.entities.Employee" /&gt;&lt;/set&gt;&lt;!--注意: 还需要配置集合中的元素对应的持久化类也使用二级缓存! 否则将会多出 n 条 SQL 语句. --&gt;=================================查询缓存&lt;!-- 查询缓存: 默认情况下, 设置的缓存对 HQL 及 QBC 查询时无效的, 但可以通过以下方式使其是有效的 --&gt;&lt;property name="cache.use_query_cache"&gt;true&lt;/property&gt;&lt;!-- 调用 Query 或 Criteria 的 setCacheable(true) 方法 查询缓存依赖于二级缓存 --&gt; ehcache 的配置文件: ehcache.xml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556&lt;ehcache&gt; &lt;!-- 指定一个目录：当 EHCache 把数据写到硬盘上时, 将把数据写到这个目录下. --&gt; &lt;diskStore path="d:\\tempDirectory"/&gt; &lt;!-- 设置缓存的默认数据过期策略 --&gt; &lt;defaultCache maxElementsInMemory="10000" eternal="false" timeToIdleSeconds="120" timeToLiveSeconds="120" overflowToDisk="true" /&gt; &lt;!-- 设定具体的命名缓存的数据过期策略。每个命名缓存代表一个缓存区域 缓存区域(region)：一个具有名称的缓存块，可以给每一个缓存块设置不同的缓存策略。 如果没有设置任何的缓存区域，则所有被缓存的对象，都将使用默认的缓存策略。即：&lt;defaultCache.../&gt; Hibernate 在不同的缓存区域保存不同的类/集合。 对于类而言，区域的名称是类名。如:com.atguigu.domain.Customer 对于集合而言，区域的名称是类名加属性名。如com.atguigu.domain.Customer.orders --&gt; &lt;!-- name: 设置缓存的名字,它的取值为类的全限定名或类的集合的名字 maxElementsInMemory: 设置基于内存的缓存中可存放的对象最大数目 eternal: 设置对象是否为永久的, true表示永不过期, 此时将忽略timeToIdleSeconds 和 timeToLiveSeconds属性; 默认值是false timeToIdleSeconds:设置对象空闲最长时间,以秒为单位, 超过这个时间,对象过期。 当对象过期时,EHCache会把它从缓存中清除。如果此值为0,表示对象可以无限期地处于空闲状态。 timeToLiveSeconds:设置对象生存最长时间,超过这个时间,对象过期。 如果此值为0,表示对象可以无限期地存在于缓存中. 该属性值必须大于或等于 timeToIdleSeconds 属性值 overflowToDisk:设置基于内存的缓存中的对象数目达到上限后,是否把溢出的对象写到基于硬盘的缓存中 --&gt; &lt;cache name="com.atguigu.hibernate.entities.Employee" maxElementsInMemory="1" eternal="false" timeToIdleSeconds="300" timeToLiveSeconds="600" overflowToDisk="true" /&gt; &lt;cache name="com.atguigu.hibernate.entities.Department.emps" maxElementsInMemory="1000" eternal="true" timeToIdleSeconds="0" timeToLiveSeconds="0" overflowToDisk="false" /&gt;&lt;/ehcache&gt;]]></content>
  </entry>
  <entry>
    <title><![CDATA[kotlin]]></title>
    <url>%2F2018%2F12%2F12%2Fkotlin%2F</url>
    <content type="text"><![CDATA[空类型任意类型都有可空和不可空两种 val notNull:String = null //错误，不能为空 val nullable:String? = null //正确，可以为空 notNull.length //正确，不为空的值可以直接使用 Nullable.length //错误，可能为空，不能直接获取长度 Nullable!!.length //正确，强制认定nullable不可空 Nullable?.length //正确，若nullable为空，返回空 智能类型转换 java style 类型转换 val sub:subClass = parent as subClass 类似于java的类型转换，失败则抛出异常 安全类型转换 val sub :subClass = parent as? subClass 如果转换失败，返回null,不抛出异常 Lamdba表达式 写法：{[参数列表] -&gt; [函数体,最后一行是返回值 ]} var还是val? 原则：如果两种方式都能满足需求的情况下，优先使用val声明，因为一方面val声明的变量是只读，一旦初始化后不能修改，还可以避免程序运行时错误的修改变量的内容；另一方面在声明引用类型使用val,对象的引用不会被修改，但是引用内容可以修改，这样会更加安全，也符合函数式编程的技术要求。 Elvis运算符 A ?: B 如果A不为空值则结果为A,否则结果为B。]]></content>
  </entry>
  <entry>
    <title><![CDATA[jpa]]></title>
    <url>%2F2018%2F12%2F12%2Fjpa%2F</url>
    <content type="text"><![CDATA[persistence.xml JPA规范要求在类路径的META-INF目录下放置persistence.xml，文件的名称是固定的。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;persistence version="2.0" xmlns="http://java.sun.com/xml/ns/persistence" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://java.sun.com/xml/ns/persistence http://java.sun.com/xml/ns/persistence/persistence_2_0.xsd"&gt; &lt;!-- 1.name 属性用于定义持久化单元的名字, 必选 2.transaction-type：指定 JPA 的事务处理策略。 RESOURCE_LOCAL：默认值，数据库级别的事务，只能针对一种数据库，不支持分布式事务。 如果需要支持分布式事务，使用JTA：transaction-type="JTA“ --&gt; &lt;persistence-unit name="jpa-1" transaction-type="RESOURCE_LOCAL"&gt; &lt;!-- 配置使用什么 ORM 产品来作为 JPA 的实现 1. 实际上配置的是 javax.persistence.spi.PersistenceProvider 接口的实现类 2. 若 JPA 项目中只有一个 JPA 的实现产品, 则也可以不配置该节点. --&gt; &lt;provider&gt;org.hibernate.ejb.HibernatePersistence&lt;/provider&gt; &lt;!-- 添加持久化类 --&gt; &lt;class&gt;com.atguigu.jpa.helloworld.Customer&lt;/class&gt; &lt;class&gt;com.atguigu.jpa.helloworld.Order&lt;/class&gt; &lt;class&gt;com.atguigu.jpa.helloworld.Department&lt;/class&gt; &lt;class&gt;com.atguigu.jpa.helloworld.Manager&lt;/class&gt; &lt;class&gt;com.atguigu.jpa.helloworld.Item&lt;/class&gt; &lt;class&gt;com.atguigu.jpa.helloworld.Category&lt;/class&gt; &lt;!-- 配置二级缓存的策略 ALL：所有的实体类都被缓存 NONE：所有的实体类都不被缓存. ENABLE_SELECTIVE：标识 @Cacheable(true) 注解的实体类将被缓存 DISABLE_SELECTIVE：缓存除标识 @Cacheable(false) 以外的所有实体类 UNSPECIFIED：默认值，JPA 产品默认值将被使用 --&gt; &lt;shared-cache-mode&gt;ENABLE_SELECTIVE&lt;/shared-cache-mode&gt; &lt;properties&gt; &lt;!-- 连接数据库的基本信息 --&gt; &lt;property name="javax.persistence.jdbc.driver" value="com.mysql.jdbc.Driver"/&gt; &lt;property name="javax.persistence.jdbc.url" value="jdbc:mysql:///jpa"/&gt; &lt;property name="javax.persistence.jdbc.user" value="root"/&gt; &lt;property name="javax.persistence.jdbc.password" value="1230"/&gt; &lt;!-- 配置 JPA 实现产品的基本属性. 配置 hibernate 的基本属性 --&gt; &lt;property name="hibernate.format_sql" value="true"/&gt; &lt;property name="hibernate.show_sql" value="true"/&gt; &lt;property name="hibernate.hbm2ddl.auto" value="update"/&gt; &lt;!-- 二级缓存相关 --&gt; &lt;property name="hibernate.cache.use_second_level_cache" value="true"/&gt; &lt;property name="hibernate.cache.region.factory_class" value="org.hibernate.cache.ehcache.EhCacheRegionFactory"/&gt; &lt;property name="hibernate.cache.use_query_cache" value="true"/&gt; &lt;/properties&gt; &lt;/persistence-unit&gt;&lt;/persistence&gt; 实体类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119@NamedQuery(name="testNamedQuery", query="FROM Customer c WHERE c.id = ?")@Cacheable(true)@Table(name="JPA_CUTOMERS")@Entitypublic class Customer &#123; private Integer id; private String lastName; private String email; private int age; private Date createdTime; private Date birth; public Customer() &#123; // TODO Auto-generated constructor stub &#125; public Customer(String lastName, int age) &#123; super(); this.lastName = lastName; this.age = age; &#125; private Set&lt;Order&gt; orders = new HashSet&lt;&gt;();// @TableGenerator(name="ID_GENERATOR",// table="jpa_id_generators",// pkColumnName="PK_NAME",// pkColumnValue="CUSTOMER_ID",// valueColumnName="PK_VALUE",// allocationSize=100)// @GeneratedValue(strategy=GenerationType.TABLE,generator="ID_GENERATOR") @GeneratedValue(strategy=GenerationType.AUTO) @Id public Integer getId() &#123; return id; &#125; public void setId(Integer id) &#123; this.id = id; &#125; @Column(name="LAST_NAME",length=50,nullable=false) public String getLastName() &#123; return lastName; &#125; public void setLastName(String lastName) &#123; this.lastName = lastName; &#125; public String getEmail() &#123; return email; &#125; public void setEmail(String email) &#123; this.email = email; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; @Temporal(TemporalType.TIMESTAMP) public Date getCreatedTime() &#123; return createdTime; &#125; public void setCreatedTime(Date createdTime) &#123; this.createdTime = createdTime; &#125; @Temporal(TemporalType.DATE) public Date getBirth() &#123; return birth; &#125; public void setBirth(Date birth) &#123; this.birth = birth; &#125; //映射单向 1-n 的关联关系 //使用 @OneToMany 来映射 1-n 的关联关系 //使用 @JoinColumn 来映射外键列的名称 //可以使用 @OneToMany 的 fetch 属性来修改默认的加载策略 //可以通过 @OneToMany 的 cascade 属性来修改默认的删除策略. //注意: 若在 1 的一端的 @OneToMany 中使用 mappedBy 属性, 则 @OneToMany 端就不能再使用 @JoinColumn 属性了. // @JoinColumn(name="CUSTOMER_ID") @OneToMany(fetch=FetchType.LAZY,cascade=&#123;CascadeType.REMOVE&#125;,mappedBy="customer") public Set&lt;Order&gt; getOrders() &#123; return orders; &#125; public void setOrders(Set&lt;Order&gt; orders) &#123; this.orders = orders; &#125; //工具方法. 不需要映射为数据表的一列. @Transient public String getInfo()&#123; return "lastName: " + lastName + ", email: " + email; &#125; @Override public String toString() &#123; return "Customer [id=" + id + ", lastName=" + lastName + ", email=" + email + ", age=" + age + ", createdTime=" + createdTime + ", birth=" + birth + "]"; &#125;&#125; 执行持久化操作流程 123456789101112131415161718192021222324252627282930313233343536//1. 创建 EntitymanagerFactoryString persistenceUnitName = "jpa-1";//可选//Map&lt;String, Object&gt; properites = new HashMap&lt;String, Object&gt;();//properites.put("hibernate.show_sql", true);EntityManagerFactory entityManagerFactory = Persistence.createEntityManagerFactory(persistenceUnitName); //Persistence.createEntityManagerFactory(persistenceUnitName, properites); //2. 创建 EntityManager. 类似于 Hibernate 的 SessionFactoryEntityManager entityManager = entityManagerFactory.createEntityManager();//3. 开启事务EntityTransaction transaction = entityManager.getTransaction();transaction.begin();//4. 进行持久化操作Customer customer = new Customer();customer.setAge(12);customer.setEmail("tom@atguigu.com");customer.setLastName("Tom");customer.setBirth(new Date());customer.setCreatedTime(new Date());entityManager.persist(customer);//5. 提交事务transaction.commit(); //6. 关闭 EntityManagerentityManager.close();//7. 关闭 EntityManagerFactoryentityManagerFactory.close(); 基本注解 @Entity标注用于实体类声明语句之前，指出该Java类为实体类，将映射到指定的数据库表。如声明一个实体类Customer，它将映射到数据库中的customer表上。 @Table 当实体类与其映射的数据库表名不同名时需要使用 @Table 标注说明，该标注与 @Entity 标注并列使用，置于实体类声明语句之前，可写于单独语句行，也可与声明语句同行。 @Table 标注的常用选项是 name，用于指明数据库的表名 @Table标注还有一个两个选项 catalog 和 schema 用于设置表所属的数据库目录或模式，通常为数据库名。uniqueConstraints选项用于设置约束条件，通常不须设置。 @Id @Id标注用于声明一个实体类的属性映射为数据库的主键列。该属性通常置于属性声明语句之前，可与声明语句同行，也可写在单独行上。 •@Id标注也可置于属性的getter方法之前。 @GeneratedValue @GeneratedValue 用于标注主键的生成策略，通过 strategy属性指定。默认情况下，JPA自动选择一个最适合底层数据库的主键生成策略：SqlServer对应identity，MySQL对应auto increment。 在 javax.persistence.GenerationType 中定义了以下几种可供选择的策略： –IDENTITY：采用数据库 ID自增长的方式来自增主键字段，Oracle 不支持这种方式； –AUTO： JPA自动选择合适的策略，是默认选项； –SEQUENCE：通过序列产生主键，通过 @SequenceGenerator 注解指定序列名，MySql 不支持这种方式 –TABLE：通过表产生主键，框架借由表模拟序列产生主键，使用该策略可以使应用更易于数据库移植。 @Basic @Basic 表示一个简单的属性到数据库表的字段的映射,对于没有任何标注的 getXxxx() 方法,默认即为@Basic fetch: 表示该属性的读取策略,有EAGER 和 LAZY两种,分别表示主支抓取和延迟加载,默认为EAGER. optional:表示该属性是否允许为null,默认为true @Column 当实体的属性与其映射的数据库表的列不同名时需要使用@Column标注说明，该属性通常置于实体的属性声明语句之前，还可与@Id标注一起使用。 @Column标注的常用属性是name，用于设置映射数据库表的列名。此外，该标注还包含其它多个属性，如：unique 、nullable、length 等。 @Column标注的columnDefinition属性:表示该字段在数据库中的实际类型.通常ORM框架可以根据属性类型自动判断数据库中字段的类型,但是对于Date类型仍无法确定数据库中字段类型究竟是DATE,TIME还是TIMESTAMP.此外,String的默认映射类型为VARCHAR,如果要将String类型映射到特定数据库的BLOB或TEXT字段类型。 @Column标注也可置于属性的getter方法之前。 @Transient 表示该属性并非一个到数据库表的字段的映射,ORM框架将忽略该属性. 如果一个属性并非数据库表的字段映射,就务必将其标示为@Transient,否则,ORM框架默认其注解为@Basic @Temporal 在核心的JavaAPI 中并没有定义Date类型的精度(temporalprecision). 而在数据库中,表示Date类型的数据有DATE,TIME, 和 TIMESTAMP三种精度(即单纯的日期,时间,或者两者兼备). 在进行属性映射时可使用@Temporal注解来调整精度。 JPA API 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527public class JPATest &#123; private EntityManagerFactory entityManagerFactory; private EntityManager entityManager; private EntityTransaction transaction; @Before public void init()&#123; entityManagerFactory = Persistence.createEntityManagerFactory("jpa-1"); entityManager = entityManagerFactory.createEntityManager(); transaction = entityManager.getTransaction(); transaction.begin(); &#125; @After public void destroy()&#123; transaction.commit(); entityManager.close(); entityManagerFactory.close(); &#125; //可以使用 JPQL 完成 UPDATE 和 DELETE 操作. @Test public void testExecuteUpdate()&#123; String jpql = "UPDATE Customer c SET c.lastName = ? WHERE c.id = ?"; Query query = entityManager.createQuery(jpql).setParameter(1, "YYY").setParameter(2, 12); query.executeUpdate(); &#125; //使用 jpql 内建的函数 @Test public void testJpqlFunction()&#123; String jpql = "SELECT lower(c.email) FROM Customer c"; List&lt;String&gt; emails = entityManager.createQuery(jpql).getResultList(); System.out.println(emails); &#125; @Test public void testSubQuery()&#123; //查询所有 Customer 的 lastName 为 YY 的 Order String jpql = "SELECT o FROM Order o " + "WHERE o.customer = (SELECT c FROM Customer c WHERE c.lastName = ?)"; Query query = entityManager.createQuery(jpql).setParameter(1, "YY"); List&lt;Order&gt; orders = query.getResultList(); System.out.println(orders.size()); &#125; /** * JPQL 的关联查询同 HQL 的关联查询. */ @Test public void testLeftOuterJoinFetch()&#123; String jpql = "FROM Customer c LEFT OUTER JOIN FETCH c.orders WHERE c.id = ?"; Customer customer = (Customer) entityManager.createQuery(jpql).setParameter(1, 12).getSingleResult(); System.out.println(customer.getLastName()); System.out.println(customer.getOrders().size()); // List&lt;Object[]&gt; result = entityManager.createQuery(jpql).setParameter(1, 12).getResultList();// System.out.println(result); &#125; //查询 order 数量大于 2 的那些 Customer @Test public void testGroupBy()&#123; String jpql = "SELECT o.customer FROM Order o " + "GROUP BY o.customer " + "HAVING count(o.id) &gt;= 2"; List&lt;Customer&gt; customers = entityManager.createQuery(jpql).getResultList(); System.out.println(customers); &#125; @Test public void testOrderBy()&#123; String jpql = "FROM Customer c WHERE c.age &gt; ? ORDER BY c.age DESC"; Query query = entityManager.createQuery(jpql).setHint(QueryHints.HINT_CACHEABLE, true); //占位符的索引是从 1 开始 query.setParameter(1, 1); List&lt;Customer&gt; customers = query.getResultList(); System.out.println(customers.size()); &#125; //使用 hibernate 的查询缓存. @Test public void testQueryCache()&#123; String jpql = "FROM Customer c WHERE c.age &gt; ?"; Query query = entityManager.createQuery(jpql).setHint(QueryHints.HINT_CACHEABLE, true); //占位符的索引是从 1 开始 query.setParameter(1, 1); List&lt;Customer&gt; customers = query.getResultList(); System.out.println(customers.size()); query = entityManager.createQuery(jpql).setHint(QueryHints.HINT_CACHEABLE, true); //占位符的索引是从 1 开始 query.setParameter(1, 1); customers = query.getResultList(); System.out.println(customers.size()); &#125; //createNativeQuery 适用于本地 SQL @Test public void testNativeQuery()&#123; String sql = "SELECT age FROM jpa_cutomers WHERE id = ?"; Query query = entityManager.createNativeQuery(sql).setParameter(1, 3); Object result = query.getSingleResult(); System.out.println(result); &#125; //createNamedQuery 适用于在实体类前使用 @NamedQuery 标记的查询语句 @Test public void testNamedQuery()&#123; Query query = entityManager.createNamedQuery("testNamedQuery").setParameter(1, 3); Customer customer = (Customer) query.getSingleResult(); System.out.println(customer); &#125; //默认情况下, 若只查询部分属性, 则将返回 Object[] 类型的结果. 或者 Object[] 类型的 List. //也可以在实体类中创建对应的构造器, 然后再 JPQL 语句中利用对应的构造器返回实体类的对象. @Test public void testPartlyProperties()&#123; String jpql = "SELECT new Customer(c.lastName, c.age) FROM Customer c WHERE c.id &gt; ?"; List result = entityManager.createQuery(jpql).setParameter(1, 1).getResultList(); System.out.println(result); &#125; @Test public void testHelloJPQL()&#123; String jpql = "FROM Customer c WHERE c.age &gt; ?"; Query query = entityManager.createQuery(jpql); //占位符的索引是从 1 开始 query.setParameter(1, 1); List&lt;Customer&gt; customers = query.getResultList(); System.out.println(customers.size()); &#125; @Test public void testSecondLevelCache()&#123; Customer customer1 = entityManager.find(Customer.class, 1); transaction.commit(); entityManager.close(); entityManager = entityManagerFactory.createEntityManager(); transaction = entityManager.getTransaction(); transaction.begin(); Customer customer2 = entityManager.find(Customer.class, 1); &#125; //对于关联的集合对象, 默认使用懒加载的策略. //使用维护关联关系的一方获取, 还是使用不维护关联关系的一方获取, SQL 语句相同. @Test public void testManyToManyFind()&#123;// Item item = entityManager.find(Item.class, 5);// System.out.println(item.getItemName());// // System.out.println(item.getCategories().size()); Category category = entityManager.find(Category.class, 3); System.out.println(category.getCategoryName()); System.out.println(category.getItems().size()); &#125; //多对所的保存 @Test public void testManyToManyPersist()&#123; Item i1 = new Item(); i1.setItemName("i-1"); Item i2 = new Item(); i2.setItemName("i-2"); Category c1 = new Category(); c1.setCategoryName("C-1"); Category c2 = new Category(); c2.setCategoryName("C-2"); //设置关联关系 i1.getCategories().add(c1); i1.getCategories().add(c2); i2.getCategories().add(c1); i2.getCategories().add(c2); c1.getItems().add(i1); c1.getItems().add(i2); c2.getItems().add(i1); c2.getItems().add(i2); //执行保存 entityManager.persist(i1); entityManager.persist(i2); entityManager.persist(c1); entityManager.persist(c2); &#125; //1. 默认情况下, 若获取不维护关联关系的一方, 则也会通过左外连接获取其关联的对象. //可以通过 @OneToOne 的 fetch 属性来修改加载策略. 但依然会再发送 SQL 语句来初始化其关联的对象 //这说明在不维护关联关系的一方, 不建议修改 fetch 属性. @Test public void testOneToOneFind2()&#123; Manager mgr = entityManager.find(Manager.class, 1); System.out.println(mgr.getMgrName()); System.out.println(mgr.getDept().getClass().getName()); &#125; //1.默认情况下, 若获取维护关联关系的一方, 则会通过左外连接获取其关联的对象. //但可以通过 @OntToOne 的 fetch 属性来修改加载策略. @Test public void testOneToOneFind()&#123; Department dept = entityManager.find(Department.class, 1); System.out.println(dept.getDeptName()); System.out.println(dept.getMgr().getClass().getName()); &#125; //双向 1-1 的关联关系, 建议先保存不维护关联关系的一方, 即没有外键的一方, 这样不会多出 UPDATE 语句. @Test public void testOneToOnePersistence()&#123; Manager mgr = new Manager(); mgr.setMgrName("M-BB"); Department dept = new Department(); dept.setDeptName("D-BB"); //设置关联关系 mgr.setDept(dept); dept.setMgr(mgr); //执行保存操作 entityManager.persist(mgr); entityManager.persist(dept); &#125; @Test public void testUpdate()&#123; Customer customer = entityManager.find(Customer.class, 10); customer.getOrders().iterator().next().setOrderName("O-XXX-10"); &#125; //默认情况下, 若删除 1 的一端, 则会先把关联的 n 的一端的外键置空, 然后进行删除. //可以通过 @OneToMany 的 cascade 属性来修改默认的删除策略. @Test public void testOneToManyRemove()&#123; Customer customer = entityManager.find(Customer.class, 8); entityManager.remove(customer); &#125; //默认对关联的多的一方使用懒加载的加载策略. //可以使用 @OneToMany 的 fetch 属性来修改默认的加载策略 @Test public void testOneToManyFind()&#123; Customer customer = entityManager.find(Customer.class, 9); System.out.println(customer.getLastName()); System.out.println(customer.getOrders().size()); &#125; //若是双向 1-n 的关联关系, 执行保存时 //若先保存 n 的一端, 再保存 1 的一端, 默认情况下, 会多出 n 条 UPDATE 语句. //若先保存 1 的一端, 则会多出 n 条 UPDATE 语句 //在进行双向 1-n 关联关系时, 建议使用 n 的一方来维护关联关系, 而 1 的一方不维护关联系, 这样会有效的减少 SQL 语句. //注意: 若在 1 的一端的 @OneToMany 中使用 mappedBy 属性, 则 @OneToMany 端就不能再使用 @JoinColumn 属性了. //单向 1-n 关联关系执行保存时, 一定会多出 UPDATE 语句. //因为 n 的一端在插入时不会同时插入外键列. @Test public void testOneToManyPersist()&#123; Customer customer = new Customer(); customer.setAge(18); customer.setBirth(new Date()); customer.setCreatedTime(new Date()); customer.setEmail("mm@163.com"); customer.setLastName("MM"); Order order1 = new Order(); order1.setOrderName("O-MM-1"); Order order2 = new Order(); order2.setOrderName("O-MM-2"); //建立关联关系 customer.getOrders().add(order1); customer.getOrders().add(order2); order1.setCustomer(customer); order2.setCustomer(customer); //执行保存操作 entityManager.persist(customer); entityManager.persist(order1); entityManager.persist(order2); &#125; /* @Test public void testManyToOneUpdate()&#123; Order order = entityManager.find(Order.class, 2); order.getCustomer().setLastName("FFF"); &#125; //不能直接删除 1 的一端, 因为有外键约束. @Test public void testManyToOneRemove()&#123;// Order order = entityManager.find(Order.class, 1);// entityManager.remove(order); Customer customer = entityManager.find(Customer.class, 7); entityManager.remove(customer); &#125; //默认情况下, 使用左外连接的方式来获取 n 的一端的对象和其关联的 1 的一端的对象. //可使用 @ManyToOne 的 fetch 属性来修改默认的关联属性的加载策略 @Test public void testManyToOneFind()&#123; Order order = entityManager.find(Order.class, 1); System.out.println(order.getOrderName()); System.out.println(order.getCustomer().getLastName()); &#125; */ /** * 保存多对一时, 建议先保存 1 的一端, 后保存 n 的一端, 这样不会多出额外的 UPDATE 语句. */ /* @Test public void testManyToOnePersist()&#123; Customer customer = new Customer(); customer.setAge(18); customer.setBirth(new Date()); customer.setCreatedTime(new Date()); customer.setEmail("gg@163.com"); customer.setLastName("GG"); Order order1 = new Order(); order1.setOrderName("G-GG-1"); Order order2 = new Order(); order2.setOrderName("G-GG-2"); //设置关联关系 order1.setCustomer(customer); order2.setCustomer(customer); //执行保存操作 entityManager.persist(order1); entityManager.persist(order2); entityManager.persist(customer); &#125; */ /** * 同 hibernate 中 Session 的 refresh 方法. */ @Test public void testRefresh()&#123; Customer customer = entityManager.find(Customer.class, 1); customer = entityManager.find(Customer.class, 1); entityManager.refresh(customer); &#125; /** * 同 hibernate 中 Session 的 flush 方法. */ @Test public void testFlush()&#123; Customer customer = entityManager.find(Customer.class, 1); System.out.println(customer); customer.setLastName("AA"); entityManager.flush(); &#125; //若传入的是一个游离对象, 即传入的对象有 OID. //1. 若在 EntityManager 缓存中有对应的对象 //2. JPA 会把游离对象的属性复制到查询到EntityManager 缓存中的对象中. //3. EntityManager 缓存中的对象执行 UPDATE. @Test public void testMerge4()&#123; Customer customer = new Customer(); customer.setAge(18); customer.setBirth(new Date()); customer.setCreatedTime(new Date()); customer.setEmail("dd@163.com"); customer.setLastName("DD"); customer.setId(4); Customer customer2 = entityManager.find(Customer.class, 4); entityManager.merge(customer); System.out.println(customer == customer2); //false &#125; //若传入的是一个游离对象, 即传入的对象有 OID. //1. 若在 EntityManager 缓存中没有该对象 //2. 若在数据库中也有对应的记录 //3. JPA 会查询对应的记录, 然后返回该记录对一个的对象, 再然后会把游离对象的属性复制到查询到的对象中. //4. 对查询到的对象执行 update 操作. @Test public void testMerge3()&#123; Customer customer = new Customer(); customer.setAge(18); customer.setBirth(new Date()); customer.setCreatedTime(new Date()); customer.setEmail("ee@163.com"); customer.setLastName("EE"); customer.setId(4); Customer customer2 = entityManager.merge(customer); System.out.println(customer == customer2); //false &#125; //若传入的是一个游离对象, 即传入的对象有 OID. //1. 若在 EntityManager 缓存中没有该对象 //2. 若在数据库中也没有对应的记录 //3. JPA 会创建一个新的对象, 然后把当前游离对象的属性复制到新创建的对象中 //4. 对新创建的对象执行 insert 操作. @Test public void testMerge2()&#123; Customer customer = new Customer(); customer.setAge(18); customer.setBirth(new Date()); customer.setCreatedTime(new Date()); customer.setEmail("dd@163.com"); customer.setLastName("DD"); customer.setId(100); Customer customer2 = entityManager.merge(customer); System.out.println("customer#id:" + customer.getId()); System.out.println("customer2#id:" + customer2.getId()); &#125; /** * 总的来说: 类似于 hibernate Session 的 saveOrUpdate 方法. */ //1. 若传入的是一个临时对象 //会创建一个新的对象, 把临时对象的属性复制到新的对象中, 然后对新的对象执行持久化操作. 所以 //新的对象中有 id, 但以前的临时对象中没有 id. @Test public void testMerge1()&#123; Customer customer = new Customer(); customer.setAge(18); customer.setBirth(new Date()); customer.setCreatedTime(new Date()); customer.setEmail("cc@163.com"); customer.setLastName("CC"); Customer customer2 = entityManager.merge(customer); System.out.println("customer#id:" + customer.getId()); System.out.println("customer2#id:" + customer2.getId()); &#125; //类似于 hibernate 中 Session 的 delete 方法. 把对象对应的记录从数据库中移除 //但注意: 该方法只能移除 持久化 对象. 而 hibernate 的 delete 方法实际上还可以移除 游离对象. @Test public void testRemove()&#123;// Customer customer = new Customer();// customer.setId(2); Customer customer = entityManager.find(Customer.class, 2); entityManager.remove(customer); &#125; //类似于 hibernate 的 save 方法. 使对象由临时状态变为持久化状态. //和 hibernate 的 save 方法的不同之处: 若对象有 id, 则不能执行 insert 操作, 而会抛出异常. @Test public void testPersistence()&#123; Customer customer = new Customer(); customer.setAge(15); customer.setBirth(new Date()); customer.setCreatedTime(new Date()); customer.setEmail("bb@163.com"); customer.setLastName("BB"); customer.setId(100); entityManager.persist(customer); System.out.println(customer.getId()); &#125; //类似于 hibernate 中 Session 的 load 方法 @Test public void testGetReference()&#123; Customer customer = entityManager.getReference(Customer.class, 1); System.out.println(customer.getClass().getName()); System.out.println("-------------------------------------");// transaction.commit();// entityManager.close(); System.out.println(customer); &#125; //类似于 hibernate 中 Session 的 get 方法. @Test public void testFind() &#123; Customer customer = entityManager.find(Customer.class, 1); System.out.println("-------------------------------------"); System.out.println(customer); &#125;&#125; 单向多对一映射 12345678910111213141516171819202122232425262728293031323334353637383940414243@Table(name="JPA_ORDERS")@Entitypublic class Order &#123; private Integer id; private String orderName; private Customer customer; @GeneratedValue @Id public Integer getId() &#123; return id; &#125; public void setId(Integer id) &#123; this.id = id; &#125; @Column(name="ORDER_NAME") public String getOrderName() &#123; return orderName; &#125; public void setOrderName(String orderName) &#123; this.orderName = orderName; &#125; //映射单向 n-1 的关联关系 //使用 @ManyToOne 来映射多对一的关联关系 //使用 @JoinColumn 来映射外键. //可使用 @ManyToOne 的 fetch 属性来修改默认的关联属性的加载策略 @JoinColumn(name="CUSTOMER_ID") @ManyToOne(fetch=FetchType.LAZY) public Customer getCustomer() &#123; return customer; &#125; public void setCustomer(Customer customer) &#123; this.customer = customer; &#125;&#125; 单向一对多 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118@NamedQuery(name="testNamedQuery", query="FROM Customer c WHERE c.id = ?")@Cacheable(true)@Table(name="JPA_CUTOMERS")@Entitypublic class Customer &#123; private Integer id; private String lastName; private String email; private int age; private Date createdTime; private Date birth; public Customer() &#123; // TODO Auto-generated constructor stub &#125; public Customer(String lastName, int age) &#123; super(); this.lastName = lastName; this.age = age; &#125; private Set&lt;Order&gt; orders = new HashSet&lt;&gt;();// @TableGenerator(name="ID_GENERATOR",// table="jpa_id_generators",// pkColumnName="PK_NAME",// pkColumnValue="CUSTOMER_ID",// valueColumnName="PK_VALUE",// allocationSize=100)// @GeneratedValue(strategy=GenerationType.TABLE,generator="ID_GENERATOR") @GeneratedValue(strategy=GenerationType.AUTO) @Id public Integer getId() &#123; return id; &#125; public void setId(Integer id) &#123; this.id = id; &#125; @Column(name="LAST_NAME",length=50,nullable=false) public String getLastName() &#123; return lastName; &#125; public void setLastName(String lastName) &#123; this.lastName = lastName; &#125; public String getEmail() &#123; return email; &#125; public void setEmail(String email) &#123; this.email = email; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; @Temporal(TemporalType.TIMESTAMP) public Date getCreatedTime() &#123; return createdTime; &#125; public void setCreatedTime(Date createdTime) &#123; this.createdTime = createdTime; &#125; @Temporal(TemporalType.DATE) public Date getBirth() &#123; return birth; &#125; public void setBirth(Date birth) &#123; this.birth = birth; &#125; //映射单向 1-n 的关联关系 //使用 @OneToMany 来映射 1-n 的关联关系 //使用 @JoinColumn 来映射外键列的名称 //可以使用 @OneToMany 的 fetch 属性来修改默认的加载策略 //可以通过 @OneToMany 的 cascade 属性来修改默认的删除策略. //注意: 若在 1 的一端的 @OneToMany 中使用 mappedBy(类似inverse,由多的一方来维护关联关系) 属性, 则 @OneToMany 端就不能再使用 @JoinColumn 属性了. // @JoinColumn(name="CUSTOMER_ID") @OneToMany(fetch=FetchType.LAZY,cascade=&#123;CascadeType.REMOVE&#125;,mappedBy="customer") public Set&lt;Order&gt; getOrders() &#123; return orders; &#125; public void setOrders(Set&lt;Order&gt; orders) &#123; this.orders = orders; &#125; //工具方法. 不需要映射为数据表的一列. @Transient public String getInfo()&#123; return "lastName: " + lastName + ", email: " + email; &#125; @Override public String toString() &#123; return "Customer [id=" + id + ", lastName=" + lastName + ", email=" + email + ", age=" + age + ", createdTime=" + createdTime + ", birth=" + birth + "]"; &#125;&#125; 双向一对多(多对一) 单向一对多和单向多对一的组合(注意使用mappedBy提高性能，防止执行多余的update语句)。 双向一对一 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879@Table(name="JPA_DEPARTMENTS")@Entitypublic class Department &#123; private Integer id; private String deptName; private Manager mgr; @GeneratedValue @Id public Integer getId() &#123; return id; &#125; public void setId(Integer id) &#123; this.id = id; &#125; @Column(name="DEPT_NAME") public String getDeptName() &#123; return deptName; &#125; public void setDeptName(String deptName) &#123; this.deptName = deptName; &#125; //使用 @OneToOne 来映射 1-1 关联关系。 //若需要在当前数据表中添加外键则需要使用 @JoinColumn 来进行映射. 注意, 1-1 关联关系, 所以需要添加 unique=true @JoinColumn(name="MGR_ID", unique=true) @OneToOne(fetch=FetchType.LAZY) public Manager getMgr() &#123; return mgr; &#125; public void setMgr(Manager mgr) &#123; this.mgr = mgr; &#125;&#125;====================================================@Table(name="JPA_MANAGERS")@Entitypublic class Manager &#123; private Integer id; private String mgrName; private Department dept; @GeneratedValue @Id public Integer getId() &#123; return id; &#125; public void setId(Integer id) &#123; this.id = id; &#125; @Column(name="MGR_NAME") public String getMgrName() &#123; return mgrName; &#125; public void setMgrName(String mgrName) &#123; this.mgrName = mgrName; &#125; //对于不维护关联关系, 没有外键的一方, 使用 @OneToOne 来进行映射, 建议设置 mappedBy=true @OneToOne(mappedBy="mgr") public Department getDept() &#123; return dept; &#125; public void setDept(Department dept) &#123; this.dept = dept; &#125;&#125; 双向多对多 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485@Table(name="JPA_CATEGORIES")@Entitypublic class Category &#123; private Integer id; private String categoryName; private Set&lt;Item&gt; items = new HashSet&lt;&gt;(); @GeneratedValue @Id public Integer getId() &#123; return id; &#125; public void setId(Integer id) &#123; this.id = id; &#125; @Column(name="CATEGORY_NAME") public String getCategoryName() &#123; return categoryName; &#125; public void setCategoryName(String categoryName) &#123; this.categoryName = categoryName; &#125; @ManyToMany(mappedBy="categories") public Set&lt;Item&gt; getItems() &#123; return items; &#125; public void setItems(Set&lt;Item&gt; items) &#123; this.items = items; &#125;&#125;================================================@Table(name="JPA_ITEMS")@Entitypublic class Item &#123; private Integer id; private String itemName; private Set&lt;Category&gt; categories = new HashSet&lt;&gt;(); @GeneratedValue @Id public Integer getId() &#123; return id; &#125; public void setId(Integer id) &#123; this.id = id; &#125; @Column(name="ITEM_NAME") public String getItemName() &#123; return itemName; &#125; public void setItemName(String itemName) &#123; this.itemName = itemName; &#125; //使用 @ManyToMany 注解来映射多对多关联关系 //使用 @JoinTable 来映射中间表 //1. name 指向中间表的名字 //2. joinColumns 映射当前类所在的表在中间表中的外键 //2.1 name 指定外键列的列名 //2.2 referencedColumnName 指定外键列关联当前表的哪一列 //3. inverseJoinColumns 映射关联的类所在中间表的外键 @JoinTable(name="ITEM_CATEGORY", joinColumns=&#123;@JoinColumn(name="ITEM_ID", referencedColumnName="ID")&#125;, inverseJoinColumns=&#123;@JoinColumn(name="CATEGORY_ID", referencedColumnName="ID")&#125;) @ManyToMany public Set&lt;Category&gt; getCategories() &#123; return categories; &#125; public void setCategories(Set&lt;Category&gt; categories) &#123; this.categories = categories; &#125;&#125;]]></content>
  </entry>
  <entry>
    <title><![CDATA[hibernate映射关系]]></title>
    <url>%2F2018%2F12%2F11%2Fhibernate%E6%98%A0%E5%B0%84%E5%85%B3%E7%B3%BB%2F</url>
    <content type="text"><![CDATA[java代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class Customer &#123; private Integer customerId; private String customerName; public Integer getCustomerId() &#123; return customerId; &#125; public void setCustomerId(Integer customerId) &#123; this.customerId = customerId; &#125; public String getCustomerName() &#123; return customerName; &#125; public void setCustomerName(String customerName) &#123; this.customerName = customerName; &#125;&#125;public class Order &#123; private Integer orderId; private String orderName; private Customer customer; public Integer getOrderId() &#123; return orderId; &#125; public void setOrderId(Integer orderId) &#123; this.orderId = orderId; &#125; public String getOrderName() &#123; return orderName; &#125; public void setOrderName(String orderName) &#123; this.orderName = orderName; &#125; public Customer getCustomer() &#123; return customer; &#125; public void setCustomer(Customer customer) &#123; this.customer = customer; &#125;&#125; 单向一对多1234567891011121314151617181920&lt;hibernate-mapping package="com.atguigu.hibernate.entities.n21"&gt; &lt;class name="Order" table="ORDERS"&gt; &lt;id name="orderId" type="java.lang.Integer"&gt; &lt;column name="ORDER_ID" /&gt; &lt;generator class="native" /&gt; &lt;/id&gt; &lt;property name="orderName" type="java.lang.String"&gt; &lt;column name="ORDER_NAME" /&gt; &lt;/property&gt; &lt;!-- 映射多对一的关联关系。 使用 many-to-one 来映射多对一的关联关系 name: 多这一端关联的一那一端的属性的名字 class: 一那一端的属性对应的类名 column: 一那一端在多的一端对应的数据表中的外键的名字 --&gt; &lt;many-to-one name="customer" class="Customer" column="CUSTOMER_ID"&gt;&lt;/many-to-one&gt; &lt;/class&gt;&lt;/hibernate-mapping&gt; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465@Testpublic void testMany2OneSave()&#123; Customer customer = new Customer(); customer.setCustomerName("BB"); Order order1 = new Order(); order1.setOrderName("ORDER-3"); Order order2 = new Order(); order2.setOrderName("ORDER-4"); //设定关联关系 order1.setCustomer(customer); order2.setCustomer(customer); //执行 save 操作: 先插入 Customer, 再插入 Order, 3 条 INSERT //先插入 1 的一端, 再插入 n 的一端, 只有 INSERT 语句. //session.save(customer); //session.save(order1); //session.save(order2); //先插入 Order, 再插入 Customer. 3 条 INSERT, 2 条 UPDATE //先插入 n 的一端, 再插入 1 的一端, 会多出 UPDATE 语句! //因为在插入多的一端时, 无法确定 1 的一端的外键值. 所以只能等 1 的一端插入后, 再额外发送 UPDATE 语句. //推荐先插入 1 的一端, 后插入 n 的一端 session.save(order1); session.save(order2); session.save(customer);&#125; @Testpublic void testMany2OneGet()&#123; //1. 若查询多的一端的一个对象, 则默认情况下, 只查询了多的一端的对象. 而没有查询关联的 //1 的那一端的对象! Order order = (Order) session.get(Order.class, 1); System.out.println(order.getOrderName()); System.out.println(order.getCustomer().getClass().getName()); session.close(); //2. 在需要使用到关联的对象时, 才发送对应的 SQL 语句. Customer customer = order.getCustomer(); System.out.println(customer.getCustomerName()); //3. 在查询 Customer 对象时, 由多的一端导航到 1 的一端时, //若此时 session 已被关闭, 则默认情况下 //会发生 LazyInitializationException 异常 //4. 获取 Order 对象时, 默认情况下, 其关联的 Customer 对象是一个代理对象!&#125;@Testpublic void testDelete()&#123; //在不设定级联关系的情况下, 且 1 这一端的对象有 n 的对象在引用, 不能直接删除 1 这一端的对象 Customer customer = (Customer) session.get(Customer.class, 1); session.delete(customer); &#125;@Testpublic void testUpdate()&#123; Order order = (Order) session.get(Order.class, 1); order.getCustomer().setCustomerName("AAA");&#125; 双向一对多 java代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869public class Customer &#123; private Integer customerId; private String customerName; /* * 1. 声明集合类型时, 需使用接口类型, 因为 hibernate 在获取 * 集合类型时, 返回的是 Hibernate 内置的集合类型, 而不是 JavaSE 一个标准的 * 集合实现. * 2. 需要把集合进行初始化, 可以防止发生空指针异常 */ private Set&lt;Order&gt; orders = new HashSet&lt;&gt;(); public Integer getCustomerId() &#123; return customerId; &#125; public void setCustomerId(Integer customerId) &#123; this.customerId = customerId; &#125; public String getCustomerName() &#123; return customerName; &#125; public void setCustomerName(String customerName) &#123; this.customerName = customerName; &#125; public Set&lt;Order&gt; getOrders() &#123; return orders; &#125; public void setOrders(Set&lt;Order&gt; orders) &#123; this.orders = orders; &#125; &#125;public class Order &#123; private Integer orderId; private String orderName; private Customer customer; public Integer getOrderId() &#123; return orderId; &#125; public void setOrderId(Integer orderId) &#123; this.orderId = orderId; &#125; public String getOrderName() &#123; return orderName; &#125; public void setOrderName(String orderName) &#123; this.orderName = orderName; &#125; public Customer getCustomer() &#123; return customer; &#125; public void setCustomer(Customer customer) &#123; this.customer = customer; &#125; &#125; 映射配置(多的一端配置与单向相同) 12345678910111213141516171819202122232425262728&lt;hibernate-mapping package="com.atguigu.hibernate.entities.n21.both"&gt; &lt;class name="Customer" table="CUSTOMERS"&gt; &lt;id name="customerId" type="java.lang.Integer"&gt; &lt;column name="CUSTOMER_ID" /&gt; &lt;generator class="native" /&gt; &lt;/id&gt; &lt;property name="customerName" type="java.lang.String"&gt; &lt;column name="CUSTOMER_NAME" /&gt; &lt;/property&gt; &lt;!-- 映射 1 对多的那个集合属性 --&gt; &lt;!-- set: 映射 set 类型的属性, table: set 中的元素对应的记录放在哪一个数据表中. 该值需要和多对一的多的那个表的名字一致 --&gt; &lt;!-- inverse: 指定由哪一方来维护关联关系. 通常设置为 true, 以指定由多的一端来维护关联关系 --&gt; &lt;!-- cascade 设定级联操作. 开发时不建议设定该属性. 建议使用手工的方式来处理 --&gt; &lt;!-- order-by 在查询时对集合中的元素进行排序, order-by 中使用的是表的字段名, 而不是持久化类的属性名 --&gt; &lt;set name="orders" table="ORDERS" inverse="true" order-by="ORDER_NAME DESC"&gt; &lt;!-- 执行多的表中的外键列的名字 --&gt; &lt;key column="CUSTOMER_ID"&gt;&lt;/key&gt; &lt;!-- 指定映射类型 --&gt; &lt;one-to-many class="Order"/&gt; &lt;/set&gt; &lt;/class&gt; &lt;/hibernate-mapping&gt; 测试 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798@Testpublic void testCascade()&#123; Customer customer = (Customer) session.get(Customer.class, 3); customer.getOrders().clear();&#125;@Testpublic void testDelete()&#123; //在不设定级联关系的情况下, 且 1 这一端的对象有 n 的对象在引用, 不能直接删除 1 这一端的对象 Customer customer = (Customer) session.get(Customer.class, 1); session.delete(customer); &#125;@Testpublic void testUpdat2()&#123; Customer customer = (Customer) session.get(Customer.class, 1); customer.getOrders().iterator().next().setOrderName("GGG"); &#125;@Testpublic void testUpdate()&#123; Order order = (Order) session.get(Order.class, 1); order.getCustomer().setCustomerName("AAA");&#125;@Testpublic void testOne2ManyGet()&#123; //1. 对 n 的一端的集合使用延迟加载 Customer customer = (Customer) session.get(Customer.class, 7); System.out.println(customer.getCustomerName()); //2. 返回的多的一端的集合时 Hibernate 内置的集合类型. //该类型具有延迟加载和存放代理对象的功能. System.out.println(customer.getOrders().getClass()); //session.close(); //3. 可能会抛出 LazyInitializationException 异常 System.out.println(customer.getOrders().size()); //4. 再需要使用集合中元素的时候进行初始化. &#125;@Testpublic void testMany2OneGet()&#123; //1. 若查询多的一端的一个对象, 则默认情况下, 只查询了多的一端的对象. 而没有查询关联的 //1 的那一端的对象! Order order = (Order) session.get(Order.class, 1); System.out.println(order.getOrderName()); System.out.println(order.getCustomer().getClass().getName()); session.close(); //2. 在需要使用到关联的对象时, 才发送对应的 SQL 语句. Customer customer = order.getCustomer(); System.out.println(customer.getCustomerName()); //3. 在查询 Customer 对象时, 由多的一端导航到 1 的一端时, //若此时 session 已被关闭, 则默认情况下 //会发生 LazyInitializationException 异常 //4. 获取 Order 对象时, 默认情况下, 其关联的 Customer 对象是一个代理对象! &#125;@Testpublic void testMany2OneSave()&#123; Customer customer = new Customer(); customer.setCustomerName("AA"); Order order1 = new Order(); order1.setOrderName("ORDER-1"); Order order2 = new Order(); order2.setOrderName("ORDER-2"); //设定关联关系 order1.setCustomer(customer); order2.setCustomer(customer); customer.getOrders().add(order1); customer.getOrders().add(order2); //执行 save 操作: 先插入 Customer, 再插入 Order, 3 条 INSERT, 2 条 UPDATE //因为 1 的一端和 n 的一端都维护关联关系. 所以会多出 UPDATE //可以在 1 的一端的 set 节点指定 inverse=true, 来使 1 的一端放弃维护关联关系! //建议设定 set 的 inverse=true, 建议先插入 1 的一端, 后插入多的一端 //好处是不会多出 UPDATE 语句 session.save(customer); //session.save(order1); //session.save(order2); //先插入 Order, 再插入 Cusomer, 3 条 INSERT, 4 条 UPDATE //session.save(order1); //session.save(order2); //session.save(customer);&#125; 一对一(外键) 实体类 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364public class Department &#123; private Integer deptId; private String deptName; private Manager mgr; public Integer getDeptId() &#123; return deptId; &#125; public void setDeptId(Integer deptId) &#123; this.deptId = deptId; &#125; public String getDeptName() &#123; return deptName; &#125; public void setDeptName(String deptName) &#123; this.deptName = deptName; &#125; public Manager getMgr() &#123; return mgr; &#125; public void setMgr(Manager mgr) &#123; this.mgr = mgr; &#125;&#125;public class Manager &#123; private Integer mgrId; private String mgrName; private Department dept; public Integer getMgrId() &#123; return mgrId; &#125; public void setMgrId(Integer mgrId) &#123; this.mgrId = mgrId; &#125; public String getMgrName() &#123; return mgrName; &#125; public void setMgrName(String mgrName) &#123; this.mgrName = mgrName; &#125; public Department getDept() &#123; return dept; &#125; public void setDept(Department dept) &#123; this.dept = dept; &#125;&#125; 映射 1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;hibernate-mapping&gt; &lt;class name="com.atguigu.hibernate.one2one.foreign.Manager" table="MANAGERS"&gt; &lt;id name="mgrId" type="java.lang.Integer"&gt; &lt;column name="MGR_ID" /&gt; &lt;generator class="native" /&gt; &lt;/id&gt; &lt;property name="mgrName" type="java.lang.String"&gt; &lt;column name="MGR_NAME" /&gt; &lt;/property&gt; &lt;!-- 映射 1-1 的关联关系: 在对应的数据表中已经有外键了, 当前持久化类使用 one-to-one 进行映射 --&gt; &lt;!-- 没有外键的一端需要使用one-to-one元素，该元素使用 property-ref 属性指定使用被关联实体主键以外的字段作为关联字段(没有property-ref将使用dept主键做关联，否则使用mgr属性对应表字段做关联) --&gt; &lt;one-to-one name="dept" class="com.atguigu.hibernate.one2one.foreign.Department" property-ref="mgr"&gt;&lt;/one-to-one&gt; &lt;/class&gt; &lt;/hibernate-mapping&gt;====================================================&lt;hibernate-mapping&gt; &lt;class name="com.atguigu.hibernate.one2one.foreign.Department" table="DEPARTMENTS"&gt; &lt;id name="deptId" type="java.lang.Integer"&gt; &lt;column name="DEPT_ID" /&gt; &lt;generator class="native" /&gt; &lt;/id&gt; &lt;property name="deptName" type="java.lang.String"&gt; &lt;column name="DEPT_NAME" /&gt; &lt;/property&gt; &lt;!-- 使用 many-to-one 的方式来映射 1-1 关联关系 --&gt; &lt;many-to-one name="mgr" class="com.atguigu.hibernate.one2one.foreign.Manager" column="MGR_ID" unique="true"&gt;&lt;/many-to-one&gt; &lt;/class&gt;&lt;/hibernate-mapping&gt; 测试 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647 @Test public void testGet2()&#123; //在查询没有外键的实体对象时, 使用的左外连接查询, 一并查询出其关联的对象 //并已经进行初始化. Manager mgr = (Manager) session.get(Manager.class, 1); System.out.println(mgr.getMgrName()); System.out.println(mgr.getDept().getDeptName()); &#125; @Test public void testGet()&#123; //1. 默认情况下对关联属性使用懒加载 Department dept = (Department) session.get(Department.class, 1); System.out.println(dept.getDeptName()); //2. 所以会出现懒加载异常的问题. //session.close(); //Manager mgr = dept.getMgr(); //System.out.println(mgr.getClass()); //System.out.println(mgr.getMgrName()); //3. 查询 Manager 对象的连接条件应该是 dept.manager_id = mgr.manager_id //而不应该是 dept.dept_id = mgr.manager_id Manager mgr = dept.getMgr(); System.out.println(mgr.getMgrName()); &#125; @Test public void testSave()&#123; Department department = new Department(); department.setDeptName("DEPT-BB"); Manager manager = new Manager(); manager.setMgrName("MGR-BB"); //设定关联关系 department.setMgr(manager); manager.setDept(department); //保存操作 //建议先保存没有外键列的那个对象. 这样会减少 UPDATE 语句 session.save(department); session.save(manager); &#125;&#125; 一对一(基于主键) 实体类同上 映射 12345678910111213141516171819202122232425262728293031323334353637383940414243&lt;hibernate-mapping package="com.atguigu.hibernate.one2one.primary"&gt; &lt;class name="Department" table="DEPARTMENTS"&gt; &lt;id name="deptId" type="java.lang.Integer"&gt; &lt;column name="DEPT_ID" /&gt; &lt;!-- 使用外键的方式来生成当前的主键 --&gt; &lt;generator class="foreign"&gt; &lt;!-- property 属性指定使用当前持久化类的哪一个属性的主键作为外键 --&gt; &lt;param name="property"&gt;mgr&lt;/param&gt; &lt;/generator&gt; &lt;/id&gt; &lt;property name="deptName" type="java.lang.String"&gt; &lt;column name="DEPT_NAME" /&gt; &lt;/property&gt; &lt;!-- 采用 foreign 主键生成器策略的一端增加 one-to-one 元素映射关联属性, 其 one-to-one 节点还应增加 constrained=true 属性, 以使当前的主键上添加外键约束 --&gt; &lt;one-to-one name="mgr" class="Manager" constrained="true"&gt;&lt;/one-to-one&gt; &lt;/class&gt;&lt;/hibernate-mapping&gt;=================================================&lt;hibernate-mapping&gt; &lt;class name="com.atguigu.hibernate.one2one.primary.Manager" table="MANAGERS"&gt; &lt;id name="mgrId" type="java.lang.Integer"&gt; &lt;column name="MGR_ID" /&gt; &lt;generator class="native" /&gt; &lt;/id&gt; &lt;property name="mgrName" type="java.lang.String"&gt; &lt;column name="MGR_NAME" /&gt; &lt;/property&gt; &lt;one-to-one name="dept" class="com.atguigu.hibernate.one2one.primary.Department"&gt;&lt;/one-to-one&gt; &lt;/class&gt; 测试 123456789101112131415161718192021222324252627282930313233343536373839@Testpublic void testGet2()&#123; //在查询没有外键的实体对象时, 使用的左外连接查询, 一并查询出其关联的对象 //并已经进行初始化. Manager mgr = (Manager) session.get(Manager.class, 1); System.out.println(mgr.getMgrName()); System.out.println(mgr.getDept().getDeptName()); &#125;@Testpublic void testGet()&#123; //1. 默认情况下对关联属性使用懒加载 Department dept = (Department) session.get(Department.class, 1); System.out.println(dept.getDeptName()); //2. 所以会出现懒加载异常的问题. Manager mgr = dept.getMgr(); System.out.println(mgr.getMgrName()); &#125;@Testpublic void testSave()&#123; Department department = new Department(); department.setDeptName("DEPT-DD"); Manager manager = new Manager(); manager.setMgrName("MGR-DD"); //设定关联关系 manager.setDept(department); department.setMgr(manager); //保存操作 //先插入哪一个都不会有多余的 UPDATE session.save(department); session.save(manager); &#125; 映射多对多 实体类 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public class Category &#123; private Integer id; private String name; private Set&lt;Item&gt; items = new HashSet&lt;&gt;(); public Integer getId() &#123; return id; &#125; public void setId(Integer id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public Set&lt;Item&gt; getItems() &#123; return items; &#125; public void setItems(Set&lt;Item&gt; items) &#123; this.items = items; &#125;&#125;public class Item &#123; private Integer id; private String name; private Set&lt;Category&gt; categories = new HashSet&lt;&gt;(); public Integer getId() &#123; return id; &#125; public void setId(Integer id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public Set&lt;Category&gt; getCategories() &#123; return categories; &#125; public void setCategories(Set&lt;Category&gt; categories) &#123; this.categories = categories; &#125;&#125; 映射(单向的配置为去掉一个的配置) 123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;hibernate-mapping package="com.atguigu.hibernate.n2n"&gt; &lt;class name="Category" table="CATEGORIES"&gt; &lt;id name="id" type="java.lang.Integer"&gt; &lt;column name="ID" /&gt; &lt;generator class="native" /&gt; &lt;/id&gt; &lt;property name="name" type="java.lang.String"&gt; &lt;column name="NAME" /&gt; &lt;/property&gt; &lt;!-- table: 指定中间表 --&gt; &lt;set name="items" table="CATEGORIES_ITEMS"&gt; &lt;key&gt; &lt;column name="C_ID" /&gt; &lt;/key&gt; &lt;!-- 使用 many-to-many 指定多对多的关联关系. column 执行 Set 集合中的持久化类在中间表的外键列的名称 --&gt; &lt;many-to-many class="Item" column="I_ID"&gt;&lt;/many-to-many&gt; &lt;/set&gt; &lt;/class&gt;&lt;/hibernate-mapping&gt;======================================================&lt;hibernate-mapping&gt; &lt;class name="com.atguigu.hibernate.n2n.Item" table="ITEMS"&gt; &lt;id name="id" type="java.lang.Integer"&gt; &lt;column name="ID" /&gt; &lt;generator class="native" /&gt; &lt;/id&gt; &lt;property name="name" type="java.lang.String"&gt; &lt;column name="NAME" /&gt; &lt;/property&gt; &lt;set name="categories" table="CATEGORIES_ITEMS" inverse="true"&gt; &lt;key column="I_ID"&gt;&lt;/key&gt; &lt;many-to-many class="com.atguigu.hibernate.n2n.Category" column="C_ID"&gt;&lt;/many-to-many&gt; &lt;/set&gt; &lt;/class&gt;&lt;/hibernate-mapping&gt; 映射继承关系]]></content>
      <tags>
        <tag>hibernate</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解java虚拟机-读书笔记]]></title>
    <url>%2F2018%2F11%2F27%2F%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3java%E8%99%9A%E6%8B%9F%E6%9C%BA-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[1.局部变量表存放了编译期可知的各种基本数据类型(boolean\byte\char\short\int\float\long\double)、对象引用（reference类型，它不等同于对象本身，可能是一个指向对象起始地址的引用指针，也可能是指向一个代表对象的句柄或其他与此对象相关的位置)和returnAddress类型(指向了一条字节码指令的地址)。 其中64位长度的long和double类型的数据会占用两个局部变量空间(slot),其余的数据类型只占用一个。局部变量表所需的内存空间在编译期间完成分配，当进入一个方法时，这个方法需要在帧中分配多大的局部变量空间是完全确定的，在方法运行期间不会改变局部变量表的大小。 在java虚拟机规范中，这个区域规定了两种异常状况： 1. 如果线程请求的栈深度大于虚拟机所允许的深度，将抛出StackOverFlow异常； 2. 如果栈可以动态扩展，如果扩展时无法申请到足够的内存，就会抛出OutOfMemoryError异常。]]></content>
  </entry>
  <entry>
    <title><![CDATA[设计模式-备忘录模式]]></title>
    <url>%2F2018%2F11%2F15%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%A4%87%E5%BF%98%E5%BD%95%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[定义保存一个对象的某个状态，以便在适当的时候恢复对象。 适用场景 保存及恢复数据相关业务场景。 后悔的时候，既想恢复到之前的状态。 总结 优点 为用户提供一种可恢复的机制。 存档信息的封装。 缺点 资源占用。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式-观察者模式]]></title>
    <url>%2F2018%2F11%2F14%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E8%A7%82%E5%AF%9F%E8%80%85%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[定义定义了对象之间的一对多依赖，让多个观察者对象同时监听某一个主题对象，当主题对象发生变化时，他的所有依赖者(观察者)都会收到通知并更新。 适用场景 关联行为场景，建立一套触发机制。 UML 总结 优点 观察者和被观察者之间建立一个抽象的耦合。 观察者模式支持广播通信。 缺点 观察者之间有过多的细节依赖，增加了时间消耗及程序的复杂度。 使用要得当，要避免循环调用。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式-解释器模式]]></title>
    <url>%2F2018%2F11%2F14%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E8%A7%A3%E9%87%8A%E5%99%A8%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[定义给定一个语言，定义它的文法的一种表示，并定义一个解释器，这个解释器使用该表示来解释语言中的句子。为了解释一种语言，而为语言创建的解释器。 适用场景 某个特定类型问题发生频率足够高。 平时需要写的并不多。 总结 优点 语法有很多类表示，容易改变及扩展此语言。 缺点 当语法规则数目太多时，增加了系统的复杂度。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式-策略模式]]></title>
    <url>%2F2018%2F11%2F11%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E7%AD%96%E7%95%A5%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[定义定义了算法家族，分别封装起来，让他们之间可以互相替换，此模式让算法的变化不会影响到使用算法的用户。 适用场景 系统有很多类，而他们的区别仅仅在于他们的行为不同。 一个系统需要动态的在几种算法中选择一个。 UML 总结 优点 符合开闭原则。 避免使用多重条件转移语句。 提高算法的保密性和安全性。 缺点 客户端必须知道所有的策略类，并自行决定使用哪一种策略类。 产生很多策略类。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jenkins配置邮件通知]]></title>
    <url>%2F2018%2F11%2F01%2Fjenkins%E9%85%8D%E7%BD%AE%E9%82%AE%E4%BB%B6%E9%80%9A%E7%9F%A5%2F</url>
    <content type="text"><![CDATA[jenkins系统配置 设置jenkins地址和管理员邮箱地址 系统管理–&gt;系统设置–&gt;Jenkins Location 设置发件人等信息 系统管理–&gt;系统设置–&gt;Extended E-mail Notification PS：这里的发件人邮箱地址切记要和系统管理员邮件地址保持一致（当然，也可以设置专门的发件人邮箱，不过不影响使用，根据具体情况设置即可） 配置邮件模板default Content 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset="UTF-8"&gt;&lt;title&gt;$&#123;ENV, var="JOB_NAME"&#125;-第$&#123;BUILD_NUMBER&#125;次构建日志&lt;/title&gt;&lt;/head&gt;&lt;body leftmargin="8" marginwidth="0" topmargin="8" marginheight="4" offset="0"&gt; &lt;table width="95%" cellpadding="0" cellspacing="0" style="font-size: 11pt; font-family: Tahoma, Arial, Helvetica, sans-serif"&gt; &lt;tr&gt; &lt;td&gt;(本邮件是程序自动下发的，请勿回复！)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;h2&gt; &lt;font color="#0000FF"&gt;构建结果 - $&#123;BUILD_STATUS&#125;&lt;/font&gt; &lt;/h2&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;br /&gt; &lt;b&gt;&lt;font color="#0B610B"&gt;构建信息&lt;/font&gt;&lt;/b&gt; &lt;hr size="2" width="100%" align="center" /&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;ul&gt; &lt;li&gt;项目名称&amp;nbsp;：&amp;nbsp;$&#123;PROJECT_NAME&#125;&lt;/li&gt; &lt;li&gt;构建编号&amp;nbsp;：&amp;nbsp;第$&#123;BUILD_NUMBER&#125;次构建&lt;/li&gt; &lt;li&gt;SVN&amp;nbsp;版本：&amp;nbsp;$&#123;SVN_REVISION&#125;&lt;/li&gt; &lt;li&gt;触发原因：&amp;nbsp;$&#123;CAUSE&#125;&lt;/li&gt; &lt;li&gt;构建日志：&amp;nbsp;&lt;a href="$&#123;BUILD_URL&#125;console"&gt;$&#123;BUILD_URL&#125;console&lt;/a&gt;&lt;/li&gt; &lt;li&gt;构建&amp;nbsp;&amp;nbsp;Url&amp;nbsp;：&amp;nbsp;&lt;a href="$&#123;BUILD_URL&#125;"&gt;$&#123;BUILD_URL&#125;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;工作目录&amp;nbsp;：&amp;nbsp;&lt;a href="$&#123;PROJECT_URL&#125;ws"&gt;$&#123;PROJECT_URL&#125;ws&lt;/a&gt;&lt;/li&gt; &lt;li&gt;项目&amp;nbsp;&amp;nbsp;Url&amp;nbsp;：&amp;nbsp;&lt;a href="$&#123;PROJECT_URL&#125;"&gt;$&#123;PROJECT_URL&#125;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;b&gt;&lt;font color="#0B610B"&gt;Changes Since Last Successful Build:&lt;/font&gt;&lt;/b&gt; &lt;hr size="2" width="100%" align="center" /&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;ul&gt; &lt;li&gt;历史变更记录 : &lt;a href="$&#123;PROJECT_URL&#125;changes"&gt;$&#123;PROJECT_URL&#125;changes&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; $&#123;CHANGES_SINCE_LAST_SUCCESS,reverse=true, format="Changes for Build #%n:&lt;br /&gt;%c&lt;br /&gt;",showPaths=true,changesFormat="&lt;pre&gt;[%a]&lt;br /&gt;%m&lt;/pre&gt;",pathFormat="&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;%p"&#125; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;b&gt;Failed Test Results&lt;/b&gt; &lt;hr size="2" width="100%" align="center" /&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;pre style="font-size: 11pt; font-family: Tahoma, Arial, Helvetica, sans-serif"&gt;$FAILED_TESTS&lt;/pre&gt; &lt;br /&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;b&gt;&lt;font color="#0B610B"&gt;构建日志 (最后 100行):&lt;/font&gt;&lt;/b&gt; &lt;hr size="2" width="100%" align="center" /&gt;&lt;/td&gt; &lt;/tr&gt; &lt;!-- &lt;tr&gt; &lt;td&gt;Test Logs (if test has ran): &lt;a href="$&#123;PROJECT_URL&#125;ws/TestResult/archive_logs/Log-Build-$&#123;BUILD_NUMBER&#125;.zip"&gt;$&#123;PROJECT_URL&#125;/ws/TestResult/archive_logs/Log-Build-$&#123;BUILD_NUMBER&#125;.zip&lt;/a&gt; &lt;br /&gt; &lt;br /&gt; &lt;/td&gt; &lt;/tr&gt; --&gt; &lt;tr&gt; &lt;td&gt;&lt;textarea cols="80" rows="30" readonly="readonly" style="font-family: Courier New"&gt;$&#123;BUILD_LOG, maxLines=100&#125;&lt;/textarea&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/table&gt;&lt;/body&gt;&lt;/html&gt; 设置邮件触发机制 上面的几步完成后，点击应用，保存即可。 项目配置 进入项目配置页面 进入系统配置页面后，点击上方的构建后操作选项，配置内容如下： 构建项目测试]]></content>
      <tags>
        <tag>jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[局域网内搭建邮件服务器-hMailServer]]></title>
    <url>%2F2018%2F11%2F01%2F%E5%B1%80%E5%9F%9F%E7%BD%91%E5%86%85%E6%90%AD%E5%BB%BA%E9%82%AE%E4%BB%B6%E6%9C%8D%E5%8A%A1%E5%99%A8-hMailServer%2F</url>
    <content type="text"><![CDATA[hMailServer安装 hMailServer,到hMailServer网站下载最新版本hMailServer安装包。 双击安装文件 设置密码，该密码在配置服务器时使用。 配置hMailServer 打开配置程序 因为内网中没有DNS服务器，所以域名随便取一个。 添加邮件地址 配置SMTP 打开logging 配置ip Ranges 修改hosts文件，添加SMTP的域名解析为本机地址。 到此，hmailServer服务端配置完成。 配置foxmail客户端]]></content>
      <tags>
        <tag>jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VisualSVN]]></title>
    <url>%2F2018%2F10%2F23%2FVisualSVN%2F</url>
    <content type="text"></content>
      <tags>
        <tag>svn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[visualsvn配置hooks调用jenkins自动构建]]></title>
    <url>%2F2018%2F10%2F23%2Fvisualsvn%E9%85%8D%E7%BD%AEhooks%E8%B0%83%E7%94%A8jenkins%E8%87%AA%E5%8A%A8%E6%9E%84%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[jenkins配置 为了让Jenkins中的job可以被触发，job需要被显式地配置为启用SCM轮询才行，未启用SCM轮询选项的job将不会被post-commit hook所触发。下图为在job中启用SCM轮询的示例： 如果Jenkins启用了跨站点请求伪造防护(默认启用)选项，那么上面的请求会返回一个403错误(“No valid crumb was included”)。在”系统管理”→”全局安全配置”中，可以看到跨站点请求伪造防护是否有启用： VisualSVN配置 提交到 VisualSVN Server 时 hook 的 post-commit.bat（post-commit.cmd） 不执行的解决方法： 这是因为 bat 文件执行需要权限，而 VisualSVN Server 默认用的是 NETWORK 用户组，该组没有执行 bat 的权限，导致了 post-commit.bat 文件不能执行，解决方法如下： 配置hooks curl -X POST http://localhost:6080/job/cecaudit/build?delay=0sec –user admin:123456789 –data-urlencode json= 链接地址为jenkins中的job地址（点击立即构建时的链接），—user后为 用户名:密码 Ps:需要先安装curl或wget]]></content>
      <tags>
        <tag>svn</tag>
        <tag>jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式-模板方法模式]]></title>
    <url>%2F2018%2F10%2F05%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E6%A8%A1%E6%9D%BF%E6%96%B9%E6%B3%95%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[定义定义了一个算法的骨架，并允许子类为一个或多个步骤提供实现。模板方法使得子类可以在不改变算法结构的情况下，重新定义算法的某些步骤。 适用场景 一次性实现一个算法的不变的部分，并将可变的行为留给子类来实现。 各个子类中公共的行为被提取出来并集中到一个公共父类中，从而避免代码重复。 UML 总结 优点 提高复用性。 提高扩展性。 符合开闭原则。 缺点 类数目增加。 增加了系统实现的复杂度。 继承关系自身缺点，如果父类添加新的抽象方法，所有子类都要改一遍。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式-迭代器模式]]></title>
    <url>%2F2018%2F10%2F05%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E8%BF%AD%E4%BB%A3%E5%99%A8%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[定义提供了一种方法，顺序访问一个集合中的各个元素，而又不暴露该对象的内部表示。 适用场景 访问一个集合对象的内容而无需暴露它的内部表示。 为遍历不同的集合结构提供一个统一的接口。 UML Iterator(抽象迭代器):它定义了访问和遍历元素的接口，声明了用于遍历数据元素的方法，例如:用于获取第一个元素的first()方法，用于访问下一个元素的next()方法，用于判断是否还有下一个元素的hasNext()方法，用于获取当前元素的currentItem()方法等，在具体迭代器中将实现这些方法。 ConcreteIterator(具体迭代器):它实现了抽象迭代器接口，完成对聚合对象的遍历，同时在具体迭代器中通过游标来记录在聚合对象中所处的当前位置，在具体实现时，游标通常是一个表示位置的非负整数。 Aggregate(抽象聚合类):它用于存储和管理元素对象，声明一个createIterator()方法用于创建一个迭代器对象，充当抽象迭代器工厂角色。 ConcreteAggregate(具体聚合类):它实现了在抽象聚合类中声明的createIterator()方法，该方法返回一个与该具体聚合类对应的具体迭代器ConcreteIterator实例。 在迭代器模式结构图中，我们可以看到具体迭代器类和具体聚合类之间存在双重关系，其中一个关系为关联关系，在具体迭代器中需要维持一个对具体聚合对象的引用，该关联关系的目的是访问存储在聚合对象中的数据，以便迭代器能够对这些数据进行遍历操作。除了使用关联关系外，为了能够让迭代器可以访问到聚合对象中的数据，我们还可以将迭代器类设计为聚合类的内部类，JDK中的迭代器类就是通过这种方法来实现的，如下AbstractList类代码片段所示: 12345678910package java.util;......public abstract class AbstractList&lt;E&gt; extends AbstractCollection&lt;E&gt; ...... private class Itr implements Iterator&lt;E&gt; &#123; int cursor = 0; ...... &#125;...... &#125; 总结 优点 分离了集合对象的遍历行为 缺点 类的个数成对增加。 增加了程序的复杂性。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式-代理模式]]></title>
    <url>%2F2018%2F10%2F05%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[定义为其他对象提供一种代理，以控制对这个对象的访问。代理对象在客户端和目标对象之间起到中介的作用。 适用场景 保护目标对象。 增强目标对象。 UML 静态代理 动态代理 总结 优点 代理模式能将代理对象与真实被调用的目标对象分离。 一定程度上降低了系统的耦合度，扩展性好。 保护目标对象。 增强目标对象。 缺点 代理模式会造成系统设计中类的数目增加。 在客户端和目标对象增加一个代理对象，会造成请求处理速度变慢。 增加系统的复杂度。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式-桥接模式]]></title>
    <url>%2F2018%2F10%2F04%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E6%A1%A5%E6%8E%A5%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[定义将抽象部分与它的具体实现部分分离，使他们都可以独立的变化。通过组合的方式建立两个类之间的联系，而不是继承。 适用场景 抽象和具体实现之间增加更多的灵活性 一个类存在两个（或多个）独立变化的纬度，且这两个（或读个）纬度都需要进行独立的扩展。 不希望使用继承，或因为多继承导致系统类的个数剧增。 UML 在桥接模式结构图中包含如下几个角色: Abstraction(抽象类):用于定义抽象类的接口，它一般是抽象类而不是接口，其中定义了 一个Implementor(实现类接口)类型的对象并可以维护该对象，它与Implementor之间具有关 联关系，它既可以包含抽象业务方法，也可以包含具体业务方法。 RefinedAbstraction(扩充抽象类):扩充由Abstraction定义的接口，通常情况下它不再是抽象类而是具体类，它实现了在Abstraction中声明的抽象业务方法，在RefinedAbstraction中可以调用在Implementor中定义的业务方法。 Implementor(实现类接口):定义实现类的接口，这个接口不一定要与Abstraction的接口完全一致，事实上这两个接口可以完全不同，一般而言，Implementor接口仅提供基本操作，而Abstraction定义的接口可能会做更多更复杂的操作。Implementor接口对这些基本操作进行了声明，而具体实现交给其子类。通过关联关系，在Abstraction中不仅拥有自己的方法，还可以调用到Implementor中定义的方法，使用关联关系来替代继承关系。 ConcreteImplementor(具体实现类):具体实现Implementor接口，在不同的ConcreteImplementor中提供基本操作的不同实现，在程序运行时，ConcreteImplementor对象将替换其父类对象，提供给抽象类具体的业务操作方法。 总结 优点 分离抽象部分及其具体实现部分。 提高了系统的可扩展性。 符合开闭原则。 符合合成复用原则。 缺点 增加了系统的设计与理解难度。 需要正确的识别出系统中两个独立变化的纬度。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式-组合模式]]></title>
    <url>%2F2018%2F10%2F04%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E7%BB%84%E5%90%88%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[定义将对象组合成树形结构以表示“部分-整体”的层次结构，组合模式使客户端对单个对象和组合对象保持一致的处理方式。 适用场景 希望客户端可以忽略组合对象与单个对象的差异时。 处理一个树形结构时。 UML 总结 优点 清楚的定义分层次的复杂对象，表示对象的全部或部分层次。 让客户端忽略层次的差异，方便对整个层次结构进行控制。 简化客户端代码。 符合开闭原则 缺点 限制类型时会较为复杂。 使设计变的更加抽象。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式-享元模式]]></title>
    <url>%2F2018%2F10%2F03%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E4%BA%AB%E5%85%83%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[定义提供了减少对象数量从而改善应用所需的对象结构的方式。运用共享技术有效的支持大量细粒度的对象。 适用场景 常常应用于系统底层的开发，以便解决系统性能的问题。 系统有大量相似对象，需要缓冲池的场景。 UML 总结 优点 减少对象的创建，降低内存中对象的数量，降低系统的内存，提高效率。 减少内存之外的其他资源的占用。（减少new操作所需的时间） 缺点 关注内/外部状态，关注线程安全问题。 使程序的逻辑复杂化。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式-外观模式]]></title>
    <url>%2F2018%2F10%2F03%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%A4%96%E8%A7%82%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[定义为子系统中的一组接口提供一个统一的入口。外观模式定义了一个高层接口，这个接口使得这一子系统更加容易使用。 UML 总结 外观模式的主要优点如下: (1) 它对客户端屏蔽了子系统组件，减少了客户端所需处理的对象数目，并使得子系统使用起来更加容易。通过引入外观模式，客户端代码将变得很简单，与之关联的对象也很少。 (2) 它实现了子系统与客户端之间的松耦合关系，这使得子系统的变化不会影响到调用它的客 户端，只需要调整外观类即可。 (3) 一个子系统的修改对其他子系统没有任何影响，而且子系统内部变化也不会影响到外观对象。 外观模式的主要缺点如下: (1) 不能很好地限制客户端直接使用子系统类，如果对客户端访问子系统类做太多的限制则减少了可变性和灵活性。 (2) 如果设计不当，增加新的子系统可能需要修改外观类的源代码，违背了开闭原则。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式-单例模式2]]></title>
    <url>%2F2018%2F10%2F01%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F2%2F</url>
    <content type="text"><![CDATA[单例模式安全问题 序列化和反射问题： 12345678910111213141516171819202122232425public class HungrySingleton implements Serializable,Cloneable&#123; private final static HungrySingleton hungrySingleton; static&#123; hungrySingleton = new HungrySingleton(); &#125; private HungrySingleton()&#123; //反射时处理，如果已存在则抛出异常 if(hungrySingleton != null)&#123; throw new RuntimeException("单例构造器禁止反射调用"); &#125; &#125; public static HungrySingleton getInstance()&#123; return hungrySingleton; &#125; private Object readResolve()&#123;//反序列化时会调用此方法，从而防止创建多个实例 return hungrySingleton; &#125; @Override protected Object clone() throws CloneNotSupportedException &#123; return getInstance(); &#125;&#125; 内部类处理方式相同 1234567891011121314public class StaticInnerClassSingleton &#123; private static class InnerClass&#123; private static StaticInnerClassSingleton staticInnerClassSingleton = new StaticInnerClassSingleton(); &#125; public static StaticInnerClassSingleton getInstance()&#123; return InnerClass.staticInnerClassSingleton; &#125; private StaticInnerClassSingleton()&#123; if(InnerClass.staticInnerClassSingleton != null)&#123; throw new RuntimeException("单例构造器禁止反射调用"); &#125; &#125;&#125; Effective java 推荐的枚举类型单例 1234567891011121314151617181920public class EnumSingleton &#123; //私有构造函数 private EnumSingleton()&#123;&#125; public static EnumSingleton getInstance()&#123; return Singleton.INSTANCE.getInstance(); &#125; private enum Singleton&#123; INSTANCE; private EnumSingleton singleton; //jvm保证这个方法绝对只调用一次 Singleton()&#123; singleton = new EnumSingleton(); &#125; public EnumSingleton getInstance()&#123; return singleton; &#125; &#125;&#125;]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jenkins自动构建配置]]></title>
    <url>%2F2018%2F09%2F04%2Fjenkins%E8%87%AA%E5%8A%A8%E6%9E%84%E5%BB%BA%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[安装jenkins 官网下载jenkins jenkins 终端运行：java -jar jenkins.war –httpPort=8080 8080为端口号，可以自行设置 在浏览器中输入：http://localhost:8080 访问jenkins服务 首次启动jenkins，出于安全考虑，jenkins会生成一个随机的口令到 /root/.jenkins/secrets/initialAdminPassword 文件中，复制文件中的口令到jenkins即可通过访问。 安装ant 官网下载Ant Ant 配置环境变量 ANT_HOME ant的根路径 path $ANT_HOME/bin classpath $ANT_HOME/lib 验证 终端输入 ant 有正常的输出，则表示安装成功 配置tomcat 配置tomcat-users.xml 添加角色和用户 123&lt;role rolename="manager-gui"/&gt; &lt;role rolename="manager-script"/&gt;&lt;user username="tomcat" password="123456" roles="manager-gui, manager-script"/&gt; 配置TOMCAT_HOME/conf/context.xml，在元素中增加一个属性antiResourceLocking=”true” antiJARLocking=”true”，默认是”false”。 1&lt;Context antiResourceLocking="true" antiJARLocking="true"&gt; 以上为了解决Jenkins部署异常：The Tomcat Manager responded FAIL - Deployed application at context path]。 异常原因： Tomcat应用更新时，把新的WAR包放到webapps目录下，Tomcat就会自动把原来的同名webapp删除，并把WAR包解压，运行新的 webapp 但是，有时候Tomcat并不能把旧的webapp完全删除，通常会留下WEB-INF/lib下的某个jar包，必须关闭Tomcat才能删除，这就导致自动部署失败 解决方法是在元素中增加一个属性antiResourceLocking=”true” antiJARLocking=”true”，默认是”false”。这样就可以热部署了 实际上，这两个参数就是配置Tomcat的资源锁定和Jar包锁定策略 安装jenkins插件 安装svn插件 Subversion Plug-in 安装 Deploy to container Plugin 插件 创建jenkins任务(svn+ant+tomcat) 新建任务，构建一个自由风格的软件项目 svn 配置 构建环境和构建 构建后操作]]></content>
  </entry>
  <entry>
    <title><![CDATA[高性能js读书笔记]]></title>
    <url>%2F2018%2F08%2F07%2F%E9%AB%98%E6%80%A7%E8%83%BDjs%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[动态脚本元素1234var script = document.createElement("script");script.type = "text/javascript";script.src = "file1.js";document.getElementsByTagName("head")[0].appendChild(script); 这种技术的重点在于：无论何时启动下载，文件的下载和执行过程都不会阻塞页面其他进程。]]></content>
  </entry>
  <entry>
    <title><![CDATA[memcached]]></title>
    <url>%2F2018%2F08%2F05%2Fmemcached%2F</url>
    <content type="text"><![CDATA[安装Libevent: 官网下载，解压缩 ./configure –prefix=/opt/install/libevent make &amp;&amp; make install memecached: 解压缩 ./configure –prefix=/opt/install/memcached –with-libevent=/opt/install/libevent make &amp;&amp; make instll 启动启动参数： -d 启动一个守护进程 -m 分配给memcached 的内存数量（单位为M） -u 运行memcached 的用户 -L 监听的服务器IP地址 -p 监听的端口号 -c 最大运行的并发连接数 -P 设置Pid 文件 操作 java客户端]]></content>
  </entry>
  <entry>
    <title><![CDATA[mac安装autojump]]></title>
    <url>%2F2018%2F08%2F01%2Fmac%E5%AE%89%E8%A3%85autojump%2F</url>
    <content type="text"><![CDATA[brew install autojump 编辑 vim ~/.zshrc 找到 plugins=，在后面添加autojump：plugins=(git autojump) 新开一行，添加：[[ -s $(brew –prefix)/etc/profile.d/autojump.sh ]] &amp;&amp; . $(brew –prefix)/etc/profile.d/autojump.sh j + 跳转的目录]]></content>
  </entry>
  <entry>
    <title><![CDATA[tmux配置使用]]></title>
    <url>%2F2018%2F07%2F23%2Ftmux%E9%85%8D%E7%BD%AE%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[什么是tmuxtmux是一个工具，用于在终端窗口中运行多个终端会话，可以使终端会话进入后台运行。 安装 $ brew install tmux 快捷键前缀为了使自身的快捷键不和其他软件的快捷键产生冲突，tmux提供了一个快捷键前缀。当使用快捷键时要先按下快捷键前缀，然后再按下快捷键，默认的前缀是Ctrl-b 创建会话 tmux new -s 假如还需要开发另一个项目，可以再创建一个新会话，但原来的会话不会消失，若要创建一个新会话，只需要按下 :，然后输入 new -s 除非显式的关闭会话，否则tmux的会话在重启计算机之前都不会消失。 切换会话 获取会话列表 s 列表中的每个会话都有一个 ID，该 ID 是从 0 开始的。按下对应的 ID 就可以进入会话。如果你已经创建了一个或多个会话，但是还没有运行 tmux，那么可以输入如下命令以接入已开启的会话: tmux attach 会话外获取会话列表： tmux ls tmux attach/a -t 在会话外进入session tmux attach/a 进入列表第一个会话 d 临时退出但不删除会话 :kill-session 在会话内退出并删除session :kill-server 删除所有session tmux kill-session -t 在会话外删除指定session 窗口一个tmux中可以包含多个窗口。 c 创建窗口 w 查看窗口列表 0 切换到指定窗口，窗口对应的数字 n 切换到下一个窗口 p 切换到上一个窗口 l 在相邻的两个窗口切换 , 重命名窗口 f 在多个窗口里搜索关键字 &amp; 删除窗口 窗格一个tmux窗口可以分割成多个窗格，并且窗格可以在不同的窗口中移动、合并、拆分。 “ 水平分割 % 垂直分割 o 按顺序在Pane之间移动 方向键 上下左右选择pane :resize-pane -U #向上调整大小 :resize-pane -D #向下 :resize-pane -L #向左 :resize-pane -R #向右 :resize-pane -D 5 #向下移动5行 { （往左边，往上面） } （往右边，往下面） x 删除pane 空格 更换pane排版 ！ 移动pane至新的window :join-pane -t $window_name 移动pane合并至某个window Ctrl+o 按顺序移动pane位置 q 显示pane编号 滚动屏幕 [ 进入copy-mode 模式，就可以进行屏幕滚动，q键退出。]]></content>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[svn安装和配置]]></title>
    <url>%2F2018%2F07%2F16%2Fsvn%E5%AE%89%E8%A3%85%E5%92%8C%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[安装服务器端程序 yum install -y subversion 创建并配置版本库 创建版本库目录 mkdir -p /var/svn/repository 版本库目录下创建具体的项目目录（可以多个） 创建svn版本库 svnadmin create /var/svn/repository/项目目录 配置svn对应的服务 svn://ip:3690/项目目录 （默认端口号3690） 修改服务配置 /etc/rc.d/init.d/svnserve (注意备份) 原版：args=”–daemon –pid-file=${pidfile} $OPTIONS” 修改版：args=”–daemon –root=/var/svn/repository(版本库根目录) –listen-port 2255(实际的端口号) –pid-file=${pidfile} $OPTIONS” 启动svn服务 service svnserve start 命令行客户端 检出（完整下载版本库中的全部内容） svn checkout svn://ip/项目目录 本地目录 工作副本 .svn所在目录为工作副本。 版本控制相关操作都要在工作副本目录下执行。 为了保证工作副本能够正常和服务器进行交互，一般不要删除.svn中的内容。 添加 svn 要求提交一个新建的文件前先把这个文件添加到版本控制体系中。 svn add 文件名 svn提交 svn commit -m “提交信息” 要提交的文件 查看服务器端文件内容 svn list svn:ip/项目目录 更新 svn update [文件名] svn权限管理 三个对应的配置文件 conf/svnserve.conf anon-access = read 匿名访问 auth-access = write 授权访问 （注意空格） passwd-db= passwd 指定设置用户名密码的配置文件 authz-db=authz 分配权限的配置文件 conf/passwd 用户名 = 密码 conf/authz [groups]创建用户组 组名=组员，组员（使用，隔开） [/] （/表示版本库根目录） @组名=rw 配置组权限 用户名=r 配置用户名权限 *= 其他人没有权限]]></content>
  </entry>
  <entry>
    <title><![CDATA[vim笔记]]></title>
    <url>%2F2018%2F07%2F15%2Fvim%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[有些时候当我们使用Vi/Vim编辑文件时如果没有注意到文件权限的时候，当最后进行保存时候的可能会提示以下错误，如果强制退出后再切换用户，肯定会丢失当前的改动，可以按下边的方法执行： 在Vi/Vim编辑器进入冒号，然后输入以下命令 %! sudo tee % /dev/null 此时会提示输入sudo用户的密码]]></content>
      <tags>
        <tag>vim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[effective-java读书笔记2]]></title>
    <url>%2F2018%2F07%2F14%2Feffective-java%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B02%2F</url>
    <content type="text"><![CDATA[第一条：考虑用静态工厂方法代替构造器优势： 静态工厂方法有名字，当一个类需要多个带有相同签名的构造器时，就用静态工厂方法代替构造器，并且慎重的选择名称以便突出他们之间的区别。 不必在每次调用它们的时候都创建一个新对象。 它们可以返回原返回类型的任何子类型的对象。这样我们在选择返回对象的类时就有了更大的灵活性。 创建参数化类型实例的时候，它们使代码变得更加简洁(但在新版本的java中已经可以省略) 1List&lt;String&gt; list = new ArrayList&lt;String&gt;(); 劣势 类如果不含公有的或者受保护的构造器，就不能被子类化。 它们与其它的静态方法实际上没有任何区别。在API文档中，它们没有像构造器那样在API文档中明确的标识出来。因此要想查明如何实例化一个类非常困难。 第六条：避免创建不必要的对象12String s = new String("bikini"); // DON'T DO THIS!String s = "bikini"; //good 判断一个字符串是否是一个合法的罗马数字： 123456789101112131415// Performance can be greatly improved! 每次调用都会创建Pattern实例，非常昂贵的static boolean isRomanNumeral(String s) &#123;return s.matches("^(?=.)M*(C[MD]|D?C&#123;0,3&#125;)"+ "(X[CL]|L?X&#123;0,3&#125;)(I[XV]|V?I&#123;0,3&#125;)$");&#125;// Reusing expensive object for improved performancepublic class RomanNumerals &#123; private static final Pattern ROMAN = Pattern.compile( "^(?=.)M*(C[MD]|D?C&#123;0,3&#125;)" + "(X[CL]|L?X&#123;0,3&#125;)(I[XV]|V?I&#123;0,3&#125;)$"); static boolean isRomanNumeral(String s) &#123; return ROMAN.matcher(s).matches(); &#125;&#125; 自动装箱、拆箱（要优先使用基本类型而不是装箱基本类型，当心无意识的自动装箱） 123456private static long sum() &#123; Long sum = 0L; //使用long将更快 for (long i = 0; i &lt;= Integer.MAX_VALUE; i++) sum += i; return sum;&#125; 第七条：消除过期的对象引用12345678910111213141516171819202122232425// Can you spot the "memory leak"?public class Stack &#123; private Object[] elements; private int size = 0; private static final int DEFAULT_INITIAL_CAPACITY = 16; public Stack() &#123; elements = new Object[DEFAULT_INITIAL_CAPACITY]; &#125; public void push(Object e) &#123; ensureCapacity(); elements[size++] = e; &#125; public Object pop() &#123; if (size == 0) throw new EmptyStackException(); return elements[--size]; &#125; / ** * Ensure space for at least one more element, roughly * doubling the capacity each time the array needs to grow. */ private void ensureCapacity() &#123; if (elements.length == size) elements = Arrays.copyOf(elements, 2 * size + 1);&#125;&#125; 解决内存泄漏 1234567public Object pop() &#123; if (size == 0) throw new EmptyStackException(); Object result = elements[--size]; elements[size] = null; // Eliminate obsolete reference return result;&#125; 一般而言，只要类时自己管理内存，程序员就应该警惕内存泄漏问题，一旦元素被释放掉，则该元素中包含的任何对象引用都应该被清空。 第八条：覆盖equals时请遵守通用约定。 什么条件下，不需要覆盖equals: 类的每个实例本质上都是唯一的。代表活动实体而不是值的类来说确实如此。 不关心类是否提供了“逻辑相等“的测试功能。 超类已经覆盖了equals,从超类继承过来的行为对于子类也是合适的。 类是私有的或者包级私有的，可以确定它的equals方法永远不会被调用。 什么时候应该覆盖equals方法： 如果类具有自己特有的“逻辑相等”概念（不同于对象等同的概念），而且超类还没有覆盖equals以实现期望的行为。 覆盖时必须遵守的通用约定： 自反性：x.equals(x)返回true。 对称性：x.equals(y) == y.equals(x)。 传递性：x.equals(y) y.equals(z) x.equals(z)。 一致性：只要对象没有被修改，多次调用返回一致。 我们无法在扩展 可实例化的类的同时，既增加新的值组件，同时又保留equals约定 实现高质量equals方法的诀窍： 使用==操作符检查“参数是否为这个对象的引用”。 使用instanceof操作符检查参数是否为正确的类型。 把参数转换成正确的类型。 检查参数中的域是否与该对象中对应的域相匹配。对于既不是float也不是double类型的基本类型域，可以使用==操作符进行比较；对于对象引用域，可以递归的调用equals方法；对于float域，可以使用Float.compare方法，对于double域，则使用Double.compare。对于数组域，Arrays.equals()方法。]]></content>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cmder配置使用]]></title>
    <url>%2F2018%2F07%2F14%2Fcmder%E9%85%8D%E7%BD%AE%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[安装cmder官网 下载的时候，有两个版本，分别是mini与full版；唯一的差别在于有没有内建msysgit工具，这是Git for Windows的标准配备；全安装版 cmder 自带了 msysgit, 压缩包 23M, 除了 git 本身这个命令之外, 里面可以使用大量的 linux 命令；比如 grep, curl(没有 wget)； 像vim, grep, tar, unzip, ssh, ls, bash, perl 对于爱折腾的Coder更是痛点需求。 配置cmder 把 cmder 加到环境变量：可以把Cmder.exe存放的目录添加到系统环境变量；加完之后，Win+r一下输入cmder，即可。 添加 cmder 到右键菜单：在某个文件夹中打开终端，在管理员权限的终端输入以下语句即可： Cmder.exe /REGISTER ALL 常用快捷键 可以利用Tab，自动路径补全； 可以利用Ctrl+T建立新页签； 利用Ctrl+W关闭页签; 还可以透过Ctrl+Tab切换页签; Alt+F4：关闭所有页签 Alt+Shift+1：开启cmd.exe Alt+Shift+2：开启powershell.exe Alt+Shift+3：开启powershell.exe (系统管理员权限) Ctrl+1：快速切换到第1个页签 Ctrl+n：快速切换到第n个页签( n值无上限) Alt + enter： 切换到全屏状态； Ctr+r 历史命令搜索; End, Home, Ctrl : Traversing text with as usual on Windows]]></content>
      <tags>
        <tag>windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[js高级程序设计-读书笔记2]]></title>
    <url>%2F2018%2F05%2F04%2Fjs%E9%AB%98%E7%BA%A7%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B02%2F</url>
    <content type="text"><![CDATA[引用类型 Object类型 创建Object实例的方式有两种： 123456789var person = new Object(); person.name = "Nicholas"; person.age = 29; //字面量方式: var person = &#123; name : "Nicholas", age :29 &#125; &lt;!--more--&gt; 一般来说，可以使用点表示法来访问对象属性，但也可以使用[]来访问对象属性，应该将要访问的属性以字符串的形式放在[]中，[]方法的主要优点是可以通过变量来访问属性。 Array类型 创建方式： 1234567891011121314151617181920212223 var colors = new Array(); var colors = new Array(20); var colors = new Array("red","blue","green"); var colors = new Array(3);//创建一个包含三项的数组 var colors = new Array("Greg");//创建包含"Greg"一项的数组 var colors = Array();//new 可以省略 var colors = []; var colors = ["red","blue","green"] var colors = ["red","blue",]//不要这样，这样会创建一个包含两个或三个的数组 var colors = [,,,,,] //不要这样，这样会创建一个包含五个或六个的数组 var colors = ["red","blue","green"];colors.length = 2;alert(colors[2]) ; //undefined var colors = ["red","blue","green"];colors.length = 4;alert(colors[3]); //undefined //利用length可以很方便的在数组末尾添加新项:var colors = ["red","blue","green"];colors[colors.length] = "black";colors[colors.length] = "brown"; 数组的每一项都可以保存任何类型的数据。 检测数组: 123if(value instanceof Array)&#123; //对数组执行某些操作&#125; instanceof操作符的问题在于，它假定只有一个全局执行环境。如果网页中包含多个框架，那实际上就存在两个以上不同的全局执行环境，从而存在两个以上不同版本的Array构造函数。如果从一个框架向另一个框架传入数组，那么传入的数组与第二个框架原生创建的数组分别具有各自不同的构造函数，要解决这个问题，es5新增了Array.isArray()方法，这个方法的目的是最终确定某个值到底是不是数组，而不管他是在哪个全局执行环境中创建的。 123if(Array.isArray(value))&#123; //对数组执行某些操作&#125; 转换方法 所有对象都具有toLocalString()、toString()、valueOf()方法。调用数组的toString()方法会返回有数组中每个值的字符串形式拼接而成的一个以逗号分隔的字符串。而调用valueOf()返回的还是数组。实际上，为了创建这个字符串会调用数组每一项的toString()方法。如果数组中的某一项的值是null、undefined,那么该值在join()、toLocalString()、toString()、valueOf()返回的结果中以空字符串表示。 栈方法 push()可以接收任意数量的参数，把他们逐个添加到数组的末尾，并返回修改后数组的长度 pop()方法从数组末尾移除最后一项，减少数组的length值，返回移除的项 队列方法 由于push()是向数组末尾添加项的方法，因此要模拟队列只需一个从数组前端取得项的方法。实现这一操作的方法是shift(),他能够移除数组中的第一个项并返回该项，同时将数组长度减一。结合shift()和push()方法，可以像使用队列一样使用数组。 unshift()与shift()用途相反，它可以在数组前端添加任意项并返回新数组的长度。因此，同时使用unshift()和pop()方法，可以从相反的方向来模拟队列 排序 sort()方法会调用每个数组项的toString()转型方法，然后比较得到的字符串，以确定如何排序。即使数组中的每一项都是数值，sort方法比较的也是字符串，如下： 123var values = [0,1,5,10,15]values.sort()alert(values) //0,1,10,15,5 2. sort()方法可以接收一个比较函数作为参数，以便我们指定那个值位于那个值得前面。比较函数接收两个参数，如果第一个参数应该位于第二个之前则返回一个负数，如果两个参数相等则返回0，如何第一个参数应该位于第二个之后则返回一个正数。 12345678910111213141516function compare(v1,v2)&#123; if(v1 &lt; v2)&#123; return 1; &#125;else if(v1&gt;v2)&#123; return -1; &#125;else&#123; return 0; &#125;&#125;values.sort(compare); // 15,10,5,1,0//对于数值类型或者valueOf()方法会返回数值类型的对象类型，可以使用一个更简单的比较函数function compare(v1,v2)&#123; return v1 - v2;&#125; 操作方法 concat()该方法会先创建当前数组一个副本，然后将接收到的参数添加到这个副本的末尾，最后返回新构建的数组。在没有给concat()传递参数的情况下，它只是复制当前数组并返回副本。如果传递给concat()方法的是一个或多个数组，则该方法会将这些数组中的每一项都添加到结果数组中。如果传递的值不是数组，这些值就会简单的添加到结果数组的末尾。例如： 1234var colors = ['red','green','blue'];var colors2 = colors.cancat('yellow',['black','brown']);alert(colors) //red,green,bluealert(colors2) //red,green,blue,yellow,black,brown slice()基于当前数组中的一个或多个项创建一个新数组。slice()方法接收一或两个参数，即要返回项的起始和结束位置。在只有一个参数的情况下，slice()返回从该参数指定位置开始到当前数组末尾的所有项。如果有两个参数，该方法返回起始和结束位置之间的项，但不包括结束位置的项。slice()方法不会影响原始数组。 splice() 删除：可以删除任意数量的项，只需指定2个参数：要删除的第 一项的位置和要删除的项数 插入：可以向指定位置插入任意数量的项，只需提供三个参数：起始位置、0(要删除的项数)和要插入的项。如果要插入多个项，可以再传入第四、第五，一直任意多个项。 替换：可以向指定位置插入任意数量的项，且同时删除任意数量的项，只需指定3个参数：起始位置、要删除的项数和要插入的任意数量的项。插入的项不必与删除的项数相等。 123456789101112var colors = ['red','green','blue'];var removed = colors.splice(0,1);//删除第一项alert(colors) //green,bluealert(removed) //redremoved = colors.splice(1,0,'yellow','orange');//从位置1开始插入两项alert(colors) //green,yellow,orange,bluealert(removed) //返回的是一个空数组removed = colors.splice(1,1,'red','purple')alert(colors) //green,red,purple,orange,bluealert(removed) // yellow 位置方法: indexOf()从数组的开头开始向后查找 lastIndexOf()从数组的末尾开始向前查找 这两个方法都返回要查找的项在数组中的位置，在没有找到的情况下返回-1。在比较第一个参数和数组中的每一项时，会使用全等操作符 迭代方法：每个方法都接收两个参数：要在每一项上运行的函数和(可选的)运行该函数的作用域对象–影响this的值。传入这些方法的函数会接收三个参数：数组项的值，该项在数组中的位置和数组对象本身。 every():对数组中的每一项运行给定函数，如果该函数对每一项都返回true,则返回true. filter():对数组中的每一项运行给定函数，返回该函数会返回true的项组成的数组。 forEach():对数组中的每一项运行给定函数,没有返回值。 map():对数组中的每一项运行给定函数,返回每次函数调用的结果组成的数组。 some():对数组中的每一项运行给定函数,如果该函数对任一项返回true,则返回true。 1234var numbers = [1,2,3,4];var everyResult = numbers.every(function(item,index,array)&#123; return (item &gt;2)&#125;) 归并方法：这两个方法都接收两个参数：一个在每一项上调用的函数和(可选的)作为归并基础的初始值。传给reduce()和reduceRight()的函数接收四个参数：前一个值、当前值、项的索引和数组对象 reduce()迭代数组中的每一项，构建一个最终的返回值，从数组的第一项开始。 reduceRight()从数组的最后一项向前遍历。 12345var values = [1,2,3,4,5];var sum = values.reduce(function(prev,cur,index,array()&#123; return prev+cur; &#125;); alert(sum) //15 1]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>js高级程序设计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[js高级程序设计-读书笔记]]></title>
    <url>%2F2018%2F04%2F30%2Fjs%E9%AB%98%E7%BA%A7%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[&lt;script&gt;标签 无论如何包含代码，只要不存在defer和async属性，浏览器都会按照&lt;script&gt;元素在页面中出现的先后顺序对他们依次进行解析。 在文档的&lt;head&gt;元素中包含所有javascript标签，意味着必须等到全部的javascript代码都被下载、解析和执行完成后，才能开始呈现页面的内容（浏览器在遇到&lt;body&gt;标签时才开始呈现内容），所以，一般都这样写： 1234567891011121314&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=&lt;device-width&gt;, initial-scale=1.0&quot;&gt; &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;ie=edge&quot;&gt; &lt;title&gt;Document&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;!--这里放内容--&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;example1.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;example2.js&quot;&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; defer属性：脚本会被延迟到整个页面都解析完毕后再运行。因此，在&lt;script&gt;元素中设置defer属性，相当于告诉浏览器立即下载，但延迟执行。defer属性只适用于外部脚本文件。在下面的代码中，虽然&lt;script&gt;放在了文档&lt;head&gt;元素中，但其中包含的脚本将延迟到浏览器遇到&lt;/html&gt;标签后再执行。HTML5规范要求脚本按照他们出现的先后顺序执行，因此第一个延迟脚本会先于第二个延迟脚本执行，而这两个脚本会先于DOMContentLoaded事件执行。在现实中，延迟脚本并不一定会按照顺序执行，也不一定会在DOMContentLoaded事件触发前执行，因此最好只包含一个延迟脚本。 1234567891011121314&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=&lt;device-width&gt;, initial-scale=1.0&quot;&gt; &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;ie=edge&quot;&gt; &lt;title&gt;Document&lt;/title&gt; &lt;script type=&quot;text/javascript&quot; defer=&quot;defer&quot; src=&quot;example1.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot; defer=&quot;defer&quot; src=&quot;example2.js&quot;&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;!--这里放内容--&gt;&lt;/body&gt;&lt;/html&gt; HTML5为&lt;script&gt;元素定义了async属性。async属性告诉浏览器立即下载文件，但并不保证按照他们的先后顺序执行。 1234567891011121314&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=&lt;device-width&gt;, initial-scale=1.0&quot;&gt; &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;ie=edge&quot;&gt; &lt;title&gt;Document&lt;/title&gt; &lt;script type=&quot;text/javascript&quot; async src=&quot;example1.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot; async src=&quot;example2.js&quot;&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;!--这里放内容--&gt;&lt;/body&gt;&lt;/html&gt; 在以上代码中，第二个脚本可能会在第一个脚本之前执行，因此，确保两者之间互不依赖非常重要。指定async属性的目的是不让页面等待两个脚本下载和执行，从而异步加载页面其他内容。为此，建议异步脚本不要在加载期间修改DOM。 数据类型 五种基本数据类型：Undefined、Null、Boolean、Number、String;一种复杂数据类型：Object。 在使用var声明变量但未对其加以初始化时，这个变量的值就是undefined。 12345var message;//这个变量声明之后默认取得了undefined值//下面这个变量并没有声明//var agealert(typeof message); //&quot;undefined&quot;alert(typeof age); //&quot;undefined&quot; 对未声明和未初始化的变量执行typeof操作符都返回了undifined值。 如果定义的变量准备在将来用于保存对象，那么最好将该变量初始化为null而不是其他值。这样一来，只要直接检查null值就可以知道相应的变量是否已经保存了一个对象的引用，如下： 12345if(car != null)&#123; //对car执行某些操作&#125;//实际上，undefined的值是派生自null值的alert(null == undefined) //true NaN,即非数值(Not a Number)是一个特殊的数值，这个数值用来表示一个本来要返回数值的操作数未返回数值的情况（这样就不会抛出错误了）。任何数值除以0会返回NaN；任何涉及NaN的操作（NaN/10）都会返回NaN;NaN与任何值都不相等，包括NaN本身。 执行环境及作用域 执行环境(execution context)是javaScript中最为重要的一个概念。执行环境定义了变量或函数有权访问的其他数据，决定了他们各自的行为。每个执行环境都有一个与之关联的变量对象(variable object)，环境中定义的所有变量和函数都保存在这个对象中。虽然我们编写的代码无法访问这个对象，但解析器在处理数据时会在后台使用它。 全局执行环境是最外围的一个执行环境。根据宿主环境不同，执行环境的对象也不一样。在web浏览器中，全局执行环境被认为是window对象，因此所有全局变量和函数都是作为window对象的属性和方法创建的。 每个函数都有自己的执行环境。当执行流进入一个函数时，函数的环境就会被推入一个环境栈中。而在函数执行之后，栈将其环境弹出，把控制权返回给之前的执行环境。 当代码在一个环境中执行时，会创建变量对象的一个作用域链(scope chain)。作用域链的用途，是保证对执行环境有权访问的所有变量和函数的有序访问。作用域链的前端，始终都是当前执行的代码所在环境的变量对象。如果这个环境是函数，则将其活动对象(activation object)作为变量对象。活动对象在最开始时只包含一个变量，即arguments对象(这个对象在全局环境中是不存在的)。作用域链中的下一个变量对象来自包含环境，而再下一个变量对象则来自下一个包含环境。这样，一直延续到全局执行环境；全局执行环境的变量对象始终都是作用域链中的最后一个对象。 ​]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>js高级程序设计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式-职责链模式]]></title>
    <url>%2F2018%2F04%2F06%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E8%81%8C%E8%B4%A3%E9%93%BE%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[定义避免请求发送者与接收者 耦合在一起，让多个对象都有可能接收请求，将这些对象连接成一条链，并且沿着这条链传递请求，直到有对象处理它为止。职责链模式是一种对象行为型模式。 UML Handler(抽象处理者):它定义了一个处理请求的接口，一般设计为抽象类，由于不同的具体处理者处理请求的方式不同，因此在其中定义了抽象请求处理方法。因为每一个处理者 的下家还是一个处理者，因此在抽象处理者中定义了一个抽象处理者类型的对象(如结构图 中的successor)，作为其对下家的引用。通过该引用，处理者可以连成一条链。 ConcreteHandler(具体处理者):它是抽象处理者的子类，可以处理用户请求，在具体处理者类中实现了抽象处理者中定义的抽象请求处理方法，在处理请求之前需要进行判断，看是 否有相应的处理权限，如果可以处理请求就处理它，否则将请求转发给后继者;在具体处理 者中可以访问链中下一个对象，以便请求的转发。 总结 职责链模式的主要优点如下: 职责链模式使得一个对象无须知道是其他哪一个对象处理其请求，对象仅需知道该请求会被处理即可，接收者和发送者都没有对方的明确信息，且链中的对象不需要知道链的结构，由客户端负责链的创建，降低了系统的耦合度。 请求处理对象仅需维持一个指向其后继者的引用，而不需要维持它对所有的候选处理者的引用，可简化对象的相互连接。 在给对象分派职责时，职责链可以给我们更多的灵活性，可以通过在运行时对该链进行动态的增加或修改来增加或改变处理一个请求的职责。 在系统中增加一个新的具体请求处理者时无须修改原有系统的代码，只需要在客户端重新建链即可，从这一点来看是符合“开闭原则”的。 职责链模式的主要缺点如下: 由于一个请求没有明确的接收者，那么就不能保证它一定会被处理，该请求可能一直到链的末端都得不到处理;一个请求也可能因职责链没有被正确配置而得不到处理。 对于比较长的职责链，请求的处理可能涉及到多个处理对象，系统性能将受到一定影响，而且在进行代码调试时不太方便。 如果建链不当，可能会造成循环调用，将导致系统陷入死循环。 适用场景 有多个对象可以处理同一个请求，具体哪个对象处理该请求待运行时刻再确定，客户端只需将请求提交到链上，而无须关心请求的处理对象是谁以及它是如何处理的。 在不明确指定接收者的情况下，向多个对象中的一个提交一个请求。 可动态指定一组对象处理请求，客户端可以动态创建职责链来处理请求，还可以改变链中处理者之间的先后次序。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式-装饰模式]]></title>
    <url>%2F2018%2F04%2F06%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E8%A3%85%E9%A5%B0%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[定义动态地给一个对象增加一些额外的职责，就增加对象功能来说，装饰模式比生成子类实现更为灵活。装饰模式是一种对象结构型模式。 UML Component(抽象构件):它是具体构件和抽象装饰类的共同父类，声明了在具体构件中实现的业务方法，它的引入可以使客户端以一致的方式处理未被装饰的对象以及装饰之后的对象，实现客户端的透明操作。 ConcreteComponent(具体构件):它是抽象构件类的子类，用于定义具体的构件对象，实现了在抽象构件中声明的方法，装饰器可以给它增加额外的职责(方法)。 Decorator(抽象装饰类):它也是抽象构件类的子类，用于给具体构件增加职责，但是具体职责在其子类中实现。它维护一个指向抽象构件对象的引用，通过该引用可以调用装饰之前构件对象的方法，并通过其子类扩展该方法，以达到装饰的目的。 ConcreteDecorator(具体装饰类):它是抽象装饰类的子类，负责向构件添加新的职责。每一个具体装饰类都定义了一些新的行为，它可以调用在抽象装饰类中定义的方法，并可以增加新的方法用以扩充对象的行为。 总结 装饰模式的主要优点如下: 对于扩展一个对象的功能，装饰模式比继承更加灵活性，不会导致类的个数急剧增加。 可以通过一种动态的方式来扩展一个对象的功能，通过配置文件可以在运行时选择不同的具体装饰类，从而实现不同的行为。 可以对一个对象进行多次装饰，通过使用不同的具体装饰类以及这些装饰类的排列组合，可以创造出很多不同行为的组合，得到功能更为强大的对象。 具体构件类与具体装饰类可以独立变化，用户可以根据需要增加新的具体构件类和具体装饰类，原有类库代码无须改变，符合“开闭原则”。 装饰模式的主要缺点如下: 使用装饰模式进行系统设计时将产生很多小对象，这些对象的区别在于它们之间相互连接的方式有所不同，而不是它们的类或者属性值有所不同，大量小对象的产生势必会占用更多 的系统资源，在一定程序上影响程序的性能。 装饰模式提供了一种比继承更加灵活机动的解决方案，但同时也意味着比继承更加易于出错，排错也很困难，对于多次装饰的对象，调试时寻找错误可能需要逐级排查，较为繁琐。 适用场景 在不影响其他对象的情况下，以动态、透明的方式给单个对象添加职责。 当不能采用继承的方式对系统进行扩展或者采用继承不利于系统扩展和维护时可以使用装饰模式。不能采用继承的情况主要有两类:第一类是系统中存在大量独立的扩展，为支持每一种扩展或者扩展之间的组合将产生大量的子类，使得子类数目呈爆炸性增长;第二类是因 为类已定义为不能被继承(如Java语言中的final类)。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式-建造者模式]]></title>
    <url>%2F2018%2F04%2F06%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%BB%BA%E9%80%A0%E8%80%85%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[定义将一个复杂对象的构建与它的表示分离，使得同样的构建过程可以创建不同的表示。建造者模式是一种对象创建型模式。 UML Builder(抽象建造者):它为创建一个产品Product对象的各个部件指定抽象接口，在该接口中一般声明两类方法，一类方法是buildPartX()，它们用于创建复杂对象的各个部件;另一 类方法是getResult()，它们用于返回复杂对象。Builder既可以是抽象类，也可以是接口。 ConcreteBuilder(具体建造者):它实现了Builder接口，实现各个部件的具体构造和装配方 法，定义并明确它所创建的复杂对象，也可以提供一个方法返回创建好的复杂产品对象。 Product(产品角色):它是被构建的复杂对象，包含多个组成部件，具体建造者创建该产品 的内部表示并定义它的装配过程。 Director(指挥者):指挥者又称为导演类，它负责安排复杂对象的建造次序，指挥者与抽象建造者之间存在关联关系，可以在其construct()建造方法中调用建造者对象的部件构造与装 配方法，完成复杂对象的建造。客户端一般只需要与指挥者进行交互，在客户端确定具体建 造者的类型，并实例化具体建造者对象(也可以通过配置文件和反射机制)，然后通过指挥 者类的构造函数或者Setter方法将该对象传入指挥者类中。 总结 建造者模式的主要优点如下: 在建造者模式中，客户端不必知道产品内部组成的细节，将产品本身与产品的创建过程解耦，使得相同的创建过程可以创建不同的产品对象。 每一个具体建造者都相对独立，而与其他的具体建造者无关，因此可以很方便地替换具体建造者或增加新的具体建造者，用户使用不同的具体建造者即可得到不同的产品对象。由于指挥者类针对抽象建造者编程，增加新的具体建造者无须修改原有类库的代码，系统扩展方便，符合“开闭原则” 可以更加精细地控制产品的创建过程。将复杂产品的创建步骤分解在不同的方法中，使得创建过程更加清晰，也更方便使用程序来控制创建过程。 建造者模式的主要缺点如下: 建造者模式所创建的产品一般具有较多的共同点，其组成部分相似，如果产品之间的差异性很大，例如很多组成部分都不相同，不适合使用建造者模式，因此其使用范围受到一定的限制。 如果产品的内部变化复杂，可能会导致需要定义很多具体建造者类来实现这种变化，导致系统变得很庞大，增加系统的理解难度和运行成本。 适用场景 需要生成的产品对象有复杂的内部结构，这些产品对象通常包含多个成员属性。 需要生成的产品对象的属性相互依赖，需要指定其生成顺序。 对象的创建过程独立于创建该对象的类。在建造者模式中通过引入了指挥者类，将创建过程封装在指 挥者类中，而不在建造者类和客户类中。 隔离复杂对象的创建和使用，并使得相同的创建过程可以创建不同的产品。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式-原型模式]]></title>
    <url>%2F2018%2F04%2F01%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%8E%9F%E5%9E%8B%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[定义使用原型实例指定创建对象的种类，并且通过拷贝这些原型创建新的对象。原型模式是一种对象创建型模式。 UML Prototype(抽象原型类):它是声明克隆方法的接口，是所有具体原型类的公共父类，可以 是抽象类也可以是接口，甚至还可以是具体实现类。 ConcretePrototype(具体原型类):它实现在抽象原型类中声明的克隆方法，在克隆方法中返回自己的一个克隆对象。 Client(客户类):让一个原型对象克隆自身从而创建一个新的对象，在客户类中只需要直 接实例化或通过工厂方法等方式创建一个原型对象，再通过调用该对象的克隆方法即可得到多个相同的对象。由于客户类针对抽象原型类Prototype编程，因此用户可以根据需要选择具体 原型类，系统具有较好的可扩展性，增加或更换具体原型类都很方便。 总结 原型模式的主要优点如下: (1) 当创建新的对象实例较为复杂时，使用原型模式可以简化对象的创建过程，通过复制一个已有实例可以提高新实例的创建效率。 (2) 扩展性较好，由于在原型模式中提供了抽象原型类，在客户端可以针对抽象原型类进行编程，而将具体原型类写在配置文件中，增加或减少产品类对原有系统都没有任何影响。 (3) 原型模式提供了简化的创建结构，工厂方法模式常常需要有一个与产品类等级结构相同的 工厂等级结构，而原型模式就不需要这样，原型模式中产品的复制是通过封装在原型类中的 克隆方法实现的，无须专门的工厂类来创建产品。 (4) 可以使用深克隆的方式保存对象的状态，使用原型模式将对象复制一份并将其状态保存起 来，以便在需要的时候使用(如恢复到某一历史状态)，可辅助实现撤销操作。 原型模式的主要缺点如下: (1) 需要为每一个类配备一个克隆方法，而且该克隆方法位于一个类的内部，当对已有的类进 行改造时，需要修改源代码，违背了“开闭原则”。 (2) 在实现深克隆时需要编写较为复杂的代码，而且当对象之间存在多重的嵌套引用时，为了实现深克隆，每一层对象对应的类都必须支持深克隆，实现起来可能会比较麻烦。 适用场景(1) 创建新对象成本较大(如初始化需要占用较长的时间，占用太多的CPU资源或网络资 源)，新的对象可以通过原型模式对已有对象进行复制来获得，如果是相似对象，则可以对 其成员变量稍作修改。(2) 如果系统要保存对象的状态，而对象的状态变化很小，或者对象本身占用内存较少时，可以使用原型模式配合备忘录模式来实现。(3) 需要避免使用分层次的工厂类来创建分层次的对象，并且类的实例对象只有一个或很少的几个组合状态，通过复制原型对象得到新实例可能比使用构造函数创建一个新实例更加方便。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[effective java读书笔记]]></title>
    <url>%2F2017%2F09%2F27%2Feffective-java%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[第52条：通过接口引用对象如果有合适的接口类型存在，那么对于参数、返回值、变量和成员变量来说，就都应该使用接口类型进行声明；如果你养成了用接口作为类型的习惯，你的程序将会更加灵活，当你决定更换实现时，所要做的就只是构造器中类的名称。有一点值得注意：如果原来的实现提供了某种特殊的功能，而这种功能并不是这个接口的通用约定所要求的，并且周围的代码又依赖于这种功能，那么新的实现也要提供相同的功能。类实现了接口，但是提供了接口中不存在的额外的方法–例如LinkedHashMap。如果程序依赖于这些额外的方法，这种类就应该只被用来引用它的实例。实际上，给定的对象是否具有适当的接口应该是很显然的。如果是，用接口引用对象就会使程序更加灵活；如果不是，则使用类层次结构中提供了必要功能的最基础的类。 第38条：检查参数的有效性每当编写方法或者构造器的时候，应该考虑它的参数有哪些限制，应该把这些限制写到文档中，并且在这个方法体的开头处，通过显式的检查来实施这些限制 第43条：返回0长度的数组或者集合，而不是Null返回类型为数组或集合的方法没理由返回null，应该返回一个零长度的数组或者集合。可以做成在每当需要返回空集合时，都返回同一个不可变的空集合。Collections.emptySet、emptyList、emptyMap提供的正是这种集合。public List&lt;Person&gt; getPersons(){ if(persons.isEmpty()){ return Collections.emptyList(); }else{ return persons; } } 第46条：for-each循环优先于传统的for循环for-each在简洁性和预防bug方面有着传统的for循环无法比拟的优势，并且没有性能损失，应该尽可能的使用for-each循环。有三种常见的情况无法使用for-each循环: 过滤:如果需要遍历集合，并删除选定的元素，就需要使用显示的迭代器，以便可以调用它的remove方法。 转换：如果需要遍历列表或者数组，并取代它部分或者全部的元素值，就需要列表迭代器或者数组索引，以便设定元素的值。 平行迭代：如果需要并行的遍历多个集合，就需要显示的控制迭代器或者索引变量，以便所有迭代器或者索引变量都可以得到同步前移。第48条：如果需要精确的答案，请避免使用float和double第49条：基本类型优先于装箱基本类型基本类型和装箱基本类型之间有三个主要的区别： 基本类型只有值，而装箱基本类型则具有与它们的值不同的地址。换句话说，两个装箱基本类型可以具有相同的值和不同的地址。 基本类型只有功能完备的值，而每个装箱基本类型除了它对应的基本类型的所有功能值外，还有个非功能值：null 基本类型通常比装箱基本类型更节省时间和空间. 1234567public static void main(String[] args)&#123; Long sum = 0L; for(long i=0;i&lt;Integer.MAX_VALUE;i++)&#123; sum += i; &#125; System.out.println(sum);&#125; 这个程序运行起来比预计的要慢一些，因为它不小心将一个局部变量(sum)声明为装箱基本类型Long,而不是基本类型long,变量被反复的装箱和拆箱，导致明显的性能下降。总之，当可以选择的时候，基本类型要优先于装箱基本类型。]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>effective-java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式-适配器模式]]></title>
    <url>%2F2017%2F09%2F20%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E9%80%82%E9%85%8D%E5%99%A8%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[定义将一个接口转换成客户希望的另一个接口，使接口不兼容的哪些类可以一起工作，其别名为包装器（Wrapper）。 UML根据适配器类与适配者类的关系不同，适配器模式可分为对象 适配器和类适配器两种，在对象适配器模式中，适配器与适配者之间是关联关系;在类适配 器模式中，适配器与适配者之间是继承(或实现)关系。在实际开发中，对象适配器的使用 频率更高,对象适配器结构如下: Target(目标抽象类):目标抽象类定义客户所需接口，可以是一个抽象类或接口也可以是具体类。 Adapter(适配器类):适配器可以调用另一个接口，作为一个转换器，对Adaptee和Target进行适配，适配器类是适配器模式的核心，在对象适配器中，它通过继承Target并关联一个 Adaptee对象使二者产生联系。 Adaptee(适配者类):适配者即被适配的角色，它定义了一个已经存在的接口，这个接口需要适配，适配者类一般是一个具体类，包含了客户希望使用的业务方法，在某些情况下可 能没有适配者类的源代码。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式-工厂方法模式]]></title>
    <url>%2F2017%2F09%2F16%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%B7%A5%E5%8E%82%E6%96%B9%E6%B3%95%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[定义与简单工厂模式不同，在工厂方法模式中，不再提供一个统一的工厂类来创建所有的产品对象，而是针对不同的产品提供不同的工厂，系统提供一个与产品等级结构对应的工厂等级结构。 UML Product(抽象产品):它是定义产品的接口，是工厂方法模式所创建对象的超类型，也就是产品对象的公共父类。 ProductA(具体产品):它实现了抽象产品接口，某种类型的具体产品由专门的具体工厂创建，具体工厂和具体产品之间一一对应。 Factory(抽象工厂):在抽象工厂类中，声明了工厂方法（FactoryMethod)，用于返回一个产品。抽象工厂是工厂方法模式的核心，所有创建对象的工厂类都必须实现该接口。 FactoryA(具体工厂):它是抽象工厂类的子类，实现了抽象工厂中定义的工厂方法，并可由客户端调用，返回一个具体产品类的实例。 总结工厂方法模式的主要优点如下: 用户只需要关心所需产品对应的工厂，不需要关心创建细节。 系统中加入新产品时，无需修改抽象工厂和抽象产品类，只需要添加具体的产品类和工厂类，符合开闭原则。 工厂方法模式的主要缺点如下: 增加产品时，需要创建具体的产品类和具体的工厂类，增加了系统的复杂度。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jvm学习笔记]]></title>
    <url>%2F2017%2F06%2F17%2Fjvm%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[java虚拟机与程序的生命周期在如下几种情况下，java虚拟机将结束生命周期： 执行了System.exit()方法 程序正常执行结束 程序在执行过程中遇到了异常或错误而异常终止 由于操作系统出现错误而导致java虚拟机进程结束 加载.class文件的方式: 从本地系统中直接加载 通过网络下载.class文件 从zip,jar等归档文件中加载.class文件 从专有数据库中提取.class文件 将java原文件动态编译为.class文件 线程运行诊断 Cpu 占用过多 用top定位那个进程对cpu的占用过高。 ps H -eo pid,tid,%cpu | grep 进程id(用ps进一步定位是那个线程引起的cpu占用过高) Jstack 进程id 可以根据线程id找到问题的线程，进一步定位问题代码的源码行数。 堆内存 jps 查看当前系统中有哪些java进程 Jmap 查看堆内存占用情况 jmap -heap 进程id Jconsole 图形界面 StringTable特性 常量池中的字符串仅是符号，第一次用到时才变成对象。 利用串池的机制，来避免重复创建字符串对象。 字符串拼接的原理是StringBuilder(1.8) 字符串常量拼接的原理是编译器优化。 可以使用intern方法，主动将串池中还没有的字符串放入串池 1.8将这个字符串对象尝试放入串池，如果有则并不会放入，如果没有则放入串池；两种情况都会把串池中的对象返回。 1.6将这个字符串对象尝试放入串池，如果有则并不会放入，如果没有会把此对象复制一份放入串池；两种情况都会把串池中的对象返回。 性能调优 调整-XX:StringTableSize=桶个数 考虑将字符串对象是否入池 四种引用 强引用 只有所用GC Roots 对象都不强引用该对象，该对象才能被垃圾回收。 软引用(SoftReference) 仅有软引用引用该对象时，在垃圾回收后，内存仍不足时会再次发出垃圾回收，回收软引用对象 可以配合引用队列释放软引用对象自身 1234567891011121314151617181920212223//-Xmx20m -XX:+PrintGCDetails -verbose:gcpublic static void soft()&#123; List&lt;SoftReference&lt;byte[]&gt;&gt; list = new ArrayList&lt;&gt;(); ReferenceQueue&lt;byte[]&gt; queue = new ReferenceQueue(); for (int i = 0; i &lt; 5; i++) &#123; //关联了引用队列，当软引用关联的byte[]被回收时，软引用自己会加入到queue中去 SoftReference&lt;byte[]&gt; ref = new SoftReference&lt;&gt;(new byte[_4MB],queue); System.out.println(ref.get()); list.add(ref); System.out.println(list.size()); &#125; //从队列中获取无用的软引用对象并移除 java.lang.ref.Reference&lt;? extends byte[]&gt; ref = queue.poll(); while (ref != null)&#123; list.remove(ref); ref = queue.poll(); &#125; System.out.println("循环结束："+list.size()); System.out.println("============================"); for (SoftReference&lt;byte[]&gt; softReference : list) &#123; System.out.println(softReference.get()); &#125; &#125; 弱引用(WeakReference) 只有弱引用引用该对象时，在垃圾回收时，无论内存是否充足，都会回收弱引用对象 可以配合引用队列释放弱引用对象自身 12345678910111213141516171819202122public static void weak()&#123; List&lt;WeakReference&lt;byte[]&gt;&gt; list = new ArrayList&lt;&gt;(); //ReferenceQueue&lt;byte[]&gt; queue = new ReferenceQueue(); for (int i = 0; i &lt; 6; i++) &#123; //关联了引用队列，当软引用关联的byte[]被回收时，软引用自己会加入到queue中去 WeakReference&lt;byte[]&gt; ref = new WeakReference&lt;&gt;(new byte[_4MB]); System.out.println(ref.get()); list.add(ref); for (WeakReference&lt;byte[]&gt; weakReference : list) &#123; System.out.println(weakReference.get()+" "); &#125; System.out.println(list.size()); &#125; //从队列中获取无用的软引用对象并移除// java.lang.ref.Reference&lt;? extends byte[]&gt; ref = queue.poll();// while (ref != null)&#123;// list.remove(ref);// ref = queue.poll();// &#125; System.out.println("循环结束："+list.size()); System.out.println("============================"); &#125; 虚引用(PhantomReference) 必须配合引用队列使用，主要配合ByteBuffer使用，被引用对象回收时，会将虚引用入队，由Reference Handler 线程调用虚引用相关方法释放直接内存。 终结器引用(FinalReference) 无需手动编码，但其内部配合引用队列使用，在垃圾回收时，终结器引用入队（被引用对象暂时没有被回收），再由Finalizer线程通过终结器引用找到被引用对象并调用他的finalize方法，第二次GC时才能回收被引用对 垃圾回收 相关VM参数 | 含义 | 参数 || :—————-: | :——————————————————–: || 堆初始大小 | -Xms || 堆最大大小 | -Xmx或-XX:MaxHeapSize=size || 新生代大小 | -Xmn或(-XX:NewSize=size + -XX:MaxNewSize=size) || 幸存区比例（动态） | -XX:InitialSurvivorRatio=ratio和-XX:+UseAdaptiveSizePolicy || 幸存区比例 | -XX:SurvivorRatio=ratio || 晋升阈值 | -XX:MaxTenuringThreshold=threshold || 晋升详情 | -XX:+PrintTenuringDistribution || GC详情 | -XX:+PrintGCDetails -verbose:gc || FullGC前minorGC | -XX:+ScavengeBeforeFullGC | 特性 大对象直接放到老年代。 线程中的OOM不会引起java进程的结束。 1234567891011121314151617181920public class Gc &#123; private static final int _512KB = 512 * 1024; private static final int _1M = 1 * 1024 * 1024; private static final int _6M = 6 * 1024 * 1024; private static final int _7M = 7 * 1024 * 1024; private static final int _8M = 8 * 1024 * 1024; public static void main(String[] args) throws InterruptedException &#123; new Thread(new Runnable() &#123; @Override public void run() &#123; List&lt;byte[]&gt; list = new ArrayList&lt;&gt;(); list.add(new byte[_8M]); list.add(new byte[_8M]); &#125; &#125;).start(); System.out.println("主线程睡一秒钟"); Thread.sleep(1000l); &#125;&#125; 垃圾回收器 串行 单线程 堆内存较小，适合个人电脑 -XX:+ 吞吐量优先 多线程 堆内存较大，多核CPU 单位时间内让STW的时间最短 12345-XX:+UseParallelGC（开启后会自动使用parallelOldGC） -XX:+UseParallelOldGC-XX:ParallelGCThreads=n (设置垃圾回收线程数，默认和cpu核数相同)-XX:UseAdaptiveSizePolicy 自适应调整新生代大小 -XX:GCTimeRatio=ratio 1/1+ratio(默认为99，一般设置为19，表示100分钟内允许5分钟的时间可以用来进行垃圾回收) 为垃圾回收时间所占的比例-XX:MaxGCPauseMillis=ms 每次垃圾回收使用的最大时间 响应时间优先 多线程 堆内存较大，多核CPU 尽可能让STW的单次时间最短 123456-XX:+UseConcMarkSweepGC(老年代) --&gt; 如果失败，会退化成SerialOld垃圾回收器 -XX:UseParNewGC (新生代) -XX:ParallelGCThreads=n (设置垃圾回收线程数，默认和cpu核数相同) -XX:ConcGCThreads=n/4 -XX:CMSInitiatingOccupancyFraction=percent 执行垃圾回收的内存占比 -XX:+CMSScavengeBeforeRemark * G1(Garbage First 取代cms回收器) * 2017年JDK9 默认的垃圾回收器 * 适用场景 * 同时注重吞吐量和低延迟，默认的暂停目标是200ms * 超大堆内存，会将堆划分为多个大小相等的Region * 整体上是标记+整理算法，两个区域之间是复制算法 * 配置 123-XX:+UseG1GC-XX:G1HeapRegionSize=size-XX:MaxGCPauseMillis=time 垃圾回收调优新生代调优 新生代特点 所有的new操作的内存分配非常廉价 TLAB thread-local allocation buffer 死亡对象的回收代价为0 大部分对象用过即死 Minor GC 的时间远远低于Full GC 调优 新生代能容纳所有[并发量*(请求-响应)]的数据 class文件结构参考文档 123456789101112131415161718ClassFile &#123; u4 magic; u2 minor_version; u2 major_version; u2 constant_pool_count; cp_info constant_pool[constant_pool_count-1]; u2 access_flags; u2 this_class; u2 super_class; u2 interfaces_count; u2 interfaces[interfaces_count]; u2 fields_count; field_info fields[fields_count]; u2 methods_count; method_info methods[methods_count]; u2 attributes_count; attribute_info attributes[attributes_count];&#125; 执行流程 123456789package test;public class HelloWorld &#123; public static void main(String[] args) &#123; int a = 10; int b = Short.MAX_VALUE + 1; int c = a + b; System.out.println(c); &#125;&#125; javap 查看字节码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293Classfile /Users/haominglfs/IdeaProjects/leetcode/out/production/leetcode/test/HelloWorld.class Last modified 2020-1-10; size 602 bytes MD5 checksum 470a732b94645485adf09a48f8e9bdd2 Compiled from "HelloWorld.java"public class test.HelloWorld minor version: 0 major version: 52 flags: ACC_PUBLIC, ACC_SUPERConstant pool: #1 = Methodref #7.#25 // java/lang/Object."&lt;init&gt;":()V #2 = Class #26 // java/lang/Short #3 = Integer 32768 #4 = Fieldref #27.#28 // java/lang/System.out:Ljava/io/PrintStream; #5 = Methodref #29.#30 // java/io/PrintStream.println:(I)V #6 = Class #31 // test/HelloWorld #7 = Class #32 // java/lang/Object #8 = Utf8 &lt;init&gt; #9 = Utf8 ()V #10 = Utf8 Code #11 = Utf8 LineNumberTable #12 = Utf8 LocalVariableTable #13 = Utf8 this #14 = Utf8 Ltest/HelloWorld; #15 = Utf8 main #16 = Utf8 ([Ljava/lang/String;)V #17 = Utf8 args #18 = Utf8 [Ljava/lang/String; #19 = Utf8 a #20 = Utf8 I #21 = Utf8 b #22 = Utf8 c #23 = Utf8 SourceFile #24 = Utf8 HelloWorld.java #25 = NameAndType #8:#9 // "&lt;init&gt;":()V #26 = Utf8 java/lang/Short #27 = Class #33 // java/lang/System #28 = NameAndType #34:#35 // out:Ljava/io/PrintStream; #29 = Class #36 // java/io/PrintStream #30 = NameAndType #37:#38 // println:(I)V #31 = Utf8 test/HelloWorld #32 = Utf8 java/lang/Object #33 = Utf8 java/lang/System #34 = Utf8 out #35 = Utf8 Ljava/io/PrintStream; #36 = Utf8 java/io/PrintStream #37 = Utf8 println #38 = Utf8 (I)V&#123; public test.HelloWorld(); descriptor: ()V flags: ACC_PUBLIC Code: stack=1, locals=1, args_size=1 0: aload_0 1: invokespecial #1 // Method java/lang/Object."&lt;init&gt;":()V 4: return LineNumberTable: line 2: 0 LocalVariableTable: Start Length Slot Name Signature 0 5 0 this Ltest/HelloWorld; public static void main(java.lang.String[]); descriptor: ([Ljava/lang/String;)V flags: ACC_PUBLIC, ACC_STATIC Code: stack=2, locals=4, args_size=1 0: bipush 10 2: istore_1 3: ldc #3 // int 32768 5: istore_2 6: iload_1 7: iload_2 8: iadd 9: istore_3 10: getstatic #4 // Field java/lang/System.out:Ljava/io/PrintStream; 13: iload_3 14: invokevirtual #5 // Method java/io/PrintStream.println:(I)V 17: return LineNumberTable: line 4: 0 line 5: 3 line 6: 6 line 7: 10 line 8: 17 LocalVariableTable: Start Length Slot Name Signature 0 18 0 args [Ljava/lang/String; 3 15 1 a I 6 12 2 b I 10 8 3 c I&#125;SourceFile: "HelloWorld.java" 图解分析 加载常量池到运行时常量池,方法字节码加载到方法区 bipush 10 将一个byte压入操作数栈(其长度会补齐4个字节) sipush将一个short压入操作数栈 ldc 将一个int压入操作数栈 Ldc2_w 将一个long压入操作数栈(分两次压入，因为long是两个字节) 这里小的数字都是和字节码指令存在一起，超过short范围的数字存入常量池 Istore_1 将操作数栈顶数据弹出，存入局部变量表的slot1 ldc #3 从常量池加载#3数据到操作数栈 istore_2 load_1 Iload_2 iadd Istore_3 getstatic #4 将堆中的System.out对象的引用压入操作数栈 iload_3 Invokevirtual #5 找到常量池第五项 定位到方法区java/io/PrintStream.println:(I)V方法 生成新的栈帧(生成locals、stack等) 传递参数，执行新栈帧中的字节码 执行完毕，弹出栈帧 清除main操作数栈内容 return 完成main方法调用，弹出main栈帧 程序结束 构造方法 ()V 1234567static int i = 10;static &#123; i = 20;&#125;static &#123; i = 30;&#125; 编译器会按照从上至下的顺序，收集所有static静态代码块和静态成员赋值的代码，合并为一个特殊的方法()V 12345670: bipush 102: putstatic #2 // Field i:I5: bipush 207: putstatic #2 // Field i:I10: bipush 3012: putstatic #2 // Field i:I15: return ()V方法会在类加载的初始化阶段被调用 ()V 编译器会按照从上至下的顺序，收集所有{}代码块和成员变量赋值的代码，形成新的构造方法，但原始构造方法内的代码总是在最后 方法调用 12345678910111213141516public class Demo &#123; public Demo()&#123;&#125; private void test1()&#123;&#125; private final void test2()&#123;&#125; public void test3()&#123;&#125; public static void test4()&#123;&#125; public static void main(String[] args) &#123; Demo d = new Demo(); d.test1(); d.test2(); d.test3(); d.test4(); Demo.test4(); &#125;&#125; 1234567891011121314150: new #2 // class test/Demo 堆空间分配内存，引用放入操作数栈3: dup //复制栈顶引用4: invokespecial #3 // Method "&lt;init&gt;":()V 调用构造方法7: astore_18: aload_19: invokespecial #4 // Method test1:()V12: aload_113: invokespecial #5 // Method test2:()V16: aload_117: invokevirtual #6 // Method test3:()V20: aload_121: pop //入栈又出栈，通过对象调用静态方法会产生两次不必要的指令22: invokestatic #7 // Method test4:()V25: invokestatic #7 // Method test4:()V28: return 构造方法、私有方法、final方法使用invokespecial；普通public方法使用invokevirtual(动态绑定)；静态方法使用invokestatic 多态 当执行invokevirtual指令时 先通过栈帧中的对象引用找到对象 分析对象头，找到对象的实际Class class结构中有vtable,他在类加载的链接阶段就已经根据方法的重写规则生成好了 查表得到方法的具体地址 执行方法的字节码 异常 12345678910public class Demo &#123; public static void main(String[] args) &#123; int i = 0; try &#123; i = 10; &#125;catch (Exception e)&#123; i = 20; &#125; &#125;&#125; 12345678910111213141516170: iconst_01: istore_12: bipush 104: istore_15: goto 128: astore_29: bipush 2011: istore_112: return Exception table:from to target type 2 5 8 Class java/lang/Exception LocalVariableTable:Start Length Slot Name Signature 9 3 2 e Ljava/lang/Exception; 0 13 0 args [Ljava/lang/String; 2 11 1 i I 可以看出多出来一个Exception table的结构，[from,to)是前闭后开的监测范围，一旦这个范围内的字节码执行出现异常，则通过type匹配异常类型，如果一致，进入target所指示行号。 8行的字节码指令astore_2是将异常对象引用存入局部变量表的slot 2 位置。 类加载加载步骤 加载 链接 验证 准备（为static变量分配空间，设置默认值） static 变量在JDK7之前存储在instanceKlass末尾，从JDK7开始，存储于java_mirror的末尾（堆中） static 变量分配空间和赋值是两个步骤，分配空间在准备阶段完成，赋值在初始化阶段完成 如果变量是final的基本类型或字符串常量，那么编译阶段值就确定了，赋值在准备阶段完成 如果变量是final的，但属于引用类型，那么赋值也会在初始化阶段完成 解析 符号引用解析为直接引用 初始化 初始化即调用()V,虚拟机会保证这个类的[构造方法]的线程安全 初始化时机 主动使用(六种): 创建类的实例 访问某个类或接口的静态变量，或者对该静态变量赋值 调用类的静态方法 反射(如 Class.forName(“info.haominglfs.test”)) 初始化一个类的子类 java虚拟机启动时被表明为启动类的类(含有main方法) 被动使用，除了以上六种情况，其他使用java类的方式都被看做是对类的被动使用，都不会导致类的初始化 访问类的static final静态常量(基本类型和字符串)不会触发初始化 类对象.class不会触发初始化 创建该类的数组不会触发初始化 类加载器的loadClass方法 Class.forName的参数2为false时 所有的java虚拟机实现必须在每个类或接口被java程序首次主动使用时才初始化他们，其他使用java类的方式都被看做是对类的被动使用，都不会导致类的初始化 单例例子 12345678910111213class Singleton &#123; //懒惰式单例模式 private Singleton() &#123; &#125; //1.首次使用时才会触发初始化 //2.静态类可以访问外部类的构造方法 private static class HolderClass &#123; private final static Singleton instance = new Singleton(); &#125; //由类加载器来保证线程安全 public static Singleton getInstance() &#123; return HolderClass.instance; &#125;&#125; 类加载器 以jdk8为例: | 名称 | 加载哪的类 | 说明 || :———————: | :——————-: | :————————-: || Bootstrap ClassLoader | JAVA_HOME/jre/lib | 无法直接访问 || Extension ClassLoader | JAVA_HOME/jre/lib/ext | 上级为Bootstrap，显示为null || Application ClassLoader | classpath | 上级为Extension || 自定义类加载器 | 自定义 | 上级为Application | 双亲委派 123456789101112131415161718192021222324252627282930313233343536protected Class&lt;?&gt; loadClass(String name, boolean resolve) throws ClassNotFoundException&#123; synchronized (getClassLoadingLock(name)) &#123; // 1.检查该类是否已经加载 Class&lt;?&gt; c = findLoadedClass(name); if (c == null) &#123; long t0 = System.nanoTime(); try &#123; if (parent != null) &#123; //2.有上级的话，委派上级loadClass c = parent.loadClass(name, false); &#125; else &#123; //3.如果没有上级(ExtClassLoader),则委派BootstrapClassLoader c = findBootstrapClassOrNull(name); &#125; &#125; catch (ClassNotFoundException e) &#123; // ClassNotFoundException thrown if class not found // from the non-null parent class loader &#125; if (c == null) &#123; //4.每一层找不到，调用findClass方法(每个类加载器自己扩展)来加载 long t1 = System.nanoTime(); c = findClass(name); //5.记录耗时 sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0); sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1); sun.misc.PerfCounter.getFindClasses().increment(); &#125; &#125; if (resolve) &#123; resolveClass(c); &#125; return c; &#125;&#125; 线程上下文类加载器 是当前线程使用的类加载器 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172public class DriverManager &#123; // 注册驱动的集合 private final static CopyOnWriteArrayList&lt;DriverInfo&gt; registeredDrivers = new CopyOnWriteArrayList&lt;&gt;(); /* Prevent the DriverManager class from being instantiated. */ private DriverManager()&#123;&#125; /** * Load the initial JDBC drivers by checking the System property * jdbc.properties and then use the &#123;@code ServiceLoader&#125; mechanism */ //初始化驱动 static &#123; loadInitialDrivers(); println("JDBC DriverManager initialized"); &#125;&#125;private static void loadInitialDrivers() &#123; String drivers; try &#123; drivers = AccessController.doPrivileged(new PrivilegedAction&lt;String&gt;() &#123; public String run() &#123; return System.getProperty("jdbc.drivers"); &#125; &#125;); &#125; catch (Exception ex) &#123; drivers = null; &#125; //1.使用serviceLoader机制加载驱动，即SPI AccessController.doPrivileged(new PrivilegedAction&lt;Void&gt;() &#123; public Void run() &#123; ServiceLoader&lt;Driver&gt; loadedDrivers = ServiceLoader.load(Driver.class); Iterator&lt;Driver&gt; driversIterator = loadedDrivers.iterator(); try&#123; while(driversIterator.hasNext()) &#123; driversIterator.next(); &#125; &#125; catch(Throwable t) &#123; // Do nothing &#125; return null; &#125; &#125;); println("DriverManager.initialize: jdbc.drivers = " + drivers); if (drivers == null || drivers.equals("")) &#123; return; &#125; //2.使用jdbc.drivers定义的驱动名加载驱动 String[] driversList = drivers.split(":"); println("number of Drivers:" + driversList.length); for (String aDriver : driversList) &#123; try &#123; println("DriverManager.Initialize: loading " + aDriver); //这里的ClassLoader.getSystemClassLoader()就是应用程序类加载器 Class.forName(aDriver, true, ClassLoader.getSystemClassLoader()); &#125; catch (Exception ex) &#123; println("DriverManager.Initialize: load failed: " + ex); &#125; &#125; &#125; public static &lt;S&gt; ServiceLoader&lt;S&gt; load(Class&lt;S&gt; service) &#123; //线程上下文类加载器 ClassLoader cl = Thread.currentThread().getContextClassLoader(); return ServiceLoader.load(service, cl); &#125; 自定义类加载器 什么时候需要自定义类加载器 想加载非classpath随意路径中的类文件 都是通过接口来使用实现，希望解耦时，常用在框架设计中 这些类希望予以隔离，不同应用的同名类都可以加载，不冲突，常见于tomcat容器 步骤 继承ClassLoader父类 要遵从双亲委派模型，重写findClass方法（不是重写loadClass方法，否则不会走双亲委派机制） 读取类文件的字节码 调用父类的defineClass方法来加载类 使用者调用该类加载器的loadClass方法 例子 123456789101112131415public class MyClassLoader extends ClassLoader &#123; @Override protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException &#123; String path = "/myclasspath/"+name+".class"; try &#123; ByteOutputStream os = new ByteOutputStream(); Files.copy(Paths.get(path),os); byte[] bytes = os.toByteArray(); return defineClass(name, bytes, 0, bytes.length); &#125; catch (IOException e) &#123; e.printStackTrace(); throw new ClassNotFoundException("类文件未找到"); &#125; &#125;&#125; ​ ​ ​​ ​]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式-简单工厂模式]]></title>
    <url>%2F2017%2F06%2F11%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E7%AE%80%E5%8D%95%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[定义简单工厂模式:定义一个工厂类，他可以根据参数的不同返回不同类的实例，被创建的实例通常都具有共同的父类。因为在简单工厂模式中用于创建实例的方法是静态(static)方法，因此简单工厂模式又被称为静态工厂方法模式。 UML Prodcut(抽象产品角色)：它是所有工厂类所创建的所有对象的父类，封装了各种产品对象的共有方法。 ProductA(具体产品角色)：它是简单工厂模式的创建目标，继承了抽象产品角色。 Factory(工厂角色）：简单工厂模式的核心，负责实现创建所有产品实例的内部逻辑，可以被外界直接调用，提供了静态工厂方法。 简单工厂模式的简化有时候，为了简化简单工厂模式，可以将抽象产品类和工厂类合并，将静态工厂方法移至抽象产品类中。 总结 简单工厂模式的主要优点如下:(1) 工厂类包含必要的判断逻辑，可以决定在什么时候创建哪一个产品类的实例，客户端可以免除直接创建产品对象的职责，而仅仅“消费”产品，简单工厂模式实现了对象创建和使用的分离。(2) 客户端无须知道所创建的具体产品类的类名，只需要知道具体产品类所对应的参数即可， 对于一些复杂的类名，通过简单工厂模式可以在一定程度减少使用者的记忆量。(3) 通过引入配置文件，可以在不修改任何客户端代码的情况下更换和增加新的具体产品类， 在一定程度上提高了系统的灵活性。 简单工厂模式的主要缺点如下(1) 由于工厂类集中了所有产品的创建逻辑，职责过重，一旦不能正常工作，整个系统都要受 到影响(2) 使用简单工厂模式势必会增加系统中类的个数(引入了新的工厂类)，增加了系统的复杂 度和理解难度。(3) 系统扩展困难，一旦添加新产品就不得不修改工厂逻辑，在产品类型较多时，有可能造成 工厂逻辑过于复杂，不利于系统的扩展和维护。(4) 简单工厂模式由于使用了静态工厂方法，造成工厂角色无法形成基于继承的等级结构。 试用场景(1) 工厂类负责创建的对象比较少，由于创建的对象较少，不会造成工厂方法中的业务逻辑太 过复杂(2) 客户端只知道传入工厂类的参数，对于如何创建对象并不关心。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式-单例模式]]></title>
    <url>%2F2017%2F06%2F10%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[在实际开发中，为了节约系统资源，有时需要确保系统中某个类只有唯一的一个实例，当这个唯一的实例创建成功后，我们无法再创建一个同类型的其他对象，所有的操作都基于这个唯一的对象，这就是单例模式的动机所在。类图如下： 为了实现唯一性，该类有以下特性：1.将该类构造函数的可见性改为private。2.定义一个静态类型的Singtelon私有变量。3.增加一个共有的静态方法，用来获得该私有变量。 123456789101112class Singleton &#123; private Singleton()&#123;&#125; //私有的构造函数 private static Singleton instance = null;//私有静态变量 public Singleton getInstance()&#123; if(instance == null)&#123; return new Singleton(); &#125; return instance; &#125;&#125; 上面的代码在多线程环境下会出现创建多个实例的情况。对此，有两种解决方案：1.饿汉式单例模式 12345678class EagerSingleton &#123; private static final EagerSingleton instance = new EagerSingleton(); private EagerSingleton() &#123; &#125; public static EagerSingleton getInstance() &#123; return instance; &#125; &#125; 2.懒汉式单例模式(使用双重检查锁) 12345678910111213141516171819class LazySingleton &#123; private volatile static LazySingleton instance = null; private LazySingleton() &#123; &#125; public static LazySingleton getInstance() &#123; //第一重判断 if (instance == null) &#123; //锁定代码块 synchronized (LazySingleton.class) &#123; //第二重判断 if (instance == null) &#123; instance = new LazySingleton(); //创建单例实例 &#125; &#125; &#125; return instance; &#125; &#125; 需要注意的是，如果使用双重检查锁定来实现懒汉式单例类，需要在静态成员变量instance之前增加修饰符volatile，被volatile修饰的成员变量可以确保多个线程都能够正确处理，且该代码只能在JDK 1.5及以上版本中才能正确执行。由于volatile关键字会屏蔽Java虚拟机所做的一些代码优化，可能会导致系统运行效率降低，因此即使使用双重检查锁定来实现单例模式也不是一种完美的实现方式。 饿汉式单例类在类被加载时就将自己实例化，它的优点在于无须考虑多线程访问问题，可以确保实例的唯一性；从调用速度和反应时间角度来讲，由于单例对象一开始就得以创建，因此要优于懒汉式单例。但是无论系统在运行时是否需要使用该单例对象，由于在类加载时该对象就需要创建，因此从资源利用效率角度来讲，饿汉式单例不及懒汉式单例，而且在系统加载时由于需要创建饿汉式单例对象，加载时间可能会比较长。 懒汉式单例类在第一次使用时创建，无须一直占用系统资源，实现了延迟加载，但是必须处理好多个线程同时访问的问题，特别是当单例类作为资源控制器，在实例化时必然涉及资源初始化，而资源初始化很有可能耗费大量时间，这意味着出现多线程同时首次引用此类的机率变得较大，需要通过双重检查锁定等机制进行控制，这将导致系统性能受到一定影响。 一种更好的实现方式Initialization Demand Holder (IoDH)技术： 123456789class Singleton &#123; private Singleton() &#123; &#125; private static class HolderClass &#123; private final static Singleton instance = new Singleton(); &#125; public static Singleton getInstance() &#123; return HolderClass.instance; &#125;&#125; 由于静态单例对象没有作为Singleton的成员变量直接实例化，因此类加载时不会实例化Singleton，第一次调用getInstance()时将加载内部类HolderClass，在该内部类中定义了一个static类型的变量instance，此时会首先初始化这个成员变量，由Java虚拟机来保证其线程安全性，确保该成员变量只能初始化一次。由于getInstance()方法没有任何线程锁定，因此其性能不会造成任何影响。 ​ ​]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git使用]]></title>
    <url>%2F2017%2F04%2F30%2Fgit%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[git使用今天写了一个在在标签页显示数字的chrome扩展程序，打算提交到github,顺便学习了将一个已有的项目提交到github的方法。 登录github，新建一个仓库 进入项目的本地目录，执行如下命令： git initgit remote add origin git@github.com:haominglfs/tab_number.git//与远程仓库建立关联git add .git commit -m &#39;tab_number extension of chrome v0.1&#39;git push -u origin master //push到远程仓库]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[iTerm2]]></title>
    <url>%2F2017%2F04%2F19%2FiTerm2%2F</url>
    <content type="text"><![CDATA[#iTerm2配置 配色1.git clone git@github.com:altercation/solarized.git2.这里我们要使用的是iterm2-colors-solarized目录下的，包括Solarized Dark.itermcolors和Solarized Light.itermcolors两个配置文件。3.打开Preferences-&gt;Profiles-&gt;Color面板，在Color Presets中将以上 两个配置方案导入，然后选择Solarized Dark或者Solarized Light即可。一般推荐使用Solarized Dark，Solarized Light有种亮瞎的感觉。 oh-my-zsh1.接下来，用oh-my-zsh来武装zsh，一行命令搞定：sh -c &quot;$(wget https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O -)&quot;2.oh-my-zsh中提供了多套主题可供选择，有不同的输出样式及配色。默认应该是robbyrussell，感觉中规中矩没啥亮点。翻看了一下，发现了agnoster主题，感觉非常入眼。接下来，编辑~/.zshrc，找到变量ZSH_THEME将其赋值改为agnoster即可。3.为了显示正常，需要安装powerline字体，方法如下：git clone git@github.com:powerline/fonts.gitcd fonts./install然后，在iTerm2-&gt;Preferences-&gt;Profiles-&gt;Text面板中将Non-ASCII Font改成Roboto Mono Powerline，显示就正常了！]]></content>
      <categories>
        <category>iTerm2</category>
      </categories>
      <tags>
        <tag>iTerm2配置</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shadowsocks配置]]></title>
    <url>%2F2017%2F04%2F19%2Fshadowsocks%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[#shadowsocks配置 第一步 用远程工具登录aws主机 第二步：安装shadowsocks依赖 sudo -s //获取超级管理员权限 apt-get update//更新apt-get apt-get install python-pip//安装pyton包管理工具 pip install shadowsocks//安装shadowsocks ssserver -c /etc/shadowsocks.json -d start//启动shadowsocks 第三步:配置shadowsocks vi /etc/shadowsocks.json//编辑配置文件 单一端口配置 12345678910&#123; &quot;server&quot;:&quot;0.0.0.0&quot;, &quot;server_port&quot;:端口, &quot;local_address&quot;:&quot;127.0.0.1&quot;, &quot;local_port&quot;:1080, &quot;password&quot;:&quot;连接密码&quot;, &quot;timeout&quot;:300, &quot;method&quot;:&quot;aes-256-cfb&quot;, &quot;fast_open&quot;:false &#125; 多端口配置 12345678910&#123; &quot;server&quot;:&quot;0.0.0.0&quot;, &quot;port_password&quot;: &#123; &quot;端口1&quot;: &quot;连接密码1&quot;, &quot;端口2&quot; : &quot;连接密码2&quot; &#125;, &quot;timeout&quot;:300, &quot;method&quot;:&quot;aes-256-cfb&quot;, &quot;fast_open&quot;: false &#125; 开启aws 入站端口 配置好shaodowsocks后，还需要将配置中的端口打开,这样客户端的服务才能链接得上EC2中的shadowsocks服务首先打开正在运行的实例，向右滚动表格，看到最后一项，安全组，点击进入 默认是开启了一个22端口（这是给ssh访问的），再建一个如下图红框标示的端口，我的shadowsocks配置的端口是8388，所以这里就开启8388， 配置文件编辑完成后，接下来就可以部署运行了： 1ssserver -c /etc/shadowsocks.json -d start 当然，我们可不希望每次重启服务器都手动启动 SS, 因此我们要把这条命令放到这个文件下：/etc/rc.d/rc.local，这样以后就能开机自动运行了。]]></content>
      <categories>
        <category>vpn</category>
      </categories>
      <tags>
        <tag>shadowsocks</tag>
      </tags>
  </entry>
</search>
